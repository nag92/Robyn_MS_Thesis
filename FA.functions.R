 ##### Compiled Factor Analysis Functions# Zack Williams# Updated 03/18/2021# Package loading using pacmanif(!require(pacman)){  install.packages("pacman")  require(pacman)}# Required packages: pacman::p_load(semTools,psych,GPArotation,lavaan,regsem,MASS,qgraph,ggplot2,               boot,parallel,mnormt,lsr,pbapply,lslx,EGAnet,igraph,wTO)# Quiet utility function - quiet return of objectquiet <- function(x) {   sink(tempfile())   on.exit(sink())   invisible(force(x)) } # loads() or loadings() function# Utility function to get standardized loadings from lavaan or other object# Now able to handle "lslx" objects (regularized SEM from lslx package)loadings <- loads <- function(obj,bifactor=NULL,add.h2=T,cut=NULL,selector="bic",...){  if("lavaan" %in% class(obj)){    loadingmat <- lavInspect(obj,"std",...)$lambda    Phi <- lavInspect(obj,"cor.lv",...)    h2 <- diag(loadingmat %*% Phi %*% t(loadingmat))    # figure out whether bifactor model is appropriate    if(is.null(bifactor)){       bifactor <- (all(abs(loadingmat[,1])>0.1) & ncol(loadingmat) > 1 & all(Phi == diag(nrow(Phi))))    }  } else if(any(attr(class(obj),"package")=="mirt")){ # mirt object    loadingmat <- extract.mirt(obj,"F")    h2 <- extract.mirt(obj,"h2")    Phi <- diag(ncol(loadingmat))    # Switch signs of loadings in matrix    signed <- sign(colSums(loadingmat))    signed[signed==0] <- 1    if(ncol(loadingmat) > 1){      cnames <- colnames(loadingmat)      loadingmat <- loadingmat %*% diag(signed)  # flips factors to be in positive direction but loses the colnames      colnames(loadingmat) <- cnames # put the names back    } else{ # for 1-factor case      loadingmat <- loadingmat * signed    }    if(is.null(bifactor)){       bifactor <- (all(abs(loadingmat[,1])>0.1) & ncol(loadingmat) > 1 & all(Phi == diag(nrow(Phi))))    }  } else if("lslx" %in% class(obj)){    loadingmat <- obj$extract_coefficient_matrix(selector=selector,block = "y<-f")$g    Phi <- obj$extract_coefficient_matrix(selector=selector,block = "f<->f")$g    h2 <- diag(loadingmat %*% Phi %*% t(loadingmat))  } else{    loadingmat <- stats::loadings(obj,...)    Phi <- obj$Phi    if(!is.null(Phi)){      h2 <- diag(loadingmat %*% Phi %*% t(loadingmat))    } else{      h2 <- diag(loadingmat %*% t(loadingmat))    }  }  # Add communalities  if(add.h2){    # figure out whether bifactor model is appropriate    if(is.null(bifactor)){      bifactor <- (all(abs(loadingmat[,1])>0.1)  & ncol(loadingmat) > 1 & all(Phi == diag(nrow(Phi))))    }    if(bifactor){IECV <- apply(loadingmat,1,function(x){x[1]^2/sum(x^2)})} else{IECV <- NULL}    if(!is.null(cut)){loadingmat[which(abs(loadingmat) < cut)] <- NA}    result <- cbind(loadingmat,"h2"=h2,"I-ECV"=IECV)  } else{    if(!is.null(cut)){loadingmat[which(abs(loadingmat) < cut)] <- NA}    result <- loadingmat  }  class(result) <- c("lavaan.matrix","matrix")  return(result)}# Wrapper function for factor correlation matrix extractionPhi <- function(obj,plot=F,selector="bic",...){  if("lavaan" %in% class(obj)){    phi <- lavInspect(obj,"cor.lv")  } else if(any(attr(class(obj),"package")=="mirt")){ # mirt model    if("MultipleGroupClass" %in% class(obj)){ # Use phi from first group      message("MultipleGroupClass detected. Using only factor correlations from first group.")      phi <- quiet(summary(obj)[[1]]$fcor)    } else{      phi <- quiet(summary(obj)$fcor)    }    # if phi is a symmetric matrix, return it. Otherwise, use lower tri to make it symmetric    if(isSymmetric(phi)){      class(phi) <- c("lavaan.matrix.symmetric","matrix")    } else{      phi_lower <- phi[lower.tri(phi)] # take values below diagonal      phi <- t(phi)      phi[lower.tri(phi)] <- phi_lower      class(phi) <- c("lavaan.matrix.symmetric","matrix")    }  } else if("lslx" %in% class(obj)){    phi <- obj$extract_coefficient_matrix(selector=selector,block = "f<->f")$g    class(phi) <- c("lavaan.matrix.symmetric","matrix")  } else{    if(is.matrix(obj) & is.null(obj$Phi)){return(NULL)}    phi <- obj$Phi # psych::fa object or GPArotation object    if(is.null(phi)){ # If no phi found, assume orthogonal      ls <- loads(obj,add.h2 = F)      phi <- diag(ncol(ls))      rownames(phi) <- colnames(phi) <- colnames(ls)    }    class(phi) <- c("lavaan.matrix.symmetric","matrix")  }  # Plot correlations  if(plot==T){    psych::corPlot(phi,xlas=2,...)  }  return(phi)}# var.prop.bf - partitions the variances attributable to general factor, specific factor, and uniquenessvar.prop.bf <- var.prop.bifactor <- function(lmat,dig=3){  return(round(cbind("G"=lmat[,1,drop=F]^2,"S"=rowSums(lmat[,-1]^2),"u2"=1-rowSums(lmat^2),"h2"=rowSums(lmat^2),"I-ECV"=lmat[,1]^2/rowSums(lmat^2),        "G/S"=lmat[,1]^2/rowSums(lmat[,-1]^2),"G/E"=lmat[,1]^2/(1-rowSums(lmat^2)),"S/E"=rowSums(lmat[,-1]^2)/(1-rowSums(lmat^2))),dig))}### fitMeasures.cML: Categorical ML estimation of fit indices# Savalei, V. (2020). Improving Fit Indices in Structural Equation Modeling with Categorical Data. # Multivariate Behavioral Research, 1-18. http://doi.org/10.1080/00273171.2020.1717922## Code to calc new fit CFI/RMSEA is cannibalized from Victoria Savalei's code, provided at https://osf.io/9asg7/## Slightly modified to allow for different # of thresholds per item if necessary# Takes output of (a) fitted DWLS/ULS lavaan model or (b) lavaan model syntax + raw data.# ... functions are for qgraph::cor_auto (used if only "data" and "model" are provided to get polycor matrix)fitMeasures.cML <- function(obj=NULL,model=NULL,data=NULL,ordered=T,orthogonal=F,missing="pairwise",digits=3,...){  # RMSEA CI functions  lower.lambda <- function(lambda){    return(pchisq(Tsb.ml, df=df, ncp=lambda) - 0.95)  }  upper.lambda <- function(lambda){    return(pchisq(Tsb.ml, df=df, ncp=lambda) - 0.05)  }    ### Generate factor model based on provided model, if one is not provided  if(is.null(model)){    if(is.null(obj)) stop("Either 'obj' or 'model' and 'data' must be specified.")    loads <- inspect(obj,"std")$lambda    model <- paste(unlist(lapply(1:ncol(loads),function(i){      fname <- colnames(loads)[i]      which.vars <- rownames(loads)[which(loads[,i]!=0)]      return(paste(fname,"=~",paste(which.vars,collapse=" + ")))    })),collapse=" \n")  }    # If no data, generate it to re-fit model. Also get correlation matrix  if(is.null(data)){    dat <- obj@Data@X[[1]]    S <- lavInspect(obj,"sampstat")$cov #S, or polychoric correlations  } else{ # data/model only    dat <- data    obj <- try(cfa(model,data=dat,ordered=ordered,orthogonal=orthogonal,missing=missing,std.lv=T,...),silent=TRUE)     S <- lavInspect(obj,"sampstat")$cov  }    N <- nrow(dat)  p <- ncol(dat)    ### Cannibalized code chunk starts here ########################################  Gamma<-lavInspect(obj, "gamma") #Gamma, asymptotic cov matrix of polychorics  Gamma<-Gamma*N/(N-1) #This puts Gamma and W.dwls on the same scale  nt <- length(lavInspect(obj,"th"))  Gamma<-Gamma[-(1:nt),-(1:nt)] #removing thresholds    if(det(S)<0){    RMSEA.MLs.ml <- CFI.MLs.ml <- TLI.MLs.ml <- RMSEA.MLs.ml.CI.lower <- RMSEA.MLs.ml.CI.upper <- NA  } else {    result1 <- cfa(model,estimator="ML",sample.cov=S,sample.nobs=N,mimic="EQS",orthogonal=orthogonal,std.lv=T,...)    # summary(result1,fit.measures=T) - for debugging    df <- fitmeasures(result1)["df"]     df.base <- fitmeasures(result1)["baseline.df"]     W.ML <- lavInspect(result1,"wls.v") #NT weight matrix    delta.ML <- lavInspect(result1, "delta") #model derivatives     E.inv.ML <- lavTech(result1, "inverted.information") #internal for: solve(t(delta) %*% W.ML%*% delta)    U.ML.full <- (W.ML- W.ML %*% delta.ML %*% E.inv.ML %*% t(delta.ML)%*%W.ML)     #dimension reduction to make U.ML compatible with WLS Gamma (removing diagonal elements)    idx <- lav_matrix_diagh_idx(n = p)    U.ML <- U.ML.full[-idx,-idx]         ks.ml<-sum(diag(t(U.ML) %*% Gamma)) #the trace correction factor         Fml.ml<-2*fitmeasures(result1)["fmin"]      ins<-Fml.ml/df-ks.ml/(df*(N-1))    if(ins>0){      RMSEA.MLs.ml<-sqrt(ins)    } else {      RMSEA.MLs.ml=0    }        Tml.ml<-fitmeasures(result1)["chisq"] #ML chi-square at ML estimates    Tsb.ml<-df/ks.ml*Tml.ml #SB chi-square at Ml estimates         #RMSEA CI    lambda.l <- try(uniroot(f=lower.lambda, lower=0, upper=Tml.ml)$root,silent=TRUE)     if(inherits(lambda.l, "try-error")) {       lambda.l <- NA; RMSEA.MLs.ml.CI.lower <- 0    } else { if(lambda.l<0){      RMSEA.MLs.ml.CI.lower=0    } else {      RMSEA.MLs.ml.CI.lower<-sqrt((ks.ml/df)*lambda.l/((N-1)*df))    }    }        N.RMSEA <- max(N, Tsb.ml*4) #stolen from lavaan     lambda.u <- try(uniroot(f=upper.lambda, lower=0,upper=N.RMSEA)$root,silent=TRUE)    if(inherits(lambda.u, "try-error")) { lambda.u <- NA; RMSEA.MLs.ml.CI.upper<-NA     } else { if(lambda.u<0){      RMSEA.MLs.ml.CI.upper=0    } else {      RMSEA.MLs.ml.CI.upper<-sqrt((ks.ml/df)*lambda.u/((N-1)*df))    }    }        base.chisq<-fitmeasures(result1)["baseline.chisq"] #the minimum of the ind model    kbs.ml<-sum(diag(Gamma))     denb<-base.chisq-kbs.ml    numb<-Tml.ml-ks.ml    CFI.MLs.ml<-1-numb/denb    ## Add in TLI using formula on Savalei (2020), p. 15    ## TLI = 1 - (df.b/df)(1 - CFI)    TLI.MLs.ml <- 1 - df.base/df*(1 - CFI.MLs.ml)        if(CFI.MLs.ml > 1){CFI.MLs.ml = 1} # Constrain CFI to 1      } #end of det(S) loop for computing FML output  out <- c(CFI.MLs.ml,TLI.MLs.ml,RMSEA.MLs.ml,RMSEA.MLs.ml.CI.lower,RMSEA.MLs.ml.CI.upper)  names(out) <- c("CFI.cML","TLI.cML","RMSEA.cML","RMSEA.cML.lower","RMSEA.cML.higher")  return(round(out,digits))}### sem.fit - pretty prints the lavaan fit indices that I care most about### Zack Williams, updated 06/03/2020## Also some extra fit indices like SRMR/R^2, max absolute residual, % absolute residuals > 0.2## Also prints out unbiased SRMR and CI for that when possible (Maydeu-Olivares, 2017; Shi et al., 2020)# (SRMR refs: https://doi.org/10.1007%2Fs11336-016-9552-7; https://doi.org/10.1080/10705511.2019.1611434)## Now includes updated fit indices for categorical data proposed by Savalei (2020)# Savalei, V. (2020). Improving Fit Indices in Structural Equation Modeling with Categorical Data. # Multivariate Behavioral Research, 1-18. http://doi.org/10.1080/00273171.2020.1717922## Code to calc new fit CFI/RMSEA is cannibalized from Victoria Savalei's code, provided at https://osf.io/9asg7/# cML option: Refits model with ML to calculate fit indices (T) or gets them from DWLS params (F)# ...: extra arguments to add to re-fitting of model for cML fit indices. Ignored if cML = F.sem.fit <- function(obj,cML=FALSE,resid.cut=0.1,...){    fmall <- fitMeasures(obj)  if("chisq.scaled" %in% names(fmall)){    fm <- fitMeasures(obj,c("chisq.scaled","df.scaled","pvalue.scaled","cfi.scaled","tli.scaled",                            "rmsea.scaled","rmsea.ci.lower.scaled","rmsea.ci.upper.scaled",                            "srmr","crmr","wrmr"))    scaled=T  } else{ # no correction (e.g., original ML)    fm <- fitMeasures(obj,c("chisq","df","pvalue","cfi","tli",                            "rmsea","rmsea.ci.lower","rmsea.ci.upper",                            "srmr","crmr","wrmr"))    scaled=F  }    res <- suppressWarnings(lavResiduals(obj)$cov)  meanr2 <- mean(inspect(obj,"r2")) # For SRMR/R^2 (see https://doi.org/10.1080/00273171.2018.1476221, https://doi.org/10.1080/10705511.2021.1992596)  absresids <- abs(res) # Absolute Residuals (for max residual and % large resids)  fm <- c(fm,"srmr/r2"=unname(fm[9]/meanr2),          "%|res|>RESCUT"=round(100*mean(absresids[lower.tri(absresids)] > resid.cut),1),          "maxresid"=res[which.max(absresids)])  names(fm) <- gsub("RESCUT$",as.character(resid.cut),names(fm))  result <- list("obj.name"=deparse(substitute(obj)),"Fit"=fm,"resids"=res,                 "Estimator"=obj@Model@estimator,"Scaled.X2"=scaled,                 "Fit.SRMR"=suppressWarnings(lavResiduals(obj)$summary),                 "Fit.CRMR"=suppressWarnings(lavResiduals(obj,type="cor.bollen")$summary),                 "resid.cut"=resid.cut)  usrsmr_overr2 <- result$Fit.SRMR[5:8,]/meanr2  rownames(usrsmr_overr2) <- c("usrmr/r2","usrmr/r2.se","usrmr/r2.ci.lower","usrmr/r2.upper")  result$Fit.SRMR <- rbind(result$Fit.SRMR,usrsmr_overr2) # Adds uSRMR/R2 values to uSRMR output (per https://doi.org/10.1080/10705511.2021.1992596)    ### New fit indices from Savalei (2020), only if DWLS estimation (i.e., WLSMV) or ULS (a special case of DWLS where W = I)  # Only calculates if is DWLS-estimated (though theoretically ULS should be OK too for cML)  # Update to skip this if mixed categorical/continuous  if(result$Estimator %in% c("DWLS","ULS") & all(lavInspect(obj,"est")$nu == 0)){    # Can't fit ML solution to non-PD matrix. Will revert to DWLS solution if that's the case.    S <- lavInspect(obj,"sampstat")$cov #S, or polychoric correlations    if(any(eigen(S)$values <= 0)){      cML <- FALSE      warning("Correlation matrix is not positive definite. Switching to cML(D) estimation.\n")    }    if(cML){ # refit as ML and calculate fit indices      orthogonal <- all(lavInspect(obj,"cov.lv")==diag(ncol(lavInspect(obj,"cov.lv"))))      if(!is.null(obj@Cache$ModelSyntax)){ # if model syntax there, use it        result$Fit.DWLS <- fitMeasures.cML(obj,orthogonal=orthogonal,model = obj@Cache$ModelSyntax,...)      } else{ # else just figure out model syntax from loadings (won't catch correlated errors, etc.)        result$Fit.DWLS <- fitMeasures.cML(obj,orthogonal=orthogonal,...)      }    } else{ # calculate from DWLS params      #functions taken from lavaan (lav_fit_measures.R), for Categorical RMSEA CI computations       lower.lambda <- function(lambda){        (pchisq(Tsb, df=df, ncp=lambda) - 0.95)      }      upper.lambda <- function(lambda){        (pchisq(Tsb, df=df, ncp=lambda) - 0.05)      }      dat <- obj@Data@X[[1]]      N <- nrow(dat)      p <- ncol(dat)      k <- length(unique(na.omit(dat[,1]))) # assumes all items have same # of categories - disregard if not true            ### Cannibalized code chunk starts here ########################################      Sigma <- fitted.values(obj)$cov #Sigma-hat      Sigmai <- solve(Sigma) #Inverse of Sigma-hat      df <- fitmeasures(obj)["df"] #degrees of freedom              #the next chunk computes the new ML-based fit indices       if(det(S)<0){        RMSEA.MLs <- CFI.MLs <- TLI.MLs <- RMSEA.MLs.CI.lower <- RMSEA.MLs.CI.upper <- NA      } else {         Fml <- sum(diag(S%*%Sigmai))-log(det(S))+log(det(Sigma))-p #evaluating the ML function at DWLS estimates         Fml.base <- (-log(det(S))) #the minimum of the ind model        df.base <- p*(p+1)/2-p #df for baseline model                Vold<-.5*lav_matrix_duplication_pre_post((Sigmai %x% Sigmai)) #NT-ML weight matrix, evaluated at DWLS estimates        idx <- lav_matrix_diagh_idx(n = p)        V <- Vold[-idx,-idx] #NT-ML weight matrix, reduced dimensions (removing elements for variances)                W.dwls <- lavInspect(obj,"wls.v") #DWLS weight matrix        Gamma <- lavInspect(obj, "gamma") #Gamma, asymptotic cov matrix of polychorics        Gamma <- Gamma*N/(N-1) #This puts Gamma and W.dwls on the same scale        delta <- lavInspect(obj, "delta") #matrix of model derivatives         E.inv <- lavTech(obj, "inverted.information") #internal for: solve(t(delta) %*% W.dwls%*% delta)        WiU <- (diag(nrow(W.dwls)) - delta %*% E.inv %*% t(delta) %*% W.dwls) #W.dwls^(-1)*U        #dimension reduction to make matrices compatible with V        nt <- length(lavInspect(obj,"th")) #number of thresholds: now accommodates different # for each item        WiU <- WiU[-(1:nt),-(1:nt)] #removing thresholds.          Gamma <- Gamma[-(1:nt),-(1:nt)] #removing thresholds        ks <- sum(diag(t(WiU) %*% V %*% WiU %*% Gamma)) #the trace correction factor         ins <- Fml/df-ks/(df*(N-1))        if(ins>0){          RMSEA.MLs <- sqrt(ins)        } else {          RMSEA.MLs <- 0        }                Tml <- (N-1)*Fml #ML chi-square at DWLS estimates        Tsb <- df/ks*Tml #SB chi-square at DWLS estimates                #RMSEA CI        lambda.l <- try(uniroot(f=lower.lambda, lower=0, upper=Tml)$root,silent=TRUE)         if(inherits(lambda.l, "try-error")) { lambda.l <- NA; RMSEA.MLs.CI.lower<-0        } else {           if(lambda.l<0){            RMSEA.MLs.CI.lower <- 0          } else {            RMSEA.MLs.CI.lower <- sqrt((ks/df)*lambda.l/((N-1)*df))          }        }                N.RMSEA <- max(N, Tsb*4) #stolen from lavaan         lambda.u <- try(uniroot(f=upper.lambda, lower=0,upper=N.RMSEA)$root,silent=TRUE)        if(inherits(lambda.u, "try-error")) {          lambda.u <- NA; RMSEA.MLs.CI.upper<-NA         } else {           if(lambda.u<0){            RMSEA.MLs.CI.upper=0          } else {            RMSEA.MLs.CI.upper<-sqrt((ks/df)*lambda.u/((N-1)*df))          }        }                kbs <- sum(diag(Gamma))         denb <- (N-1)*Fml.base-kbs        numb <- (N-1)*Fml-ks        CFI.MLs <- 1-numb/denb                ### Cannibalized code chunk ends here ########################################        ## Add in TLI using formula on Savalei (2020), p. 15        ## TLI = 1 - (df.b/df)(1 - CFI)        TLI.MLs <- 1 - df.base/df*(1 - CFI.MLs)                if(CFI.MLs > 1){CFI.MLs = 1} # Constrain CFI to 1      } #end of if det(S) loop            newfits <- c(CFI.MLs,TLI.MLs,RMSEA.MLs,RMSEA.MLs.CI.lower,RMSEA.MLs.CI.upper)      names(newfits) <- c("CFI.cMLD","TLI.cMLD","RMSEA.cMLD","RMSEA.cMLD.lower","RMSEA.cMLD.upper")      result$Fit.DWLS <- newfits    }  }    class(result) <- "semFit"  return(result)}# Pretty print the sem.fit outputprint.semFit <- function(x,resids=F,digits=3){  fm <- x$Fit  cat(paste0("Fit Measures for ",x$obj.name," (",x$Estimator," estimation):\n"))  if(x$Scaled.X2){cat(" Scaled")} # say if X2 is scaled if appropriate  cat(paste0(" X2(",round(fm[2]),") = ",round(fm[1],2)," P = ",format(fm[3],digits=3)))  if(!is.null(x$Fit.DWLS)){ # If calculated, print the ML-approximating DWLS indices    cat(paste0("\n Unbiased Categorical Fit Indices (Savalei, 2020):"))    cat(paste0("\n  ",names(x$Fit.DWLS)[1]," = ",round(x$Fit.DWLS[1],digits)," | 'old' CFI = ",round(fm[4],digits)))    cat(paste0("\n  ",names(x$Fit.DWLS)[2]," = ",round(x$Fit.DWLS[2],digits)," | 'old' TLI = ",round(fm[5],digits)))    cat(paste0("\n  ",names(x$Fit.DWLS)[3]," = ",round(x$Fit.DWLS[3],digits)," (90% CI: ",               round(x$Fit.DWLS[4],digits),"—",round(x$Fit.DWLS[5],digits),")"))    cat(paste0(" | 'old' RMSEA = ",round(fm[6],digits)," (90% CI: ",round(fm[7],digits),"—",round(fm[8],digits),")"))  } else{    cat(paste0("\n CFI = ",round(fm[4],digits),"\n TLI = ",round(fm[5],digits)))    cat(paste0("\n RMSEA = ",round(fm[6],digits)," (90% CI: ",round(fm[7],digits),"—",round(fm[8],digits),")"))  }  if(!is.null(x$Fit.SRMR)){    # When available, uses categorical SRMR (Shi et al., 2020)    cat("\n Unbiased SRMR/CRMR (Maydeu-Olivares, 2017; Shi et al., 2020)")    cat(paste0("\n  uSRMR = ",round(x$Fit.SRMR[5,1],digits),               " (90% CI: ",round(x$Fit.SRMR[7,1],digits),"—",round(x$Fit.SRMR[8,1],digits),")"))    cat(paste0(" | 'old' SRMR = ",round(x$Fit.SRMR[1,1],digits),               " (90% CI: ",round(x$Fit.SRMR[1,1] - qnorm(0.95)*x$Fit.SRMR[2,1],digits),"—",               round(x$Fit.SRMR[1,1] + qnorm(0.95)*x$Fit.SRMR[2,1],digits),")"))    cat(paste0("\n  uCRMR = ",round(x$Fit.CRMR[5,1],digits),               " (90% CI: ",round(x$Fit.CRMR[7,1],digits),"—",round(x$Fit.CRMR[8,1],digits),")"))    cat(paste0(" | 'old' CRMR = ",round(x$Fit.CRMR[1,1],digits),               " (90% CI: ",round(x$Fit.CRMR[1,1] - qnorm(0.95)*x$Fit.CRMR[2,1],digits),"—",               round(x$Fit.CRMR[1,1] + qnorm(0.95)*x$Fit.CRMR[2,1],digits),")"))    cat(paste0("\n uSRMR/R^2 = ",round(x$Fit.SRMR[12,1],digits),        " (90% CI: ",round(x$Fit.SRMR[14,1],digits),"—",round(x$Fit.SRMR[15,1],digits),")"))  } else{ # if no Summary, just get normal SRMR. Also print SRMR/R^2    cat(paste0("\n SRMR = ",round(fm[9],digits), " (SRMR/R^2 = ",round(fm[12],digits),")"))    cat(paste0("\n CRMR = ",round(fm[10],digits)))  }  if(x$Estimator != "ML"){    cat(paste0("\n WRMR = ",round(fm[11],digits))) # Print WRMR for non-ML fits  }  cat(paste0("\n |Residuals| > ",x$resid.cut," = ",round(fm[13],1),"%"))  # Maximum absolute residual items:  maxres <- which.max(abs(x$resids))  maxcol <- maxres %/% nrow(x$resids) + 1  maxrow <- which.max(abs(x$resids[,maxcol]))  maxname <- paste(colnames(x$resids)[c(maxrow,maxcol)],collapse="|")  cat(paste0("\n Largest Residual = ",round(fm[14],digits)," (",maxname,")"))  if(resids){    cat("\n\nResidual Matrix:\n")    print(x$resids)  }}### residCheck- now for mirt and Lavaan objects (and EFA from psych)# Takes object, prints out residuals greater than value (default 0.1, though 0.2 is another decent cutoff)# Updated 04/08/21 to now print out average RMS residual for each item and check for outliers within that set# Updated 08/30/21 to now average residuals from multiple groupsresidCheck <- function(obj,cut=0.1,dig=3,n.large=T,pos.only=F,item.resids=F){  if(is.null(attr(class(obj),"package"))){    resmat <- residuals(obj)  } else if(attr(class(obj),"package")=="lavaan"){    resmat <- residuals(obj)$cov  } else if(attr(class(obj),"package")=="mirt"){    QMC <- extract.mirt(obj,"nfact")>3    resmat <- M2(obj,na.rm = T,residmat = T,QMC=QMC)    # Merge together if multipleGroup class    if(length(obj@Data$groupNames) > 1){      group_props <- prop.table(table(obj@Data$group))      resmat <- Reduce("+",lapply(1:length(resmat),function(i){        return(resmat[[i]] * group_props[i])      }))    }    resmat[upper.tri(resmat)] <- t(resmat)[upper.tri(resmat)]  } else{    resmat <- residuals(obj)  }    inames <- colnames(resmat)    # Matrix of names (for vector of item pairs)  name_mat <- sapply(1:length(inames),function(i){    sapply(1:length(inames),function(j){      paste0(inames[i],"|",inames[j])    })  })    res <- resmat[lower.tri(resmat)]  names(res) <- name_mat[lower.tri(resmat)]  RMSR <- sqrt(mean(res^2,na.rm=T))    if(pos.only){ # positive residuals only    largeResids <- res[which(res > cut)]  } else{ # positive and negative residuals    largeResids <- res[which(abs(res) > cut)]  }    if(n.large){    ## Look at how many high/low outliers from each item (only items with 2+)    n_high_res <- sort(sapply(inames,function(i){      length(grep(paste0("\\|",i,"$"),names(largeResids))) + length(grep(paste0("^",i,"\\|"),names(largeResids)))    }),decreasing=T)    n_high_res <- n_high_res[n_high_res > 1]    if(length(n_high_res)==0){n_high_res <- NULL}  } else{    n_high_res <- NULL  }    itemResids <- apply(resmat,1,function(X){sqrt(mean(X^2,na.rm=T))})    result <- list("Residuals"=round(resmat,dig),"Large.Resids"=round(largeResids,dig),"RMSR"=round(RMSR,dig),"N.Large.Resids"=n_high_res)  if(item.resids){ # Include item-level RMSR in output    result$Item.RMSR <- round(itemResids,dig)    item_outliers <- which((itemResids-hd(itemResids))/(1.4826*hd(abs(itemResids-hd(itemResids)))) > 2.24)    if(length(item_outliers)>0){      result$RMSR.Outliers <- round(itemResids,dig)[item_outliers]    }    result$Cutoff <- cut  } else{    item_outliers <- which((itemResids-hd(itemResids))/(1.4826*hd(abs(itemResids-hd(itemResids)))) > 2.24)    if(length(item_outliers)>0){      result$RMSR.Outliers <- round(itemResids,dig)[item_outliers]    }    result$Cutoff <- cut  }  return(result)}# corCheck - checks to see whether correlations are greater than or equal to X, prints out plot too# Update 03.27.21 - now looks at cors above cut in absolute valuecorCheck <- function(obj,cut=0.7,dig=3,pos.only=F,...){  if(isSymmetric(as.matrix(obj))){    cormat <- obj  } else{    cormat <- cor_auto(obj,verbose=F,...)  }    inames <- colnames(cormat)    # Matrix of names (for vector of item pairs)  name_mat <- sapply(1:length(inames),function(i){    sapply(1:length(inames),function(j){      paste0(inames[i],"|",inames[j])    })  })  avg_iic <- sort((rowSums(cormat)-1)/(ncol(cormat)-1),decreasing=T)  cors <- cormat[lower.tri(cormat)]  names(cors) <- name_mat[lower.tri(cormat)]    if(pos.only){ # only flag cors that are positive and larger than 'cut'    largeCors <- cors[which(cors >= cut)]  } else{ # flag any cor with abs(r) > cut    largeCors <- cors[which(abs(cors) >= cut)]  }    return(list("Avg.Cor"=round(avg_iic,dig),"LargeCors"=sort(round(largeCors,dig),decreasing=T),"Cutoff"=cut))}# corOutliers# Zack Williams, updated 06/27/19# Uses the mad_ouliers function to find the outlying correlations in a correlation matrix# Returns high and low outliers (i.e., over or under the median correlation) corOutliers <- function(obj,dig=3,bend=2.24,byrow=NULL,absvals=F,n.outliers=T,...){  if(isSymmetric(as.matrix(obj))){    cormat <- obj  } else{    cormat <- cor_auto(obj,verbose=F,...)  }  inames <- colnames(cormat)  # absvals==T does calculations on correlation absolute values (i.e., if reverse coded items)  if(absvals==T){    cormat <- abs(cormat)  }  if(is.null(byrow)){ # Default: use byrow for 7 or more items    if(ncol(cormat) < 7){      byrow <- F    } else{      byrow <- T    }  }  if(byrow==T){    # Go through and take each row, find outliers, name, and return    outliers <- unlist(sapply(1:nrow(cormat),function(i){      R <- cormat[i,-i]      madout <- mad_outliers(R,bend = bend)$Vals      if(length(madout)==0) return(NULL)      names(madout) <- paste0(colnames(cormat)[i],"|",names(madout))            mdn_cor <- median(R) # get median correlation to determine if high or low      names(madout)[which(madout > mdn_cor)] <- paste0(names(madout)[which(madout > mdn_cor)],".~~HIGH~~")      names(madout)[which(madout < mdn_cor)] <- paste0(names(madout)[which(madout < mdn_cor)],".~~LOW~~")      return(madout)    }))    # Now get median correlations so that cutoffs can be displayed    mdn_cor <- apply(cormat + diag(NA,ncol(cormat)),2,median,na.rm=T)    # if no outliers at all:    if(is.null(outliers)){      return(list("N.Outliers"=c("High"=0,"Low"=0),"Median.Cor"=round(mdn_cor,dig)))    }    # Remove duplicate correlations in the list (e.g., X|Y and Y|X)    outliers <- outliers[!duplicated(round(outliers,4))]    # High correlations (above median of each item)    high_cors <- outliers[grep("~~HIGH~~$",names(outliers))]    if(length(high_cors)>0){      high_cors <- sort(high_cors,decreasing = T)      names(high_cors) <- gsub(".~~HIGH~~$","",names(high_cors))    }    # Low correlations (below median of each item)    low_cors <- outliers[grep("~~LOW~~$",names(outliers))]    if(length(low_cors)>0){      low_cors <- sort(low_cors,decreasing = T)      names(low_cors) <- gsub(".~~LOW~~$","",names(low_cors))    }  } else{ # else look for outliers across all correlations (arguably less sensitive)    # Matrix of names (for vector of item pairs)    name_mat <- sapply(1:length(inames),function(i){      sapply(1:length(inames),function(j){        paste0(inames[i],"|",inames[j])      })    })    # Get names lower triangle of matrix    cors <- cormat[lower.tri(cormat)]    mdn_cor <- median(cormat)    names(cors) <- name_mat[lower.tri(cormat)]    # Find outliers    outliers <- mad_outliers(cors,bend = bend)$Vals        # Remove duplicate correlations in the list (e.g., X|Y and Y|X)    outliers <- outliers[!duplicated(outliers)]    # High correlations (above median)    high_cors <- outliers[which(outliers > mdn_cor)]    if(length(high_cors)>0){high_cors <- sort(high_cors,decreasing = T)}    # Low correlations (below median)    low_cors <- outliers[which(outliers < mdn_cor)]    if(length(low_cors)>0){low_cors <- sort(low_cors,decreasing = F)}  }  if(n.outliers){    ## Look at how many high/low outliers from each item (only items with 2+)    high_noutliers <- sort(sapply(inames,function(i){      length(grep(i,names(high_cors)))    }),decreasing=T)    high_noutliers <- high_noutliers[high_noutliers > 1]    if(length(high_noutliers)==0){high_noutliers <- NULL}    low_noutliers <- sort(sapply(inames,function(i){      length(grep(i,names(low_cors)))    }),decreasing=T)    low_noutliers <- low_noutliers[low_noutliers > 1]    if(length(low_noutliers)==0){low_noutliers <- NULL}  } else{    high_noutliers <- low_noutliers <- NULL  }    # How many of each  n.out <- c("High"=length(high_cors),"Low"=length(low_cors))  result <- list("N.Outliers"=n.out,"High.Outliers"=round(high_cors,dig),"Low.Outliers"=round(low_cors,dig),       "Median.Cor"=round(mdn_cor,dig),"N.Outliers.High"=high_noutliers,"N.Outliers.Low"=low_noutliers)  return(result[!sapply(result,function(X){length(X)==0})])}# corPlot_cutoff# Make a correlation plotcorPlot_cutoff <- function(obj,cut=-1,cex.axis=0.5,...){  if(isSymmetric(as.matrix(obj))){    cormat <- obj  } else{    cormat <- cor_auto(obj,verbose=F)  }  corPlot(cormat,xlas=2,cex.axis=cex.axis,zlim=c(cut,1),...)}# Identify Misspecified Parameters in SEM using miPowerFit in semTools# Zack Williams# Procedure outlined in Saris, Satorra, and van der Veld (2009)# See also semTools::miPowerFit documentationsem.misspec <- function(fit,stdLoad=0.4, cor=0.1, stdBeta=0.1, intcept=0.2, stdDelta=NULL,                         delta=NULL, cilevel = 0.90){  # Use miPowerFit on model with parameters entered  pfit <- pf<- miPowerFit(fit,stdLoad=stdLoad, cor=cor, stdBeta=stdBeta, intcept=intcept, stdDelta=stdDelta,                           delta=delta, cilevel = 0.90)  pfit[,c(5:8,13:17)] <- apply(pfit[,c(5:8,13:17)],2,as.numeric)  pfit <- pfit[sort(pfit$mi,decreasing=T,index.return=T)$ix,]    covs <- which(pfit$op == "~~")  loads <- which(pfit$op == "=~")  regs <- which(pfit$op == "~")  ints <- which(pfit$op == "~1")    # Misspecified values by power method  m_covs_pow <- pfit[which(pfit$decision.pow %in% c("EPC:M","M") & pfit$op == "~~"),                     c(1:3,5:7,13:15,12,18)]  m_loads_pow <- pfit[which(pfit$decision.pow %in% c("EPC:M","M") & pfit$op == "=~"),                      c(1:3,5:7,13:15,12,18)]  m_regs_pow <- pfit[which(pfit$decision.pow %in% c("EPC:M","M") & pfit$op == "~"),                     c(1:3,5:7,13:15,12,18)]  m_ints_pow <- pfit[which(pfit$decision.pow %in% c("EPC:M","M") & pfit$op == "~1"),                     c(1:3,5:7,13:15,12,18)]  # Misspecified values by CI method  m_covs_ci <- pfit[which(pfit$decision.ci %in% c("M") & pfit$op == "~~"),                    c(1:3,5:7,13:15,12,18)]  m_loads_ci <- pfit[which(pfit$decision.ci %in% c("M") & pfit$op == "=~"),                     c(1:3,5:7,13:15,12,18)]  m_regs_ci <- pfit[which(pfit$decision.ci %in% c("M") & pfit$op == "~"),                    c(1:3,5:7,13:15,12,18)]  m_ints_ci <- pfit[which(pfit$decision.ci %in% c("M") & pfit$op == "~1"),                    c(1:3,5:7,13:15,12,18)]    nrow(m_ints_ci)  # Overall  s_pow <- summary(factor(gsub("EPC:","",pfit$decision.pow),levels=c("M","NM","I")))  s_pow <- c(s_pow,"%M"=round(100*unname(s_pow[1])/sum(s_pow),2))  s_ci <- summary(factor(pfit$decision.ci,levels=c("M","NM","I")))  s_ci <- c(s_ci,"%M"=round(100*unname(s_ci[1])/sum(s_ci),2))  # Covariances  if(length(covs)>0){    s_pow_cov <- summary(factor(gsub("EPC:","",pfit[covs,]$decision.pow),levels=c("M","NM","I")))    s_pow_cov <- c(s_pow_cov,"%M"=round(100*unname(s_pow_cov[1])/sum(s_pow_cov),2))    s_ci_cov <- summary(factor(pfit[covs,]$decision.ci,levels=c("M","NM","I")))    s_ci_cov <- c(s_ci_cov,"%M"=round(100*unname(s_ci_cov[1])/sum(s_ci_cov),2))  } else{    s_pow_cov <- s_ci_cov <- NULL   }  # Loadings  if(length(loads)>0){    s_pow_load <- summary(factor(gsub("EPC:","",pfit[loads,]$decision.pow),levels=c("M","NM","I")))    s_pow_load <- c(s_pow_load,"%M"=round(100*unname(s_pow_load[1])/sum(s_pow_load),2))    s_ci_load <- summary(factor(pfit[loads,]$decision.ci,levels=c("M","NM","I")))    s_ci_load <- c(s_ci_load,"%M"=round(100*unname(s_ci_load[1])/sum(s_ci_load),2))  } else{    s_pow_load <- s_ci_load <- NULL  }  # Regression weights  if(length(regs)>0){    s_pow_reg <- summary(factor(gsub("EPC:","",pfit[regs,]$decision.pow),levels=c("M","NM","I")))    s_pow_reg <- c(s_pow_reg,"%M"=round(100*unname(s_pow_reg[1])/sum(s_pow_reg),2))    s_ci_reg <- summary(factor(pfit[regs,]$decision.ci,levels=c("M","NM","I")))    s_ci_reg <- c(s_ci_reg,"%M"=round(100*unname(s_ci_reg[1])/sum(s_ci_reg),2))  } else{    s_pow_reg<- s_ci_reg <- NULL  }  # Intercepts  if(length(ints)>0){    s_pow_int <- summary(factor(gsub("EPC:","",pfit[ints,]$decision.pow),levels=c("M","NM","I")))    s_pow_int <- c(s_pow_int,"%M"=round(100*unname(s_pow[1])/sum(s_pow_int),2))    s_ci_int <- summary(factor(pfit[ints,]$decision.ci,levels=c("M","NM","I")))    s_ci_int <- c(s_ci_int,"%M"=round(100*unname(s_ci[1])/sum(s_ci_int),2))  } else{    s_pow_int <- s_ci_int <- NULL  }      summary_mat <- rbind("Pow.all"=s_pow,                       "CI.all"=s_ci,                       "Pow.cov"=s_pow_cov,                       "Pow.load"=s_pow_load,                       "Pow.reg"=s_pow_reg,                       "Pow.int"=s_pow_int,                       "CI.cov"=s_ci_cov,                       "CI.load"=s_ci_load,                       "CI.reg"=s_ci_reg,                       "CI.int"=s_ci_int)      result <- list(Summary=summary_mat,stdLoad=stdLoad, cor=cor, stdBeta=stdBeta,                  intcept=intcept, stdDelta=stdDelta, delta=delta, cilevel = 0.90,                 Full=pf)    if(nrow(m_covs_pow)>0){result$Covariances.pow <- m_covs_pow}  if(nrow(m_covs_ci)>0){result$Covariances.ci <- m_covs_ci}  if(nrow(m_loads_pow)>0){result$Loadings.pow <- m_loads_pow}  if(nrow(m_loads_ci)>0){result$Loadings.ci <- m_loads_ci}  if(nrow(m_regs_pow)>0){result$Regweights.pow <- m_regs_pow}  if(nrow(m_regs_ci)>0){result$Regweights.ci <- m_regs_ci}  if(nrow(m_ints_pow)>0){result$Intercepts.pow <- m_ints_pow}  if(nrow(m_ints_ci)>0){result$Intercepts.ci <- m_ints_ci}    class(result) <- "misspec.list"  return(result)}print.misspec.list <- function(x){  cat("Calculation of misspecified parameters in structural equation model\n")  cat("\nDelta Values:\n")  if(!is.null(x$stdDelta)){    cat(paste(x$stdDelta))    if(length(x$stdDelta == 1)){ cat("for all parameters")}    cat("\n")  } else if(!is.null(x$delta)){    cat(paste(x$delta))    if(length(x$delta == 1)){ cat("for all parameters")}    cat("\n")  }else{    if(length(grep("cov",rownames(x$Summary)))>0){cat(paste(" Covariances:",x$cor,"\n"))}    if(length(grep("load",rownames(x$Summary)))>0){cat(paste(" Loadings:",x$stdLoad,"\n"))}    if(length(grep("reg",rownames(x$Summary)))>0){cat(paste(" Regression weights:",x$stdBeta,"\n"))}    if(length(grep("int",rownames(x$Summary)))>0){cat(paste(" Intercepts:",x$stdBeta,"\n"))}  }    cat("\nParameter Values:\n")  cat(" M = Misspecified (%M = percentage misspecified parameters)\n NM = Not Misspecified\n I = Inconclusive\n")  print(x$Summary)}## as.fa function## Zack Williams 5/22/2018 (last updated 06/01/2020)## Input: loadings matrix + correlation matrix## Output: psych fa object with all typical specifiers## Now does lavaan input for WLSMV/ULSMV factoring - use output of semtools::efaUnrotate as "lavFit"as.fa <- function(loadings, r=NULL,n.obs=NA,cor="cor",rotate=NULL,Phi=NULL,fm="minres",cl=match.call(),                  use="pairwise",scores="tenBerge",bifactor=FALSE,covar=FALSE,lavFit=NULL,rot.mat=NULL,...){      if (is.null(r) & is.null(lavFit)) {    stop("\n\n FATAL ERROR: Either correlation matrix or raw data must be specified")  }   if (is.null(rotate)) {    stop("\n\n FATAL ERROR: Must specify rotation type")  }   # Convert non-numeric data to numeric  if(!is.null(r) & any(!apply(r,2,is.numeric))){    r[] <- apply(r,2,as.numeric)  }    # Takes GPArotation output as input  if("GPArotation" %in% class(loadings)){    gpa <- loadings    loadings <- gpa$loadings    Phi <- gpa$Phi    rot.mat <- gpa$Th  }  dims <- dim(loadings)  nfactors <- dims[2]    # Do a factor analysis using the settings that were input with no rotation  if(!(fm %in% c("wlsmv","WLSMV","ulsmv","ULSMV","dwls","DWLS","uls","ULS"))){    f <- suppressWarnings(fa(r,nfactors = nfactors,n.obs = n.obs,rotate="none",warnings=FALSE,fm=fm,use=use,cor=cor,...))    r <- f$r # if r not a correlation matrix, make it one  } else{    # In the case where you've provided an EFA from lavaan    if(is.null(lavFit)) {      stop("\n\n FATAL ERROR: Lavaan EFA fit object must be specified with that value of fm")    }    r <- lavInspect(lavFit,"sampstat")$cov    fit.stats <- fitMeasures(lavFit)  }  if(nfactors < 1) {    stop("\n\n FATAL ERROR: Invalid number of factors specified.")  }    signed <- sign(colSums(loadings))  signed[signed==0] <- 1  if(nfactors > 1){    loadings <- loadings %*% diag(signed)  #flips factors to be in positive direction but loses the colnames    if(!is.null(Phi)){      Phi <- diag(signed) %*% Phi %*% diag(signed) # Changes factor correlation matrix to mirror flipped factors    }  } else{ # for 1-factor case    loadings <- loadings * signed  }    if(!is.null(Phi)){    model <- loadings %*% Phi %*% t(loadings)  } else { # Oblique case    model <- loadings %*% t(loadings)  }    switch(fm,          alpha = {colnames(loadings) <- paste("alpha",1:nfactors,sep='')},          wls={colnames(loadings) <- paste("WLS",1:nfactors,sep='')	},         pa= {colnames(loadings) <- paste("PA",1:nfactors,sep='')} ,         gls = {colnames(loadings) <- paste("GLS",1:nfactors,sep='')},         ml = {colnames(loadings) <- paste("ML",1:nfactors,sep='')},          minres = {colnames(loadings) <- paste("MR",1:nfactors,sep='')},         minrank = {colnames(loadings) <- paste("MRFA",1:nfactors,sep='')},         uls =  {colnames(loadings) <- paste("ULS",1:nfactors,sep='')},         old.min = {colnames(loadings) <- paste0("oldmin",1:nfactors)},         minchi = {colnames(loadings) <- paste("MC",1:nfactors,sep='')},         wlsmv = {colnames(loadings) <- paste("DWLS",1:nfactors,sep='')},         ulsmv = {colnames(loadings) <- paste("ULS",1:nfactors,sep='')})  if(rotate %in% c("schmid","bifadFR","sli","bifactor","biquartimin") | bifactor==TRUE){    colnames(loadings) <- c("G",paste0("GRP",1:(nfactors-1)))    bifactor <- TRUE  }      rownames(loadings) <- colnames(r)  class(loadings) <- "loadings"  if(nfactors < 1) nfactors <- n  if(max(abs(loadings) > 1.0) && !covar) warning(' A loading greater than abs(1) was detected.  Examine the loadings carefully.')   result <- suppressWarnings(psych::factor.stats(r,loadings,Phi,n.obs=n.obs))   #do stats as a subroutine common to several functions  # Change Chi-square if using lavaan object  if(!is.null(lavFit)){    result$STATISTIC <- fit.stats["chisq.scaled"]    result$PVAL <- fit.stats["pvalue.scaled"]    result$CFI <- fit.stats["cfi.scaled"]    result$TLI <- fit.stats["tli.scaled"]    result$objective <- fit.stats["fmin"]    result$RMSEA <- c(fit.stats["rmsea.scaled"],fit.stats["rmsea.ci.lower.scaled"],                      fit.stats["rmsea.ci.upper.scaled"],0.9)    names(result$RMSEA) <- c("RMSEA","lower","upper","confidence")    result$null.chisq <- fit.stats["baseline.chisq.scaled"]    result$null.model <- fit.stats["baseline.chisq.scaled"]/(n.obs-1)    result$BIC <- result$STATISTIC^2 - 2*result$dof  }  result$rotation <- rotate  result$communality <- diag(model)  if(max(result$communality > 1.0) && !covar) warning("An ultra-Heywood case was detected.  Examine the results carefully")    result$uniquenesses <- diag(r-model)  result$values <-  eigen(model)$values  result$e.values <- eigen(r)$values  result$loadings <- loadings  result$model <- model  #diag(result$model) <- diag(r)   result$fm <- fm  #remember what kind of analysis we did  if(!is.null(Phi)){    colnames(Phi)  <- rownames(Phi) <- colnames(loadings) #added 2/14/16 to help with fa.multi    result$Phi <- Phi      #the if statement was incorrectly including oblique.scores.  Fixed Feb, 2012 following a report by Jessica Jaynes    Structure <- loadings %*% Phi    if(is.null(rot.mat) & rotate != "none"){      result$rot.mat <- targetQ(f$loadings,Target = loadings)$Th    } else{      result$rot.mat <- rot.mat    }  } else {    Structure <- loadings    if(is.null(rot.mat) & rotate != "none"){      result$rot.mat <- targetT(f$loadings,Target = loadings)$Th    } else{      result$rot.mat <- rot.mat    }  }  class(Structure) <- "loadings"  result$Structure <- Structure #added December 12, 2011       #Some of the Grice equations use the pattern matrix, but some use the structure matrix  #we are now dropping this oblique score option  (9/2/17)  result$method=scores  #this is the chosen method for factor scores  result$scores <- factor.scores(r,f=loadings,Phi=NULL,method=scores) # always oblique factor scores (except if orthogonal model)  if(is.null( result$scores$R2))  result$scores$R2 <- NA  result$R2.scores <- result$scores$R2    result$weights <- result$scores$weights    #these are the weights found in factor scores and will be different from the ones reported by factor.stats   result$scores <- result$scores$scores  if(!is.null(result$scores)) colnames(result$scores) <- colnames(loadings) #added Sept 27, 2013  result$factors <- nfactors   result$r <- r   #save the correlation matrix   result$fn <- "fa"  if(bifactor==TRUE){    result$"I-ECV" <- loadings[,1]^2/result$communality    result$ECV <- sum(loadings[,1]^2)/sum(loadings[]^2)  }    #Find the summary statistics of Variance accounted for  #normally just found in the print function  (added 4/22/17)  #from  the print function  if(is.null(Phi)) {    if(nfactors > 1)  {      vx <- colSums(loadings^2)     } else {      vx <- sum(loadings^2)    }  } else {    vx <- diag(Phi %*% t(loadings) %*% loadings)  }    vtotal <- sum(result$communality + result$uniquenesses)  names(vx) <- colnames(loadings)  varex <- rbind("SS loadings" =   vx)  varex <- rbind(varex, "Proportion Var" =  vx/vtotal)  if (nfactors > 1) {    varex <- rbind(varex, "Cumulative Var"=  cumsum(vx/vtotal))    varex <- rbind(varex, "Proportion Explained"=  vx/sum(vx))    varex <- rbind(varex, "Cumulative Proportion"=  cumsum(vx/sum(vx)))   }    result$Vaccounted <- varex  result$Call <- cl  if(is.null(result$Phi)){result$Phi <- diag(result$factors)}  class(result) <- c("psych", "fa")  return(result)}### Empirically Refined Target Matrix Rotation (RETAM; Lorenzo-Seva & Ferrando, 2020)### Zack Williams, 06/07/2020## Based on: Lorenzo-Seva, U., & Ferrando, P. J. (2020). ## Unrestricted factor analysis of multidimensional test items based on an objectively ## refined target matrix. Behavior Research Methods, 52(1), 116-130.## https://doi.org/10.3758/s13428-019-01209-1 #### Requires specification of initial partially-specified target matrix (Tmat; made of all 0s or NAs)## This matrix is then refined using the Promin criteria, and target rotation is carried out again## Algorithm proceeds iteratively until the same target rotation is utilized twice in a row. # loadmat - unrotated loadings matrix# Tmat - target matrix; loadmat - matrix of unrotated loadings# method = Complete Refinement ("CR")/Make Simple ("MS")/Make Complex ("MC") - see original paper for details# max.iter = 100; automatically stops at 100 iterations# return.fa = return a psych 'fa' type object base on the as.fa function. # return.fa requires either data or correlation matrix input (r argument)# ... extra provided input to the as.fa functionRETAM <- function(loadmat,Tmat,method=c("CR","MS","MC"),max.iter=100,return.fa=FALSE,r=NULL,cor="cor",...){  # Error catching  if(!all(dim(loadmat)==dim(Tmat))){    stop("Loading matrix and Target matrix do not have the same dimensions! Check input matrices for discrepancies.")  }  if(!method[1] %in% c("CR","MS","MC")){    stop(paste0("Method is not recognized. Possible options are 'CR' for complete refinement (default), ",                "'MS' for make simple (only remove loadings from Tmat), and 'MC' for ",                "make complex (only add loadings to Tmat)."))  }  if(!all(Tmat %in% c(0,NA))){    stop("Values in Tmat must be either '0' (loading fixed to 0) or 'NA' (loading estimated).")  }  # Define internal variables  nr <- nrow(loadmat)  nc <- ncol(loadmat)  iter_done <- FALSE  n_iter <- 1  tmat_list <- list()  # Meat of the function: loop of target rotation refinement  while(iter_done==FALSE){ # Keep running until iter_done is true (convergence)    tmat_list[[n_iter]] <- Tmat # add Tmat to list    # Conduct (oblique) target rotation using Tmat     t_rot <- psych::TargetQ(loadmat,Target=list(Tmat))    B <- t_rot$loadings # Save loadings (B matrix by Lorenzo-Seva & Ferrando Notation)    # normalize rows of matrix    V <- diag(diag(B %*% t(B))^(-1/2)) # normalization matrix    C2 <- (V %*% B)^2 # C = normalized loadings matrix; C2 is that matrix squared    # Go through C^2 matrix, compare values to Promin threshold (col mean + col SD/4)    Tmat <- sapply(1:nc,function(j){      cj <- C2[,j]      vj <- mean(cj) # column mean      sj <- sd(cj) # column SD      tj <- vj + sj/4 # column threshold            out_col <- ifelse(cj > tj,NA,0) # complete refinement      # Restrict matrix for methods = "MS" or "MC"      if(method[1]=="MC"){ # Make complex - i.e., keep all original NA values        tmat_j <- tmat_list[[1]][,j]        out_col[which(is.na(tmat_j))] <- NA      } else if(method[1]=="MS"){ # Make simple - i.e., keep all original 0 values        tmat_j <- tmat_list[[1]][,j]        out_col[which(tmat_j==0)] <- 0      }      return(out_col)    })    # Check to see if converged    if(all(is.na(Tmat)==is.na(tmat_list[[n_iter]]))){ # convergence achieved      iter_done <- TRUE      message(paste0("RETAM algorithm converged after ",n_iter," iterations.\n"))    } else if(n_iter==max.iter){      stop(paste0("Maximum iterations achieved without convergence.",                  " Increase max.iter beyond ",max.iter," or specify a different target matrix."))    } else{ # need to iterate more      n_iter <- n_iter + 1    }  }  # Once convergence has been achieved, modify the solution slightly  # flip sign of factors so most salient loadings are positive  signed <- ifelse(sign(colSums(B))>=0,1,-1)  t_rot$loadings <- B %*% diag(signed)  t_rot$Phi <- diag(signed) %*% t_rot$Phi %*% diag(signed)  t_rot$Th <- t_rot$Th %*% diag(signed)  # Add in factor names  if(!is.null(colnames(B))){    colnames(t_rot$loadings) <- colnames(t_rot$Phi) <- rownames(t_rot$Phi) <- colnames(B)  } else {    colnames(t_rot$loadings) <- colnames(t_rot$Phi) <- rownames(t_rot$Phi) <- paste0("F",1:nc)  }    if(return.fa==T){ # transform into fa object using as.fa. ... arguments go here    if(is.null(r)){      message("No correlation matrix or raw data specified. Returning GPArotation object instead.")      return(t_rot)    }     return(as.fa(t_rot$loadings,Phi=t_rot$Phi,rot.mat = t_rot$Th,r = r,cor=cor,...))  } else{ # otherwise return GPArotation object    return(t_rot)  }}# fa.RETAM - uses empirical rotation (default: geominQ) to create target RETAM function# Zack Williams, 06/08/2020# Note that only oblique target rotations are supported at the moment. For bifactor rotations, use SLi function.# See above RETAM function for more details# R = data or correlation matrix, n.obs = number of observations (used with correlation matrix input)# fm = factor method (also supports "wlsmv" and "ulsmv" extraction using lavaan)# rotate = rotation (default rotation geomin - used for constructing Promin-based target matrix)# cor = correlation type (used as input to "fa")# ... other arguments to "fa" function.fa.RETAM <- function(R,n.obs=NA,nfactors=NULL,fm="minres",rotate="geomin",cor="poly",method="CR",Target=NULL,...){  # Error Throwing  if(is.null(nfactors)){    if(!is.null(Target)){      nfactors <- ncol(Target)    } else{      stop("Number of factors not specified!")    }  } else if(nfactors==1){    stop("Target rotation not meaningful for one-factor solution. Specify a greater number of factors or use 'fa' function.")  }  # allow for isSymmetric function for dataframes  isSymmetric.data.frame <- function(df){    return(isSymmetric(as.matrix(df)))  }    # Assuming factor method is something covered by 'fa' function  if(fm %in% c("dwls","DWLS","wlsmv","WLSMV","ordinal")){ # DWLS extraction    if(isSymmetric(R)) stop("ERROR: Full data required for WLSMV factor analysis.")    fm <- "wlsmv"    fa.obj <- efa.dwls(x=R,nfactors = nfactors,rotate="none",...)  } else if(fm %in% c("uls","ULS","ulsmv","ULSMV")){ # Ordinal ULS extraction    fm <- "ulsmv"    if(isSymmetric(R)) stop("ERROR: Full data required for ULSMV factor analysis.")    fa.obj <- efa.dwls(x=R,nfactors = nfactors,rotate="none",estimator="ULSMV",...)  } else{ # All other rotations (uses fa function)    fa.obj <- suppressWarnings(psych:::fa (r = R,n.obs=n.obs,cor=cor,nfactors = nfactors,rotate ="none",fm = fm,...))  }    unrotated_l <- fa.obj$loadings[]  # if rotated function ends in Q, remove it for use in GPFoblq  # Save rotated loadings (B matrix by Lorenzo-Seva & Ferrando Notation)  if(!is.null(Target) | rotate %in% c("targetQ","target","TargetQ","Target","TargetT","targetT","RETAM","retam")){ # target rot    if(is.null(Target)){      message("Target rotation selected but no 'Target' matrix provided. Reverting to geomin rotation.")      rotate <- "geomin"      B <- GPFoblq(unrotated_l,method = "geomin",maxit=10000)$loadings    } else{      rotate <- "target"      if(!all(dim(Target)==dim(unrotated_l))){        stop("Target matrix dimensions are incorrect. Check the number of factors you meant to extract.")      }      B <- targetQ(unrotated_l,Target=Target,maxit=10000)$loadings    }  } else{    B <- GPFoblq(unrotated_l,method = gsub("[QT]$","",rotate),maxit=10000)$loadings  }  # normalize rows of matrix  V <- diag(diag(B %*% t(B))^(-1/2)) # normalization matrix  C2 <- (V %*% B)^2 # C = normalized loadings matrix; C2 is that matrix squared  # Go through C^2 matrix, compare values to Promin threshold (col mean + col SD/4)  Tmat.emp <- sapply(1:ncol(B),function(j){    cj <- C2[,j]    vj <- mean(cj) # column mean    sj <- sd(cj) # column SD    tj <- vj + sj/4 # column threshold    out_col <- ifelse(cj > tj,NA,0) # complete refinement    return(out_col)  })    out.RETAM <- RETAM(loadmat = unrotated_l,Tmat = Tmat.emp,method = method,cor=cor,                     return.fa = T,r = R,lavFit=fa.obj$Lavaan,fm=fm,rotate="RETAM")  fa.obj$FitIndices$obj.name <- paste0("efa.",fm,".RETAM")  out.RETAM$FitIndices <- fa.obj$FitIndices  return(out.RETAM)}## Compute orthogonal Procrustes rotationProcrustes <-function(M1, M2){  tM1M2 <- t(M1) %*% M2  svdtM1M2 <- svd(tM1M2)  P <- svdtM1M2$u  Q <- svdtM1M2$v  T <- Q %*% t(P)  ## Orthogonally rotate M2 to M1  M2 %*% T}# Schmid-Leiman Transformation of loading matrix# Utilizes either the classic or direct bifactor (BiFAD; Waller, 2017) methods# Zack Williams 01/25/2019SL <- function(loadmat, Phi=NULL, rotate="geomin",fm="minres",BiFAD=F,salient = .20,B=NULL,...){  cl <- match.call()  # If no correlation matrix, assume orthogonal factors  if(is.null(Phi)){    message("No factor correlation matrix provided. Assuming orthogonal loadings and rotating with method = ",rotate,".")    ob <- GPFoblq(loadmat,method=rotate,maxit=5000,...)    Phi <- ob$Phi    ortho <- loadmat    loadmat <- ob$loadings  }    # This code adapted from psych "schmid" procedure  model <- as.matrix(loadmat)  Phi <- as.matrix(Phi)  fact <- model %*% Phi  #find the orthogonal (structure) matrix from the oblique pattern and the Phi matrix  orth.load <- fact   #but this is not the correct factor  solution  ev <- eigen(Phi)  orth.load <- model %*% ev$vector %*% sqrt(diag(ev$values))  colnames(orth.load) <- colnames(Phi)  nfactors <- dim(fact)[2]  fload <- model  factr <- Phi  model <- fload %*% Phi %*% t(fload)  diag(model) <- 1    # 2 factor case: force direct bifactor solution  if(nfactors==2){    BiFAD <- T    message("Two group factors detected: Using direct bifactor rotation.")  }    # Throws error if only 1 factor  if (nfactors ==1) {    gload <- c(1)    stop("Cannot perform Schmid-Leiman with only one group factor")  } else if(BiFAD==T){ # 2-factor case or direct bifactor    gload <- NULL    L0 <- cbind(orth.load,0)        ## Create 0/1 Target matrix if B not supplied    Bflag <- 1 # set to 1 if Target matrix (B) is known    if(is.null(B)) Bflag <- 0    if(!Bflag & (!is.null(nfactors))){      B <- loadmat      ## Record signs of loadings      signB <- sign(B)      ## Convert Target matrix into signed 0/1 matrix      B <- signB* matrix(as.numeric(abs(B) >= salient),                         nrow(L0), ncol = nfactors)      ## Append ones vector for general factor      B <- cbind(1,B)    } ## END Create 0/1 Target matrix        ## Rotate rank deficient loading matrix to Target    BstarSL <- Procrustes(B, L0)    # Make sure that group factor signs all in right direction    signed <- sign(colSums(BstarSL))    signed[signed==0] <- 1    BstarSL <- BstarSL %*% diag(signed)         gprimaryload <- as.matrix(BstarSL[,1])        colnames(gprimaryload) <- "g"    h2 <- diag(orth.load %*% t(orth.load))     # u2 <- 1 - h2        u2 <- diag(model) - h2                         uniq <- diag(model)- fload^2        g.percent <- gprimaryload^2/h2    colnames(g.percent) <- "p2"        sm <-  BstarSL[,-1]  } else { # if (nfactors>2 & direct==F)    colnames(factr) <- rownames(factr) <- paste("F",1:nfactors,sep="")  #make it a vector    gfactor <- suppressWarnings(fa(factr,fm=fm))   #The first factor of the factor intercorrelation matrix    gload <- loadings(gfactor,h2=F)     gprimaryload <- fload %*% gload        colnames(gprimaryload) <- "g"    h2 <- diag(orth.load %*% t(orth.load))     # u2 <- 1 - h2        u2 <- diag(model) - h2                         uniq <- diag(model)- fload^2    guniq <- diag(model) - gprimaryload^2    #Ig <- matrix(0, ncol = nfactors, nrow = nfactors)    #diag(Ig) <- gload    Ig <- diag(drop(gload))   #3/5/11    primeload <- fload %*% Ig    g.percent <- gprimaryload^2/h2    colnames(g.percent) <- "p2"    uniq2 <- diag(model) - uniq - primeload^2    uniq2[uniq2<0] <- 0    sm <-  sign(fload) * sqrt(uniq2)  #added June 1, 2010 to correctly identify sign of group factors  }  colnames(sm) <- paste0("F",1:nfactors,"*")  result <- list(sl = cbind(gprimaryload, sm,h2, u2,p =g.percent), orthog = orth.load,                 oblique=fload,phi =factr, gloading = gload, Direct=BiFAD, Tmat=B, Call=cl)    class(result) <- c("psych" ,"schmid")  return(result)}### NEST Next Eigenvalue Sufficiency Tests (NEST; Code from Achim, 2017; https://doi.org/10.20982/tqmp.13.1.p064)# Added by Zack Williams 11/10/22# TODO: Make pretty output for this functionNEST <- function(data, N=0, nrep=1000, alpha=.05, converg=.00001, maxIter=500){  varnames<-colnames(data)  data<-as.matrix(data)  ns<-dim(data)[1]  nv<-dim(data)[2]  if(ns==nv)  {    if(N==0) stop("Error: 'N=' must be specified with a correlation matrix")     ns<-N  }  else data<-cor(data)  eig<-eigen(data,symmetric=TRUE,only.values=TRUE)  lim<-floor(.8*nv)  s<-""  note<-list()  nt<-""  for (nf in 0:lim)  {    if(nf==2) s<-"s"    nf1<-nf+1    rnk<-rep(1,nf1)    if (nf==0) model<-diag(rep(1,nv))    else    # prepare the PAF model    {      R<-data      pre<-diag(data)      for (jj in 1:maxIter)   # maximum nb of iterations      {        ei<-eigen(R,symmetric=TRUE)        if (nf==1)        {          ld<-ei$vectors[,1] * sqrt(ei$values[1])          commun<-ld*ld        }        else        {          ff<-nf          while (ei$value[ff]<=0) {ff<-ff-1}          ld<-ei$vectors[,1:ff] %*% diag(sqrt(ei$values[1:ff]))          commun<-rowSums(ld*ld)        }        if (max(commun)>1)        {          R<- R+diag(pre-diag(R))          cat("prepModel: Warning! Communality overflow prevented in",nf,"factor model\n")          break        }        diff<-max(abs(pre-commun))        if (diff<converg) break        R<- R+diag(commun-pre)        pre<-commun      }      if (jj>=maxIter)      {        nt<-paste("  Warning! Convergence not complete for the",nf,"factor model (",diff,">=",converg,")")      }      else nt<-""      model<-t(cbind(ld,diag(sqrt(1-commun))))    }    nr<-nf+nv    for (j in 1:nrep)    {      ei<-eigen(cor(matrix(rnorm(ns*nr),nrow=ns,ncol=nr) %*% model),symmetric=TRUE,only.values=TRUE)      rnk<-rnk+(ei$values[1:nf1]>=eig$values[1:nf1])    }    note1<-paste("H0: ", nf, " factor", s,": ",sep="")    note2<-paste(as.character(rnk), collapse=" ")    note1<-paste(note1,note2)    if (rnk[nf1]> alpha*(nrep+1)) verdict<-"accepted" else verdict<-"rejected"    note1<-paste(note1," (/",nrep+1,") H0 ",verdict,nt,sep="")    cat(note1,"\n")    note[nf+1]<-note1    if (rnk[nf1]> alpha*(nrep+1))    {      loadings<-data.frame(t(model[1:nf,]))      break    }  }  rownames(loadings)<-varnames  colnames(loadings)<-paste("F",1:nf,sep="")  out<-list(ncases=ns,nfactors=nf,note=note,loadings=loadings)	  return(out)}### CFA.1f - quick function to fit 1-factor model in lavaan (ordinal WLSMV by default)## Zack Williams, 05/11/2020# X = data set or correlation matrix# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.CFA.1f <- function(X,n.obs=NULL,ordered=TRUE,estimator=NULL,std.lv=TRUE,missing=ifelse(length(ordered)>1 | any(TRUE %in% ordered),"pairwise","fiml"),                   factor.name="F",more=NULL,...){  # allow for isSymmetric function for dataframes  isSymmetric.data.frame <- function(df){    return(isSymmetric(as.matrix(df)))  }  # Lavaan model syntax  MOD_1F <- paste0(paste(factor.name,"=~",paste(colnames(X),collapse = " + ")),"\n",more)  # Sort out what sort of input you have and fit appropriately  if(isSymmetric(X)){ # Cor/Cov matrix input - note ordered must be false    if(length(ordered)>1 | any(TRUE %in% ordered)) stop("Ordered data not usable with covariance matrix input. Change option 'ordered' to 'FALSE'")    if(is.null(n.obs)) stop("Number of observations needed with covariance matrix input.")    # Assuming it has everything you need, fit the model    if(is.null(estimator)){ # ML by default      result <- lavaan::cfa(model=MOD_1F,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator="ML",...)    } else{      result <- lavaan::cfa(model=MOD_1F,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator=estimator,...)    }  } else{ # Raw data input    if(is.null(estimator)){ # WLSMV by default      if(any(ordered==F)){        result <- lavaan::cfa(model=MOD_1F,data=X,std.lv=std.lv,missing=missing,estimator="MLMV",...)      } else{        result <- lavaan::cfa(model=MOD_1F,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator="WLSMV",...)      }    } else{      result <- lavaan::cfa(model=MOD_1F,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator=estimator,...)    }  }  result@Cache$ModelSyntax <- MOD_1F  return(result)}### CFA.bifactor - quick function to fit confirmatory bifactor model in lavaan (ordinal WLSMV by default)## Zack Williams, 06/01/2020# X = data set or correlation matrix# spec is vector of specific factor loadings. Can include NA for just general factor loads. like mirt::bfactor# If you want something more than one factor per variable supported, provide spec as a list# each item of the spec list should include a numeric vector with all items that load on that specific factor# The factor names will be taken from the list (if applicable), or the 'spec.names' argument# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.CFA.bifactor <- CFA.bf <- function(X,spec=NULL,more=NULL,n.obs=NULL,ordered=TRUE,estimator=NULL,std.lv=TRUE,                                   missing="pairwise",g.factor.name="G",spec.names=NULL,...){  # allow for isSymmetric function for dataframes  isSymmetric.data.frame <- function(df){    return(isSymmetric(as.matrix(df)))  }  # Parse data on specific factors (either list or vector input)  if(is.vector(spec) & !is.list(spec)){ # vector input     if(!is.numeric(spec)){stop("'spec' must be a numeric vector or list of specific factor assignments.")}    if(length(spec)!=ncol(X)){      stop("'spec' values must be specified for all variables. Use 'NA' to have a variable not load on the general factor")    }    spec <- as.numeric(as.factor(spec)) # if numbers not consecutive or started at 2, fix that.    n.spec <- max(spec,na.rm = T) # how many specific factors?    spec_list <- lapply(1:n.spec,function(fac){which(spec==fac)})  } else if("list" %in% class(spec)){ # list input    if(is.null(spec.names) & !is.null(names(spec))){spec.names <- names(spec)} # get factor names from list    n.spec <- length(spec)    if(!is.numeric(spec[[1]])){stop("'spec' must be a numeric vector or list of specific factor assignments.")}    spec_list <- lapply(spec,as.numeric)  } else{    stop("'spec' must be a numeric vector or list of specific factor assignments.")  }  # Fix spec_list and get rid of any empty entries  empty_vals <- which(unlist(lapply(spec_list,function(x){length(x)==0})))  if(length(empty_vals)>0){spec_list <- spec_list[-empty_vals]}  # Now take the specific factor list and turn it into a lavaan model  spec_mod <- lapply(1:length(spec_list),function(s){    if(!is.null(spec.names)){      fname <- spec.names[s]    } else{      fname <- paste0("S",s)    }    s_items <- spec_list[[s]]    return(paste0("\n",paste(fname,"=~",paste(colnames(X)[s_items],collapse = " + "))))  })  # Lavaan model syntax  MOD_BF <- paste0(paste(g.factor.name,"=~",paste(colnames(X),collapse = " + ")),                   paste(spec_mod,collapse=""),"\n",more)  # Sort out what sort of input you have and fit appropriately  if(isSymmetric(X)){ # Cor/Cov matrix input - note ordered must be false    if(length(ordered)>1 | any(TRUE %in% ordered)) stop("Ordered data not usable with covariance matrix input. Change option 'ordered' to 'FALSE'")    if(is.null(n.obs)) stop("Number of observations needed with covariance matrix input.")    # Assuming it has everything you need, fit the model    if(is.null(estimator)){ # ML by default      result <- lavaan::cfa(model=MOD_BF,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator="ML",orthogonal=T,...)    } else{      result <- lavaan::cfa(model=MOD_BF,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator=estimator,orthogonal=T,...)    }  } else{ # Raw data input    if(is.null(estimator)){ # WLSMV by default (or MLMV if continuous)      if(ordered==F){        result <- lavaan::cfa(model=MOD_BF,data=X,std.lv=std.lv,missing=missing,estimator="MLMV",orthogonal=T,...)      } else{        result <- lavaan::cfa(model=MOD_BF,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator="WLSMV",orthogonal=T,...)      }    } else{      result <- lavaan::cfa(model=MOD_BF,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator=estimator,orthogonal=T,...)    }  }  result@Cache$ModelSyntax <- MOD_BF  return(result)}### CFA.oblique - quick function to fit confirmatory correlated-factors model in lavaan (ordinal WLSMV by default)## Zack Williams, 06/02/2020# X = data set or correlation matrix# spec is vector of specific factor loadings.# If you want something more than one factor per variable supported, provide spec as a list# each item of the spec list should include a numeric vector with all items that load on that specific factor# The factor names will be taken from the list (if applicable), or the 'spec.names' argument# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.CFA.oblique <- CFA.ob <- function(X,spec=NULL,n.obs=NULL,ordered=TRUE,estimator=NULL,std.lv=TRUE,                                    missing="pairwise",factor.names=NULL,more=NULL,...){  # allow for isSymmetric function for dataframes  isSymmetric.data.frame <- function(df){    return(isSymmetric(as.matrix(df)))  }  # Parse data on specific factors (either list or vector input)  if(is.vector(spec) & !is.list(spec)){ # vector input     if(!is.numeric(spec)){stop("'spec' must be a numeric vector or list of factor assignments.")}    if(length(spec)!=ncol(X)){      stop("'spec' values must be specified for all variables.")    }    spec <- as.numeric(as.factor(spec)) # if numbers not consecutive or started at 2, fix that.    n.fac <- max(spec,na.rm = T) # how many specific factors?    spec_list <- lapply(1:n.fac,function(fac){which(spec==fac)})  } else if("list" %in% class(spec)){ # list input    if(is.null(factor.names) & !is.null(names(spec))){factor.names <- names(spec)} # get factor names from list    n.spec <- length(spec)    if(!is.numeric(spec[[1]])){stop("'spec' must be a numeric vector or list of specific factor assignments.")}    spec_list <- lapply(spec,as.numeric)  } else{    stop("'spec' must be a numeric vector or list of specific factor assignments.")  }  # Fix spec_list and get rid of any empty entries  empty_vals <- which(unlist(lapply(spec_list,function(x){length(x)==0})))  if(length(empty_vals)>0){spec_list <- spec_list[-empty_vals]}  # Now take the specific factor list and turn it into a lavaan model  spec_mod <- lapply(1:length(spec_list),function(s){    if(!is.null(factor.names)){      fname <- factor.names[s]    } else{      fname <- paste0("F",s)    }    s_items <- spec_list[[s]]    return(paste0("\n",paste(fname,"=~",paste(colnames(X)[s_items],collapse = " + "))))  })  # Lavaan model syntax  MOD_OBLQ <- paste0(paste(spec_mod,collapse=""),"\n",more)  # Sort out what sort of input you have and fit appropriately  if(isSymmetric(X)){ # Cor/Cov matrix input - note ordered must be false    if(length(ordered)>1 | any(TRUE %in% ordered)) stop("Ordered data not usable with covariance matrix input. Change option 'ordered' to 'FALSE'")    if(is.null(n.obs)) stop("Number of observations needed with covariance matrix input.")    # Assuming it has everything you need, fit the model    if(is.null(estimator)){ # ML by default      result <- lavaan::cfa(model=MOD_OBLQ,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator="ML",...)    } else{      result <- lavaan::cfa(model=MOD_OBLQ,sample.cov=X,sample.nobs=n.obs,std.lv=std.lv,estimator=estimator,...)    }  } else{ # Raw data input    if(is.null(estimator)){ # WLSMV by default (or MLMV if continuous)      if(ordered==F){        result <- lavaan::cfa(model=MOD_OBLQ,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator="MLMV",...)      } else{        result <- lavaan::cfa(model=MOD_OBLQ,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator="WLSMV",...)      }    } else{      result <- lavaan::cfa(model=MOD_OBLQ,data=X,std.lv=std.lv,ordered=ordered,missing=missing,estimator=estimator,...)    }  }  result@Cache$ModelSyntax <- MOD_OBLQ  return(result)}# Do a WLSMV EFA using SEMtools# Zack Williams# Updated 05/11/2020# Now will use CFA.1f if 1-factor model (eliminates the constraints of efaUnrotate)efa.dwls <- fa.dwls <- function(x,nfactors=1,rotate="geominQ",...){    message("Computing lavaan EFA. This may take a while for large datasets.\n")    n.obs <- nrow(x)  if(nfactors==1){    fit <- CFA.1f(x,nf=nfactors,ordered=T,...)  } else{    fit <- semTools::efaUnrotate(x,nf=nfactors,ordered=T,...)  }  loadings <- as.matrix(lavInspect(fit,"std")$lambda)    if(nfactors > 1){    if (rotate=="varimax" |rotate=="Varimax" | rotate=="quartimax" | rotate =="bentlerT" | rotate =="geominT" | rotate =="targetT" | rotate =="bifactor"   | rotate =="TargetT"|rotate =="geominQrevised"|        rotate =="equamax"| rotate =="varimin"|rotate =="specialT" | rotate =="Promax"  | rotate =="promax"| rotate =="cluster" |rotate == "biquartimin" |rotate == "TargetQ"  |rotate =="specialQ") {      Phi <- NULL       switch(rotate,  #The orthogonal cases  for GPArotation + ones developed for psych             varimax = {rotated <- stats::varimax(loadings)  #varimax is from stats, the others are from GPArotation              loadings <- rotated$loadings             rot.mat <- rotated$rotmat},             Varimax = {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}               #varimax is from the stats package, Varimax is from GPArotations               #rotated <- do.call(rotate,list(loadings))               #rotated <- do.call(getFromNamespace(rotate,'GPArotation'),list(loadings))               rotated <- GPArotation::Varimax(loadings)               loadings <- rotated$loadings               rot.mat <- t(solve(rotated$Th))} ,             quartimax = {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}                              #rotated <- do.call(rotate,list(loadings))               rotated <- GPArotation::quartimax(loadings)               loadings <- rotated$loadings               rot.mat <- t(solve(rotated$Th))} ,             bentlerT =  {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}                              #rotated <- do.call(rotate,list(loadings))               rotated <- GPArotation::bentlerT(loadings)               loadings <- rotated$loadings               rot.mat <- t(solve(rotated$Th))} ,             geominT	= {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}                              #rotated <- do.call(rotate,list(loadings))               rotated <- GPArotation::geominT(loadings)               loadings <- rotated$loadings               rot.mat <- t(solve(rotated$Th))} ,             targetT = {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}               rotated <- GPArotation::targetT(loadings,Tmat=diag(ncol(loadings)))               loadings <- rotated$loadings               rot.mat <- t(solve(rotated$Th))} ,                          bifactor = {rot <- bifactor(loadings)             loadings <- rot$loadings             rot.mat <- t(solve(rot$Th))},               TargetT =  {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}               rot <- GPArotation::targetT(loadings,Tmat=diag(ncol(loadings)))               loadings <- rot$loadings               rot.mat <- t(solve(rot$Th))},             geominQrevised =  {if (!requireNamespace('GPArotation')) {stop("I am sorry, to do this rotation requires the GPArotation package to be installed")}               rot <- GPArotation::geominQ(loadings,Tmat=diag(ncol(loadings)),delta=.5)               loadings <- rot$loadings               rot.mat <- t(solve(rot$Th))               Phi <- rot$Phi},             equamax =  {rot <- equamax(loadings)             loadings <- rot$loadings             rot.mat <- t(solve(rot$Th))},              varimin = {rot <- varimin(loadings)             loadings <- rot$loadings             rot.mat <- t(solve(rot$Th))},             specialT =  {rot <- specialT(loadings)             loadings <- rot$loadings             rot.mat <- t(solve(rot$Th))},              Promax =   {pro <- Promax(loadings)  #Promax without Kaiser normalization             loadings <- pro$loadings             Phi <- pro$Phi              rot.mat <- pro$rotmat},             promax =   {#pro <- stats::promax(loadings)   #from stats               pro <- kaiser(loadings,rotate="Promax")   #calling promax will now do the Kaiser normalization before doing Promax rotation               loadings <- pro$loadings               rot.mat <- pro$rotmat               # ui <- solve(rot.mat)               # Phi <-  cov2cor(ui %*% t(ui))               Phi <- pro$Phi              },	             cluster = 	 {loadings <- varimax(loadings)$loadings           			             pro <- target.rot(loadings)             loadings <- pro$loadings             Phi <- pro$Phi             rot.mat <- pro$rotmat},             biquartimin =    {ob <- biquartimin(loadings)             loadings <- ob$loadings             Phi <- ob$Phi             rot.mat <- t(solve(ob$Th))},              TargetQ  =  {ob <- TargetQ(loadings)             loadings <- ob$loadings             Phi <- ob$Phi             rot.mat <- t(solve(ob$Th))},              specialQ = {ob <- specialQ(loadings)             loadings <- ob$loadings             Phi <- ob$Phi             rot.mat <- t(solve(pro$Th))})    } else {      #The following oblique cases all use GPArotation			                      if (rotate =="oblimin"| rotate=="quartimin" | rotate== "simplimax" | rotate =="geominQ"  | rotate =="bentlerQ"  |rotate == "targetQ"  ) {        if (!requireNamespace('GPArotation')) {warning("I am sorry, to do these rotations requires the GPArotation package to be installed")          Phi <- NULL} else {                         ob <- try(do.call(getFromNamespace(rotate,'GPArotation'),list(loadings)))            if(class(ob)== as.character("try-error"))  {warning("The requested transformaton failed, Promax was used instead as an oblique transformation")              ob <- psych::Promax(loadings)}                        loadings <- ob$loadings            Phi <- ob$Phi            rot.mat <- t(solve(ob$Th))}      } else { # Rotate = none or some other value that's not recognized        if(rotate != "none"){          message("Specified rotation not found, rotate='none' used")          rotate <- "none"        }        rot.mat <- NULL        Phi <- NULL      }    }  } else{ # 1-factor case    rotate <- "none"    rot.mat <- NULL    Phi <- NULL  }    # Use as.fa function to get output  f <- as.fa(loadings=loadings,n.obs=n.obs,Phi=Phi,rot.mat=rot.mat,             rotate=rotate,lavFit=fit,cor="poly",fm="wlsmv")  f$FitIndices <- sem.fit(fit,cML = F) # fit measures from sem.fit  f$Misspec <- sem.misspec(fit)  f$Lavaan <- fit  return(f)}## Direct Bifactor Rotation (BiFAD)################################################################# Nils Waller## August 18, 2017#### Arguments:##   R: Input correlation matrix##   B: bifactor target matrix. If B=NULL the program will create an empirically defined target matrix.##   nGroup: Number of group factors in bifactor solution.##   factorMethod: factor extraction method. Options include:##   minres (minimum residual), ml (maximum likelihood),##   pa (principal axis), gls (generalized least squares).##   rotation: factor rotation method. Current options include: oblimin, geominQ, quartimin, promax.##   salient: Threshold value for creating an empirical target matrix.##   maxitFA: Maximum iterations for the factor extraction method.##   maxitRotate: Maximum iterations for the gradient pursuit##   rotation algorithm.##   gamma: Optional tuning parameter for oblimin rotation.#### Value:##   B: User defined or empirically generated target matrix.##   BstarSL: Direct S-L solution.##   BstarFR: Direct full rank bifactor solution.##   rmsrSL: Root mean squared residual of (B - BstarSL).##   rmsrFR: Root mean squared residual of (B - BstarFR).################################################################BiFAD <- function(R, B = NULL, nGroup = NULL,                  factorMethod = "minres",                  rotation="oblimin",                   salient = .20,                  maxitFA = 5000,                  maxitRotate = 5000,                  gamma = 0,                  cor="cor",                  use="pairwise",                  fullrank=TRUE,                  n.obs=NA){    # Code lifted from psych to get correlation matrix from data matrix  if (dim(R)[1]!=dim(R)[2]) {     n.obs <- dim(R)[1]    switch(cor,            cor = {R <- cor(R,use=use)},           cov = {R <- cov(R,use=use)            covar <- TRUE},           tet = {R <- tetrachoric(R)$rho},           tetrachoric = {R <- tetrachoric(R)$rho},           poly = {R <- polychoric(R)$rho},           polychoric = {R <- polychoric(R)$rho},           mixed = {R <- mixed.cor(R,use=use)$rho},           Yuleb = {R <- YuleCor(R,bonett=TRUE)$rho},           YuleQ = {R <- YuleCor(R,1)$rho},           YuleY = {R <- YuleCor(R,.5)$rho })  }  else {if(!is.matrix(R)) R <- as.matrix(R)} # if given a rectangular data matrix    ## FATAL ERROR  if(is.null(B) & is.null(nGroup)){    stop("\n\n FATAL ERROR: Either B or nGroup must be specified")  }    ## Compute unrotated factor solution  if(is.null(nGroup)) nGroup <- ncol(B) - 1  F <- suppressWarnings(psych::fa(r = R,nfactors = nGroup,fm = factorMethod,rotate = "none"))$loadings[]  ## Append column of zeros to create rank deficient matrix  L0 <- cbind(F,0)    ## Create 0/1 Target matrix if B not supplied  Bflag <- 1 # set to 1 if Target matrix (B) is known  if(is.null(B)) Bflag <- 0  if(!Bflag & (!is.null(nGroup))){    # Start rotation from random position    TmatRandom <- qr.Q(qr(matrix(rnorm(nGroup * nGroup ),nGroup )))    switch(rotation,           "oblimin" = {gpa.out <- GPArotation::oblimin(F, Tmat=TmatRandom,gam = gamma, maxit=maxitRotate)},           "geominQ" = {gpa.out <- GPArotation::geominQ(F, Tmat = TmatRandom, maxit=maxitRotate)},           "quartimin" = {gpa.out <- GPArotation::quartimin(F, Tmat = TmatRandom, maxit=maxitRotate)},           "promax" = {gpa.out <- promax(F)})    B <- gpa.out$loadings[]    ## Record signs of loadings    signB <- sign(B)    ## Convert Target matrix into signed 0/1 matrix    B <- signB* matrix(as.numeric(abs(B) >= salient),                       nrow(L0), ncol = nGroup)    ## Append ones vector for general factor    B <- cbind(1,B)  } ## END Create 0/1 Target matrix    ## Rotate rank deficient loading matrix to Target  BstarSL <- Procrustes(B, L0)    signed <- sign(colSums(BstarSL))  signed[signed==0] <- 1  BstarSL <- BstarSL %*% diag(signed)   ## Compute non-hierarchical bifactor solution  F2 <- suppressWarnings(psych::fa(r = R,nfactors = nGroup+1,fm=factorMethod,rotate="none"))$loadings[]  ## Rotate full rank loading matrix to Target  BstarFR <- Procrustes(B, F2)    ## Compute root mean squared residual of Target matrix (B)  ## and best fitting SL and FR solutions (Bstar)  rmsrSL <- rmsrFR <- NA  if(Bflag == 1){    rmsrSL = sqrt(mean((B - BstarSL)^2))    rmsrFR = sqrt(mean((B - BstarFR)^2))  }    # Original return function  # list(B = B,BstarSL = BstarSL,BstarFR = BstarFR,rmsrSL = rmsrSL,rmsrFR = rmsrFR)    # New return function with psych.fa class  if(fullrank==TRUE){    loads <- BstarSL    method <- "bifadFR"  } else {    loads <- BstarFR    method <- "sl"  }    result <- as.fa(loadings=loads,r=R,n.obs = n.obs,cor = cor,rotate=method)  result$cutpoint.bifad <- salient  result$B <- B    return(result)} ## END BiFAD################################################################# Utility Functions###############################################################MatchBiFactors <- function(A, B){  ## Match (and reorder) the factors of B to A  ## using Tucker congruence coefficients (cosines)  ## This function assumes that A and B are  ## bifactor loadings matrices  nc <- ncol(A)  F1 <- A[,2:nc]  F2 <- B[,2:nc]  D.F1<-diag(1/sqrt(apply(F1^2,2,sum)))  D.F2<-diag(1/sqrt(apply(F2^2,2,sum)))  cosF1F2<-D.F1 %*% t(F1) %*% F2 %*% D.F2  best<-apply(abs(cosF1F2),1,which.max)  ## find factor orientation  signvec<-rep(0,(nc-1))  for(i in 1:(nc-1)){    signvec[i]<- sign(cosF1F2[i,best[i]])  }  f1sign<- -1  if(sign(sum(A[,1]))==sign(sum(B[,1]))) f1sign <- 1  B[,c(1,best+1)] %*% diag(c(f1sign,signvec))} ## END MatchFactors#### Compute root mean squared residual: RMSRRMSR <- function(F1, F2) sqrt(mean((F1 - F2)^2))### Another way of doing bifactor target rotations: Sli(D)############################################################################################################################################ # Supplementary data providing requested code for performing iterative Schmid-Leiman Analysis### Code modified by Zack Williams to include BiFAD option, output fa object## 06/01/2020 - last update 09/04/2021# See Below papers for original code and discussion of methods# # Garcia-Garzon et al. (2019). Improving Bi-Factor Exploratory Modeling: # Empirical Target Rotation Based on Loading Differences. Methodology, 15(2), 45–55. # http://doi.org/10.1027/1614-2241/a000163## Abad, F.J., Garcia-Garzon, E., Garrido, L.E. & Barrada, J.R. (2017).# Iteration of Partially Specified Target Matrices: Application to the Bi-factor Case.# Multivariate Behavioral Research, 52(4), # http://doi.org/10.1080/00273171.2017.1301244################################################################################################################################################################################################################### Support functions############################################################################################################################################perform_rotation <- function(L, method = NULL, targ = NULL, reps = 50) {  # Compute factor rotation for geomin and target rotations  #  # Args:  #  #   L: Unrotated factor solution  #   method: Factor rotation method, as in GPArotation:  #           targetT: target  #           geominQ: oblique geomin  #           Default is NULL  #   Targ: Target matrix required for target rotation.   #         Default is NULL.  #   reps: Number of random starts.   #         Default is 50.  #  # Returns:  #  #   Loadings: Rotated factor loading matrix  #   Phi     : Factor correlation matrix (only when geominQ is specified)  #   Rotating matrix: Rotation matrix  #    # Error handling      if (is.null(method)) {    stop("A rotation method must be specified")    cat ("\n")  }    # Compute the factor rotation using GPArotation package    results   <- rep( list(list()), reps)   criterion <- rep(NA,reps)    for (i in 1:reps) {        if (method == "targetT") {      x <- GPArotation:::targetT (L, Tmat =GPArotation::: Random.Start(ncol(L)), maxit = 5000, Target = targ)    }  else if (method == "geominQ") {      x <- GPArotation:::geominQ (L, Tmat =GPArotation::: Random.Start(ncol(L)), maxit = 5000)    } else if(method %in% c("retam","RETAM")){      gq <- GPArotation:::geominQ (L, Tmat =GPArotation::: Random.Start(ncol(L)), maxit = 5000)      x <- RETAM(L,Tmat = get_target_from(gq$loadings)$Targ)    }        # Selecting the best random start    if (x$convergence == TRUE) {      criterion[i] <- min(x$Table[,2])      results[[i]] <- x          } else {       criterion[i] <- NA      results[[i]] <- NA      cat("Convergence problem in factor rotation for random start ",i, "\n")      cat ("\n")    }  }    #return the best random start solution      j <- order(criterion)[1]  return(results[[j]])}############################################################################################################################################get_target_from <- function (L, cutpoint = "dif") {  # Computes the target matrix   #  # Args:  #  #   L: Factor loading matrix  #   cutpoint: Cut-off for factor loading substantivity.   #             Default is "dif":  performs a promin-based estimation of the cut-off point  #             Fixed cut-offs estimated by providing a value between 0 and 1  #  # Returns:  #  #   Targ = Partially Specified Target rotation  #    if (is.null(cutpoint)){    stop("A cut-off point must be specified")  }    if (is.character(cutpoint)){    if (cutpoint != "dif") {      stop("for applying the SLiD, please write cutpoint = dif")    }  }    Targ <- L    if (cutpoint == "dif") {        c2 <- c()    c2 <- (Targ[, -1]/sqrt(apply(Targ[, -1]^2, 1, sum)))    c2 <- c2^2        i <- 1    b <- list()    dat <- list()    cuts_mix <- c()        for (i in 1:ncol(c2)) {            I <- sort(c2[, i])      a <- c()      j <- 1      for (j in 1:length(I)) {        a[j] <- I[j] - I[j - 1]      }            b[[i]] <- a      dat[[i]] <- round(data.frame(sort(abs(Targ[, i + 1])), sort(c2[, i]), b[[i]]), 3)      names(dat[[i]]) <- c("sl.loadings","norm.loadings","diff.loadings")      cuts_mix[i] <- dat[[i]][which(dat[[i]]$diff.loadings > mean(dat[[i]]$diff.loadings, na.rm = T)) - 1, ][1, ][[1]]          }        c2vec <- c()    c2ref <- c()    c2vec <- t(matrix(cuts_mix, ncol(Targ) - 1, nrow(Targ)))    c2ref <- c2 - c2vec        Targ[, -1][c2ref > 0] <- NA    Targ[, -1][c2ref <= 0] <- 0    Targ[, 1] <- NA        ### Identification conditions check    j <- 1    Targ2 <- Targ[, -1]    Targ2[is.na(Targ2)] <- 1        # prepare matrix for rank computation    multiplier <- L    a <- matrix(ncol = ncol(Targ2), nrow = 1)        for (j in 1:ncol(Targ[, -1])) {            if (mean(L[, j + 1]) < 0) {        multiplier[, j + 1] <- L[, j + 1] * (-1)      }            # submatrix rank assessment        m <- Targ2[which(Targ2[, j] == 0), -j]      if (length(m) <= 2) {        a[1, j] <- qr(t(m))$rank      } else {        a[1, j] <- qr(m)$rank      }    }        if (all(a == (ncol(Targ2) - 1))) {            Targ2[Targ2 == 1] <- NA      Targ[, 2:ncol(Targ)] <- Targ2            return(list (Targ = Targ))          } else {            warning(paste("Solution might not be identified"))            c <- which(a != (ncol(Targ2) - 1))      h <- 1      Targ2[Targ2 == 0] <- NA            for (h in c) {        Targ2[which.min(as.matrix(multiplier[, h + 1]) * Targ2[, h]),h] <- NA        m <- Targ2[which(is.na(Targ2[, h])), -h]        m[which(is.na(m))] <- 0        if (length(m) <= 2) {          a[1, h] <- qr(t(m))$rank        } else {          a[1, h] <- qr(m)$rank        }      }            Targ2[is.na(Targ2)] <- 0      Targ2[Targ2 == 1] <- NA      Targ[, 2:ncol(Targ)] <- Targ2            return(list (Targ = Targ))    }      } else {        Targ[abs(Targ) >  cutpoint] <- NA    Targ[abs(Targ) <= cutpoint] <- 0  }    Targ[,1] <- NA  return(list(Targ = Targ))}######################################################################target_convergence_check <- function (Targetprev, Targetnew) {    # Convergence check for the SLi rotation  #  # Args:  #  #   Targetprev: Target matrix from previous iteration  #   Targetnew : Target matrix from actual iteration  #  # Returns:  #  #   converg: If TRUE, convergence is achieved  #            If FALSE, convergence is not achieved    Targetprev[is.na(Targetprev)] <- 1  Targetnew [is.na(Targetnew) ] <- 1    if (sum((Targetnew - Targetprev)^2) == 0) {    converg    <- TRUE  }    else {    converg    <- FALSE  }  return(converg)}####################################################################### Schmid-Leiman algorithm function (SLi and SLiD [default] methods)######################################################################sli <- slid <- SLiD <- SL.iter <- SLi <- function(matrix = NULL,specific_factors = NULL, fm = "minres", cutpoint= "dif", iterations = 20, BiFAD = FALSE,                   B=NULL, # target matrix for BiFAD rotation - ignored otherwise                   cutpoint.bifad=0.2, rot = "geominQ", cor="cor", use="pairwise", ordinalLevelMax=11, n.obs=NA,NCYCLES=2000,...){        # Compute the iterative target factor rotation    #    # Args:    #    #   matrix: Correlation or Covariance matrix to be analyzed. Default is NULL.    #   specific_factors: number of specific factors to be extracted. Default is NULL.    #   fm: factor estimation method. Default is MINRES. Other alternatives can     #       be found in the fa() function documentation (psych) package.    #   cutpoint: Value for cut-off point criterion (e.g., .20). Default is the Difference loadings criterion (SLiD).    #   iterations: iterations number. Default is 20. If 0, Schmid-Leiman with target rotation (without iterations) is performed.    #      # Returns:    #    #   Loadings: Rotated factor loading matrix    #   Target: Last target applied    #      # Error handling:          if (is.null(matrix)){      stop("A correlation or covariance matrix must be specified")    }        if (is.null(specific_factors)){      stop("A number of factors must be specified")    }        if (is.null(cutpoint)){      stop("A cut-off point must be specified")    }        if (dim(matrix)[1]!=dim(matrix)[2]) {       rawdata <- matrix # save raw data for DWLS extraction      n.obs <- dim(matrix)[1]      matrix <- cor_auto(rawdata,ordinalLevelMax = ordinalLevelMax,verbose = F)    }  else {      rawdata <- NULL      if(!is.matrix(matrix)) matrix <- as.matrix(matrix)      if(fm %in% c("mirt","fiml","mhrm","MHRM","dwls","DWLS","wlsmv","WLSMV","ordinal","uls","ULS","ulsmv","ULSMV")){        stop("Factor extraction method not usable without raw data input.")      }    }        ## BiFAD: Direct BiFactor algorithm from Waller (2017) Psychometrika    ## Alternative to SL routine specified in original code    if(BiFAD == TRUE){      if(fm %in% c("mirt","fiml","mhrm","MHRM","dwls","DWLS","wlsmv","WLSMV","ordinal","uls","ULS","ulsmv","ULSMV")){        bifad <- BiFAD(R = cor_auto(rawdata,ordinalLevelMax = ordinalLevelMax,verbose = F),B=B,                       nGroup = specific_factors,                       rotation = ifelse(rot %in% c("retam","RETAM"),"geominQ",rot),                       factorMethod = "minres",                       salient = cutpoint.bifad)      } else{        bifad <- BiFAD(R = matrix,B=B,                       nGroup = specific_factors,                       rotation = ifelse(rot %in% c("retam","RETAM"),"geominQ",rot),                       factorMethod = fm,                       salient = cutpoint.bifad)      }            SL_loadings <- bifad$loadings      B <- bifad$B    } else{ # SL the normal way      ## Step 1: First order factor analysis with Geomin oblique rotation      if(fm %in% c("mirt","fiml")){        lp <- loads(mirt(rawdata,specific_factors,technical=list(NCYCLES=NCYCLES),method = "QMCEM",...),add.h2 = F)      } else if(fm %in% c("mhrm","MHRM")){        lp <- loads(mirt(rawdata,specific_factors,method = "MHRM",...),add.h2 = F)      } else if(fm %in% c("dwls","DWLS","wlsmv","WLSMV","ordinal","uls","ULS","ulsmv","ULSMV")){        lp  <- psych:::fa (r = matrix, nfactors = specific_factors, rotate = "none", fm = "minres")$loadings      } else{        lp  <- psych:::fa (r = matrix,                            nfactors = specific_factors,                            rotate = "none",                            fm = fm)$loadings      }            lp_rotated     <- perform_rotation (lp, method = "geominQ")      convergence_lp <- lp_rotated$convergence            # Convergence check:          if (convergence_lp == FALSE) {        stop("Convergence problems when estimating first order solution for SL solution")        cat ("\n")      }            ## Step 2: Second order factor analysis      lp1            <- psych:::fa (lp_rotated$Phi, nfactors=1, fm = "minres")            ## Step 3: Schmid-Leiman transformation      lpSL1          <- lp_rotated$loadings %*% lp1$loadings      psl1           <- matrix (0, dim(lp1$loadings), dim(lp1$loadings))      diag(psl1)     <- sqrt(1- lp1$loadings^2)      lpsl2          <- lp_rotated$loadings %*% psl1      SL_loadings    <- cbind (lpSL1,lpsl2)    }        ## Step 4: Calculate an unrotated solution with specific + 1 factors          ### Modifications by Zack Williams to use DWLS/ULS extraction if asked to    if(fm %in% c("dwls","DWLS","wlsmv","WLSMV","ordinal")){ # DWLS extraction      fm <- "wlsmv"      efa_mod <- paste(paste0('efa(\"efa\")*',paste0("F",1:(specific_factors+1)),collapse=" + \n")," =~ ",paste(colnames(rawdata),collapse=" + "))      message("Computing lavaan EFA. This may take a while for large datasets.")      unrotated_l <- loadings(sem(efa_mod,data=rawdata,ordered=T,rotation="none",std.lv=T,estimator="WLSMV",missing="pairwise",...),add.h2=F)    } else if(fm %in% c("uls","ULS","ulsmv","ULSMV")){ # Ordinal ULS extraction      fm <- "ulsmv"      efa_mod <- paste(paste0('efa(\"efa\")*',paste0("F",1:(specific_factors+1)),collapse=" + \n")," =~ ",paste(colnames(rawdata),collapse=" + "))      message("Computing lavaan EFA. This may take a while for large datasets.")      unrotated_l <- loadings(sem(efa_mod,data=rawdata,ordered=T,rotation="none",std.lv=T,estimator="ULSMV",missing="pairwise",...),add.h2=F)    } else if(fm %in% c("mirt","fiml")){      message("Computing mirt EFA. This may take a while for large datasets.")      unrotated_l <- loads(mirt(rawdata,specific_factors+1,technical=list(NCYCLES=NCYCLES),method = "QMCEM",...),add.h2 = F)    } else if(fm %in% c("mhrm","MHRM")){      unrotated_l <- loads(mirt(rawdata,specific_factors+1,method = "MHRM",...),add.h2 = F)    } else{ # All other extraction methods (uses fa function)      unrotated_l <- psych:::fa (r = matrix,nfactors = (specific_factors+1),rotate ="none",fm = fm)$loadings    }        # Step 4: Schmid-Leiman iterated target rotation (SLi)        targF           <- get_target_from (SL_loadings, cutpoint = cutpoint)    SLt_result      <- perform_rotation (L = unrotated_l,                                          method = "targetT",                                         targ = targF$Targ)        # Convergence check    if ( SLt_result$convergence == FALSE) {      stop("Convergence problems when estimating SL target rotation")      cat ("\n")    }    # SLt rotation factor loadings    loadings_targ_prev <- SLt_result$loadings    prev_target        <- targF        # Step 5: Schmid-Leiman iterated target rotation (SLi)        if (iterations == 0) {      loadings_targ_new  <- SLt_result$loadings      return(list(loadings = round(loadings_targ_new,3)))    } else {      cat("Iteration: ")      for (it in 1:iterations) {        cat(paste0(it," "))        new_target          <- get_target_from  (loadings_targ_prev, cutpoint = cutpoint)        new_SLi_result      <- perform_rotation (loadings_targ_prev,                                                  method = "targetT",                                                  targ = new_target$Targ)                if ( new_SLi_result$convergence == FALSE) {          stop("Convergence problems when estimating SLi target rotation")          cat ("\n")        }                loadings_targ_new   <- new_SLi_result$loadings                # Check criteria for ending the iterative procedure        converg <- target_convergence_check(prev_target$Targ, new_target$Targ)        if (converg == TRUE) {          cat("Convergence achieved in", it, "iterations \n")          break()        }   else {          loadings_targ_prev   <- loadings_targ_new          prev_target          <- new_target        }      }      # Convergence check      if (converg == FALSE) {        cat("Convergence has not obtained for the target matrix. Please increase the number of iterations.")        cat ("\n")      }    }    # Return rotated factor matrix as a fa object (Zack Williams Modification)    result <- as.fa(loadings=loadings_targ_new,r=matrix,n.obs = n.obs,cor=cor,rotate="sli")    result$cutpoint.sli <- cutpoint    if(BiFAD==TRUE){      result$cutpoint.bifad <- cutpoint.bifad      result$B <- B    }        return(result)  }  ######################################################################## Another method for determining the number of factors to retain## Zack Williams. 12/3/2017. Last updated 5/2/2018## Source: http://psycnet.apa.org/fulltext/2016-15750-001.pdf## Braeken, J., & Van Assen, M. A. (2017). An empirical Kaiser criterion. Psychological methods, 22(3), 450.# EKC FunctiongetEKC = function(r,n.obs=NULL,max_j=ncol(r),plot=TRUE,returnNF=FALSE,quiet=F){  # If given data matrix, convert to correlation matrix  if(!isSymmetric(as.matrix(r))){ # if not symmetric, not correlation matrix    n.obs <- dim(r)[1]    r <- cor_auto(r,verbose = F)  } else if(is.null(n.obs)){    message("No number of observations given. Arbitrarily using n=500.")    n.obs <- 500  }  # J = number of variables being factored  J <- dim(r)[2]  if(max_j > J){ max_j <- J-1 } # If max_j greater than n_vars, reduce  gamma <- J/n.obs # Parameter based on sample size  l_up <- (1+sqrt(gamma))^2  l_vals <- c(0,eigen(r)$values)  l_ref <- rep(NA,max_j)  for(j in 1:max_j){    l_j <- (J - sum(l_vals[1:j]))/(J - j + 1)*l_up    l_ref[j] <- l_j  }    l_r <- (l_vals[-1])[1:length(l_ref)]    l_ref[which(l_ref < 1)] <- 1  nfactors <- max(which(l_r > l_ref))  if(!quiet){    cat(paste("\nThe EKC indicates that",nfactors,"factors should be retained.","\n"))  }    sum_frame <- rbind(l_r,l_ref)  colnames(sum_frame) <- paste0("l.",1:ncol(sum_frame))  rownames(sum_frame) <- c("Data","Reference")  if(plot){    matplot(t(sum_frame),type=c("b","l"),pch=16,main="Scree Plot",xlab="Index",ylab = "Eigenvalue")  }  if(returnNF){    return(nfactors)  } else{    return(sum_frame[,1:min(ncol(sum_frame),max_j)])      }}# Multiple plot function for Nfactor Hull function (below)## ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)# - cols:   Number of columns in layout# - layout: A matrix specifying the layout. If present, 'cols' is ignored.## If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),# then plot 1 will go in the upper left, 2 will go in the upper right, and# 3 will go all the way across the bottom.#multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {  library(grid)    # Make a list from the ... arguments and plotlist  plots <- c(list(...), plotlist)    numPlots = length(plots)    # If layout is NULL, then use 'cols' to determine layout  if (is.null(layout)) {    # Make the panel    # ncol: Number of columns of plots    # nrow: Number of rows needed, calculated from # of cols    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),                     ncol = cols, nrow = ceiling(numPlots/cols))  }    if (numPlots==1) {    print(plots[[1]])      } else {    # Set up the page    grid.newpage()    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))        # Make each plot, in the correct location    for (i in 1:numPlots) {      # Get the i,j matrix positions of the regions that contain this subplot      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))            print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,                                      layout.pos.col = matchidx$col))    }  }}# Nfactor.hull# Zack Williams# 05/02/2018 - Updated 06/23/2019 to add DWLS factors# Use Hull method to find the number of factors# See Lorenzo-Seva, Timmerman, & Kiers (2011), MBR# The Hull Method for Selecting the Number of Common Factorsnfactor.hull <- function(x,n.obs=NULL,nfactors=NULL,plot=TRUE,fm="ml",cor="cor",use="pairwise",extra=FALSE){  # Additional use for ordinal EFA (dwls estimation)  if(fm %in% c("dwls","DWLS","wlsmv","WLSMV","ordinal")){    if(dim(x)[1]==dim(x)[2]) stop("ERROR: Full data required for WLSMV factor analysis.")    n.obs <- dim(x)[1]  } else if (dim(x)[1]!=dim(x)[2]) {     # Code lifted from psych to get correlation matrix from data matrix    n.obs <- dim(x)[1]    switch(cor,            cor = {x <- cor(x,use=use)},           cov = {x <- cov(x,use=use)            covar <- TRUE},           tet = {x <- tetrachoric(x)$rho},           tetrachoric = {x <- tetrachoric(x)$rho},           poly = {x <- polychoric(x)$rho},           polychoric = {x <- polychoric(x)$rho},           mixed = {x <- mixed.cor(x,use=use)$rho},           Yuleb = {x <- YuleCor(x,bonett=TRUE)$rho},           YuleQ = {x <- YuleCor(x,1)$rho},           YuleY = {x <- YuleCor(x,.5)$rho })  }  else {if(!is.matrix(x)) x <- as.matrix(x)}  # if given a rectangular   if(is.null(n.obs)) {    message("n.obs was not specified and was arbitrarily set to 1000. Fit indices may be incorrect.")    n.obs <- 1000  }    if(is.null(nfactors)) { # If nfactors not specified, use EKC + 1 as maximum    message("nfactors was not specified - Using EKC + 1 as maximum")    nfactors <- getEKC(r=x,n.obs=n.obs,plot=F,returnNF = T,max_j = max(20,floor(ncol(x)/3))) + 1  }    # Define vectors of fit indices  # First value signifies hypothetical 0-factor case  params <- CFI <- TLI <- IFI <- c(0,rep(NA,nfactors))  RMSEA <- RMR <- c(1,rep(NA,nfactors))  if(fm %in% c("dwls","wlsmv","ordinal")){    CAF <- suppressMessages(c(1 - KMO(cor_auto(x))$MSA,rep(NA,nfactors)))  } else{    CAF <- c(1 - KMO(x)$MSA,rep(NA,nfactors))  }  # Do a factor analysis for each # factors, calculate fit indices  for(i in 2:(nfactors+1)){    cat(paste0("Computing FA ",i-1,"\n"))    # If dwls method selected, use fa.dwls function instead    if(fm %in% c("dwls","wlsmv","ordinal")){      f <- suppressWarnings(fa.dwls(x,nfactors = i-1,parameterization = "theta",rotate="none"))    } else{      f <- suppressWarnings(fa(x,n.obs=n.obs,nfactors = i-1,fm = fm))    }    df.mod <- f$dof    df.null <- f$null.dof    x2.null <- f$null.chisq    if(fm %in% c("ml","dwls","wlsmv","ordinal")){ # If using ML, calculate normal-theory Chi-squared, otherwise use OLS Chi-squared      x2.mod <- f$STATISTIC # This is also the WLSMV x2 if fm = "dwls"    } else {      x2.mod <- f$chi    }    # df = m*q - 1/2 (q)(q-1) where m = #items, q = #factors    params[i] <- dim(x)[2] * (i-1) + (i-1)*(i-2)/2    CFI[i] <- 1 - max((x2.mod - df.mod),0)/(x2.null - df.null)    CAF[i] <- 1 - KMO(f$residual)$MSA    if(is.nan(CAF[i])){ CAF[i] <- 1 } # Fixed to not break for Heywood Cases    RMSEA[i] <- sqrt(max(x2.mod - df.mod,0)/(df.mod*(n.obs - 1)))    RMR[i] <- f$rms    TLI[i] <- ((x2.null/df.null) - (x2.mod/df.mod))/(x2.null/df.null-1)    IFI[i] <- (x2.null - x2.mod)/(x2.null - df.mod)  }   results <- data.frame(CFI=CFI,CAF=CAF,RMSEA=RMSEA,RMR=RMR,TLI=TLI,IFI=IFI,params=params)    # Algorithm to exclude points not on convex hull  # First/last points excluded  # For every set of 3 points on fit~paramters plot, draw line between point 1 and 3  # Exclude point 2 if line is above (or below for RMSEA/RMR)  isHull <- data.frame("CFI"=c(FALSE,rep(NA,nfactors-1),FALSE),                        "CAF"=c(FALSE,rep(NA,nfactors-1),FALSE),                       "RMSEA"=c(FALSE,rep(NA,nfactors-1),FALSE),                       "RMR"=c(FALSE,rep(NA,nfactors-1),FALSE),                       "TLI"=c(FALSE,rep(NA,nfactors-1),FALSE),                       "IFI"=c(FALSE,rep(NA,nfactors-1),FALSE))  for(j in 2:(nfactors)){    # CFI    slope.cfi <- (CFI[j+1]-CFI[j-1])/(params[j+1]-params[j-1]) # Slope between pts 1/3    isHull[j,"CFI"] <- CFI[j] > (CFI[j-1] + slope.cfi*(params[j]-params[j-1])) # Slope below point 2?        # CAF - aka 1-KMO statistic of residual matrix    slope.caf <- (CAF[j+1]-CAF[j-1])/(params[j+1]-params[j-1])    isHull[j,"CAF"] <- CAF[j] > (CAF[j-1] + slope.caf*(params[j]-params[j-1]))    # RMSEA    slope.rmsea <- (RMSEA[j+1]-RMSEA[j-1])/(params[j+1]-params[j-1]) # Slope between pts 1/3    isHull[j,"RMSEA"] <- RMSEA[j] < (RMSEA[j-1] + slope.rmsea*(params[j]-params[j-1])) # Slope above point 2?    # RMR    slope.RMR <- (RMR[j+1]-RMR[j-1])/(params[j+1]-params[j-1]) # Slope between pts 1/3    isHull[j,"RMR"] <- RMR[j] < (RMR[j-1] + slope.RMR*(params[j]-params[j-1])) # Slope above point 2?    # TLI    slope.tli <- (TLI[j+1]-TLI[j-1])/(params[j+1]-params[j-1]) # Slope between pts 1/3    isHull[j,"TLI"] <- TLI[j] > (TLI[j-1] + slope.tli*(params[j]-params[j-1])) # Slope below point 2?    # IFI    slope.ifi <- (IFI[j+1]-IFI[j-1])/(params[j+1]-params[j-1]) # Slope between pts 1/3    isHull[j,"IFI"] <- IFI[j] > (IFI[j-1] + slope.ifi*(params[j]-params[j-1])) # Slope below point 2?  }  # Go through points on hull, pick best step (st, i.e., ∆fit/∆param) ratio  st <- data.frame(CFI=rep(NA,nfactors+1),CAF=NA,RMSEA=NA,RMR=NA,TLI=NA,IFI=NA)  for(i in 1:6){ # Go through each fit index individually, because potentially different hull values    for(k in which(isHull[,i])){       st[k,i] <- ((results[k,i]-results[k-1,i])/(params[k]-params[k-1]))/        ((results[k+1,i]-results[k,i])/(params[k+1]-params[k]))    }      }    # Print results    cat("\n\n\nCFI values:",round(CFI,2))  cat("\nCFI","suggests",which.max(st[,"CFI"])-1,"factors. CFI.max =",round(CFI[which.max(st[,"CFI"])],2))  cat("\nCAF values:",round(CAF,2))  cat("\nCAF","suggests",which.max(st[,"CAF"])-1,"factors. CAF.max =",      round(CAF[which.max(st[,"CAF"])],2))  if(extra==TRUE){ # Add RMSEA, RMR, TLI, and IFI    cat("\n\nRMSEA values:",round(RMSEA,2))    cat("\nRMSEA","suggests",which.max(st[,"RMSEA"])-1,"factors. RMSEA.min =",        round(RMSEA[which.max(st[,"RMSEA"])],2))    cat("\nRMR values:",round(RMR,2))    cat("\nRMR","suggests",which.max(st[,"RMR"])-1,"factors. RMR.min =",        round(RMR[which.max(st[,"RMR"])],2))    cat("\nTLI values:",round(TLI,2))    cat("\nTLI","suggests",which.max(st[,"TLI"])-1,"factors. TLI.max =",        round(TLI[which.max(st[,"TLI"])],2))     cat("\nIFI values:",round(IFI,2))    cat("\nIFI","suggests",which.max(st[,"IFI"])-1,"factors. IFI.max =",        round(IFI[which.max(st[,"IFI"])],2))   }      # Plots  if(plot==TRUE){ # Plot those suckas    cfi.plot = ggplot(results,aes(x=params, y=CFI)) +       ggtitle("CFI Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"CFI"]) +      scale_y_continuous(name='CFI')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"CFI"])], linetype = 'dashed')    caf.plot = ggplot(results,aes(x=params, y=CAF)) +       ggtitle("CAF Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"CAF"]) +      scale_y_continuous(name='CAF')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"CAF"])], linetype = 'dashed')    rmsea.plot = ggplot(results,aes(x=params, y=RMSEA)) +       ggtitle("RMSEA Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"RMSEA"]) +      scale_y_continuous(name='RMSEA')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"RMSEA"])], linetype = 'dashed')     RMR.plot = ggplot(results,aes(x=params, y=RMR)) +       ggtitle("RMR Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"RMR"]) +      scale_y_continuous(name='RMR')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"RMR"])], linetype = 'dashed')    tli.plot = ggplot(results,aes(x=params, y=TLI)) +       ggtitle("TLI Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"TLI"]) +      scale_y_continuous(name='TLI')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"TLI"])], linetype = 'dashed')    ifi.plot = ggplot(results,aes(x=params, y=IFI)) +       ggtitle("IFI Plot") +       geom_line() +       geom_point(size=3,color=1+isHull[,"IFI"]) +      scale_y_continuous(name='IFI')+      scale_x_continuous(name='df parameters', breaks=params)+      geom_vline(xintercept = params[which.max(st[,"IFI"])], linetype = 'dashed')    if(extra==TRUE){      multiplot(cfi.plot, caf.plot, rmsea.plot, RMR.plot, tli.plot, ifi.plot, cols=3)    } else{      multiplot(cfi.plot, caf.plot, cols=2)    }  }  cat("\n\n")  return(round(results,3))}### nfactor.EGA function## Zack Williams## 04/07/2021 - updated 09/26/22 to allow correlation matrix input# Wrapper for EGAnet::EGA that includes estimation of wTO (heuristic: >0.3 is significant overlap)# Also added "potentially unrelated nodes" using low outliers in mean absolute edge weight (>3 MADs below median). # Updated to add network loadings. Salient cross-loadingsnfactor.EGA <- function(data,algorithm="walktrap",n=NULL,...,ordinalLevelMax=11,forcePD=T,missing = "pairwise",spearman=FALSE,wTO.cut=NULL,maxload.cut=0.1,outlier_cutoff=2.24){  if(!is.null(n)){    n.cases <- n # N is input        cor.mat <- data # Correlation matrix is given  } else{    n.cases <- round(mean(apply(data,2,function(X){sum(!is.na(X))}))) # Mean of pairwise Ns is the calculated N        # Correlation matrix is calculated     if(spearman){      cor.mat <- cor(data,method="spearman",use = missing)    } else{      cor.mat <- cor_auto(data,ordinalLevelMax=ordinalLevelMax,forcePD=forcePD,missing = missing)    }  }    ega <- EGA(data=cor.mat,n = n.cases,algorithm=algorithm,...,plot.EGA = F)  if(is.null(ega)){stop("ERROR: EGA failed to complete!")}    rownames(ega$dim.variables) <- NULL  # Look at item overlap - based on wTO > max(0.3,Mdn + 2.24*MAD) by default    if(is.null(wTO.cut)){    wtomat <- wTO::wTO(ega$network,sign="sign")    wto_vals <- wtomat[lower.tri(wtomat)]    wTO.cut <- max(0.3,hd(wto_vals) + outlier_cutoff * hd(abs(wto_vals - hd(wto_vals))))  }    large_wto <- corCheck(wTO(ega$network,sign="sign"),cut = wTO.cut)$LargeCors  # Make matrix of keyed items based on communities  factor_list <- lapply(1:max(ega$wc,na.rm=T),function(X){which(ega$wc==X)})  key_mat <- make.keys(ncol(data),factor_list)  colnames(key_mat) <- names(factor_list) <- paste0("F",1:ncol(key_mat))  rownames(key_mat) <- colnames(data)    ega_loads <- net.loads(ega$network,ega$wc)    # Add network loadings (similar to net.loads function)  node_strength <- rowSums(abs(ega$network))  loads_unstd <- sapply(1:max(ega$wc),function(i){    rowSums(abs(ega$network)[,ega$wc==i])  })  # Special case for unidimensional  if(max(ega$wc)==1){    loads_unstd <- cbind("F1"=loads_unstd)    loads_std <- loads_unstd/sqrt(sum(loads_unstd))  } else{    loads_std <- apply(loads_unstd,2,function(X){X/sqrt(sum(X))})    colnames(loads_unstd) <- colnames(loads_std) <- paste0("F",1:ncol(key_mat))  }    # Look for nodes with max_loads < min_load (default 0.1)  max_loads <- apply(loads_std,1,max)    if(is.null(maxload.cut)){    maxload_cut <- hd(max_loads) - outlier_cutoff*hd(abs(max_loads - hd(max_loads))) # median - 2.24 * median abs deviation from median    mad_loading <- TRUE  } else{    maxload_cut <- maxload.cut    mad_loading <- FALSE  }    small_loads <- max_loads[max_loads < maxload_cut]    salient_loads <- loads_std  salient_loads[salient_loads < maxload_cut] <- 0  cross_loads <- round(salient_loads[rowSums(salient_loads >= maxload_cut) > 1,,drop=FALSE],3)  result <- list("n.dim"=ega$n.dim,"wc"=ega$wc,"overlap"=large_wto,"dim.variables"=key_mat,"dim.list"=factor_list,"max.netloads.std"=max_loads,                 "net.loads"=list("std"=loads_std,"raw"=loads_unstd,"salient"=salient_loads,"crossloads"=cross_loads),                 "network"=ega$network,"methods"=ega$Methods,"wTO.cut"=wTO.cut,"maxload.cut"=maxload_cut,"outlier.MAD"=mad_loading,"outlier.nMADs"=outlier_cutoff)  class(result) <- "nf.EGA"  if(length(small_loads > 0)){result$small.loads <- small_loads}  result$call <- match.call()  return(result)}# print.nfactor.EGA: Pretty-print the nfactor.EGA outputprint.nf.EGA <- function(X){  cat("EGA results:\n")  if(X$methods$model=="glasso"){    cat(paste0(" EBICglasso estimation "," (gamma = ",X$methods$model.args$gamma,", lambda = ",X$methods$model.args$lambda,")"))  } else{    cat(paste0(" TMFG estimation"))  }    if(X$methods$algorithm=="walktrap"){    cat(paste0(", Walktrap algorithm (",X$methods$algorithm.args$steps," steps)"))  } else{    cat(", Louvain algorithm")  }  cat("\n\nCommunity Structure:\n")  print(X$dim.variables)  if(length(X$net.loads$crossloads) > 0){    cat("\nSalient Cross-Loadings:\n")    print(X$net.loads$crossloads)  }    cat(paste0("\n**EGA indicates that ",X$n.dim," factors are present.**\n"))  cat(paste0("\nBased on wTO > ",X$wTO.cut,", there are ",length(X$overlap)," potentially redundant item pairs:"))  if(length(X$overlap)>0){    cat("\n\n")    print(X$overlap)  }  # If manually entered loading cutoff  if(X$outlier.MAD){    cat(paste0("\nBased on maximum network loading < ",round(X$maxload.cut,3)," (median minus ",X$outlier.nMADs,"*MAD), there are ",length(X$small.loads)," potentially unrelated items:"))  } else{    cat(paste0("\nBased on maximum network loading < ",round(X$maxload.cut,3)," (manually-entered cutoff), there are ",length(X$small.loads)," potentially unrelated items:"))  }  if(length(X$small.loads)>0){    cat("\n\n")    print(round(X$small.loads,3))  }}## Pa.permute function## Zack Williams ## Updated 02/22/20# Based on the recommendations of Lubbe (2019) http://dx.doi.org/10.1037/met0000171# Parallel analysis with categorical variables: Impact of category probability # proportions on dimensionality assessment accuracy. Psychological Methods, 24(3), 339-351.################################################################################# Parallel analysis for ordinal data using polychoric correlations# * Permutes all columns of the sample data matrix to produce ordinal comparison data #   with equal category proportions to sample data (see above paper for why this matters).# * Uses traditional PCA-based PA (Horn, 1965) with mean eigenvalues to determine the # of factors# * Compares to Empirical Kaiser Criterion (Braeken & Van Assen, 2018)# * Should be faster than equivalent psych program because of fewer functions and#   cor_auto calculating polychoric correlations in ~1/2 the time of psych::polychoricpa.permute <- function(x,n.iter=10,n.cores=detectCores()-1,plot=T,seed=12345,ordinalLevelMax = 12,...){  options("mc.cores"=n.cores) # set number of cores  set.seed(seed)  rx <- cor_auto(x,verbose = T,ordinalLevelMax = ordinalLevelMax,...) # Polychoric correlation matrix  valuesx  <- eigen(rx)$values #these are the PC values    EKC <- t(getEKC(rx,n.obs=nrow(x),quiet=T,plot = F))    # Permute the columns of the matrix   templist <- pblapply(1:n.iter,function(XX){    x_i <- apply(x,2,sample) # randomly permute every column    rx_i <- cor_auto(x_i,verbose = F)    values_i  <- eigen(rx_i)$values  },cl=n.cores)  values <- t(matrix(unlist(templist),ncol=n.iter))  meanEVs <- colMeans(values)  q95EVs <- apply(values,2,quantile,probs=0.95)    EVmat <- cbind(EKC,meanEVs,q95EVs)  colnames(EVmat) <- c("Data","EKC","PA","PA.95%ile")  nfactors <- colSums(EVmat[,1] > EVmat[,-1],na.rm=T)      if(plot==T){    EV_wide <- data.frame(cbind("Factor"=1:nrow(EVmat),EVmat[,-4]))    EV_long <- reshape2::melt(EV_wide, id="Factor")    # Make Scree Plot    p <- ggplot(data=EV_long,aes(x=Factor, y=value, colour=variable)) +      scale_color_manual("",values = c("black","blue","red")) +      scale_shape_manual("",values = c(19,1,1)) +      scale_size_manual("",values=c(2,0,0)) +      geom_line(linetype=c(rep("solid",nrow(EVmat)),rep("dashed",2*nrow(EVmat)))) +      geom_point(aes(shape = variable,size=variable)) +      xlab("Factor") +       ylab("Eigenvalue") +      ggtitle("Scree Plot") +      theme(legend.position=c(1,1),            legend.justification = "right",            plot.title = element_text(size=14, face="bold", hjust=0.5),            axis.text.x = element_text(size=10),            axis.text.y = element_text(size=10),            axis.title.x = element_text(size=12,face="bold"),            axis.title.y = element_text(size=12,face="bold"),            strip.text.y = element_text(size = 20, colour = "white"),            strip.background = element_rect(colour="darkgrey", fill="darkgrey"))  } else{    p <- NULL  }    result <- list("nfactors"=nfactors,"Values"=EVmat,"Simulated.values"=values,"Plot"=p,                 "n.iter"=n.iter,call=match.call())  class(result) <- "pa"  return(result)}# Pretty Print Function for pa.permute objectprint.pa <- function(x,plot=T,digits=3){  cat("Call: ")  print(x$call)  cat("\n")  cat(paste0("Parallel Analysis using polychoric correlations and permuted data (",x$n.iter," iterations):"))  cat(paste0("\n Parallel Analysis suggests that ",x$nfactors[2]," factors should be retained",             " (",x$nfactors[3]," based on 95th percentile EV)"))  cat(paste0("\n The EKC indicates that ",x$nfactors[1]," factors should be retained."))  cat("\n\n")  cat(paste0("First ",max(x$nfactors)+1," eigenvalues:\n"))  print(round(x$Values[1:(max(x$nfactors)+1),],digits))  if(plot){    print(x$Plot)  }}# Return the list of thresholds from categorical lavaan object# Modified to work with dataframes as wellgetThreshold <- function(object) {  if(any(class(object)=="lavaan")){    coef <- lavInspect(object, "est")    ths <- coef$tau    targettaunames <- rownames(ths)  } else if(any(class(object)=="data.frame")){    ths <- suppressWarnings(lavCor(object,ordered=colnames(object),missing="pairwise",output="th"))    targettaunames <- names(ths)    ths <- t(ths)  }    barpos <- sapply(strsplit(targettaunames, ""), function(x) which(x == "|"))  varthres <- apply(data.frame(targettaunames, barpos - 1), 1, function(x) substr(x[1], 1, x[2]))  result <- list(split(ths, varthres))    return(result)}# OmegaCat function# Calculate Categorical Omega using variance components, according to formula of Green and Yang (2009)# Function modified from semTools (made to use multiple cores)omegaCat <- function(truevar, implied, threshold, denom) {  # denom could be polychoric correlation, model-implied correlation, or model-implied without error correlation  # Categorical formula using thresholds based on formula from Green and Yang (2009)  polyc <- truevar  invstdvar <- 1 / sqrt(diag(implied))  polyr <- diag(invstdvar) %*% polyc %*% diag(invstdvar)  nitem <- ncol(implied)  denom <- cov2cor(denom)  sumnum <- 0  addden <- 0    p2 <- function(t1, t2, r) {    mnormt::pmnorm(c(t1, t2), c(0,0), matrix(c(1, r, r, 1), 2, 2))  }    Values <- rowSums(mcmapply(function(j,jp){    t1 <- threshold[[j]]    t2 <- threshold[[jp]]    vals <- rowSums(mapply(function(c,cp){      return(c(p2(t1[c], t2[cp], polyr[j, jp]),p2(t1[c], t2[cp], denom[j, jp])))    },rep(1:length(t1),each=length(t2)),1:length(t2)))    sumprobn1 <- sum(pnorm(t1))    sumprobn1p <- sum(pnorm(t2))        return(c(vals[1] - sumprobn1 * sumprobn1p,vals[2] - sumprobn1 * sumprobn1p))  },rep(1:nitem,each=nitem),1:nitem))    omega <- Values[1] / Values[2]  return(omega)}# CalcOmega Function# Zack Williams# 07/22/2019 - updated 09/04/2021 # Based off of semTools::reliability # Now with subscale omegas for bifactor models and functionality for psych objects# Now has functionality for mirt singleGroupClass objects as well# Also calculates scale-level ECV for bifactor models# Updated to include ECVss (from https://uknowledge.uky.edu/edsc_etds/59/)calcOmega <- function(object, categorical = FALSE, data = NULL, cut = NULL,standard=T,bifactor = F,digits=3) {  computeAlpha <- function(S, k) k/(k - 1) * (1.0 - sum(diag(S)) / sum(S))  threshold <- NULL    # Different Input types  if(any(class(object) == "lavaan")){    if(standard==T){ # Standardized Model Parameters      param <- lavInspect(object, "std")      categorical <- length(lavNames(object, "ov.ord"))==length(lavInspect(object,"std")$nu)      ly <- param$lambda # Loadings      ps <- lavInspect(object, "cor.lv") # Inter-factor correlations      te <- param$theta # Diagonal matrix of uniquenesses      SigmaHat <- lavInspect(object, "cor.ov") # Model-implied correlation matrix (accounts for correlated resids)      S <- cov2cor(lavInspect(object, "sampstat")$cov) # Sample covariance matrix    } else{ # Unstandardized Model Parameters      param <- lavInspect(object, "est")      categorical <- length(lavNames(object, "ov.ord"))==length(lavInspect(object,"std")$nu)      ly <- param$lambda # Loadings      ps <- lavInspect(object, "cov.lv") # Inter-factor correlations      te <- param$theta # Diagonal matrix of uniquenesses      SigmaHat <- lavInspect(object, "cov.ov") # Model-implied covariance matrix      S <- lavInspect(object, "sampstat")$cov # Sample covariance matrix    }    if (categorical) threshold <- getThreshold(object)[[1]] else threshold <- NULL # Thresholds if categorical  } else if (any(class(object) == "omega")){ # Psych Omega objects    # If 'cut' not given, set to 0.2    if(is.null(cut)){      cut <- 0.2      message("Subscale cutpoint not set. Using factor loadings  > 0.2 to assign items to subscales by default.\n")    }    bifactor <- TRUE    ly <- object$schmid$sl[,1:(dim(object$schmid$sl)[2]-3)]    ps <- diag(1,dim(object$schmid$sl)[2]-3)    te <- diag(object$schmid$sl[,ncol(ly)+2])    SigmaHat <- ly %*% t(ly) + te    S = object$R  } else if(any(class(object) == "schmid")){    if(is.null(cut)){      cut <- 0.2      message("Subscale cutpoint not set. Using factor loadings  > 0.2 to assign items to subscales by default.\n")    }    bifactor <- TRUE    ly <- object$sl[,1:(dim(object$sl)[2]-3)]    ps <- diag(1,dim(object$sl)[2]-3)    te <- diag(object$sl[,ncol(ly)+2])    SigmaHat <- ly %*% t(ly) + te    if(!is.null(data)){      S <- qgraph::cor_auto(data)    } else{      S <- NULL    }  } else if (any(attr(class(object),"package")=="mirt")){ # mirt objects    categorical <- TRUE    data <- as.data.frame(object@Data$data)    if(object@Options$dentype=="bfactor"){bifactor <- TRUE} # called using 'bfactor'    ly <- loads(object,add.h2 = F)     ps <- Phi(object) # factor correlations     threshold <- getThreshold(data)[[1]]    te <- diag(1 - diag(ly %*% ps %*% t(ly))) # Diagonal matrix of item uniquenesses    S <- suppressWarnings(lavCor(data,ordered=names(data),missing="pairwise"))    # Getting model-implied covariance matrix from residuals    res <- suppressMessages(residCheck(object)$Residuals)     ResidMat <- t(res)    ResidMat[lower.tri(ResidMat)] <- res[lower.tri(ResidMat)]    diag(ResidMat) <- 0    SigmaHat <- S - ResidMat # Model-implied Covariance Matrix  } else if(any(class(object) == "fa")){    if(is.null(cut)){      cut <- 0.2      message("Subscale cutpoint not set. Using factor loadings  > 0.2 to assign items to subscales by default.\n")    }    if(object$rotation %in% c("bifactor","biquartimin","sl","bifadFR","sli")){bifactor <- TRUE}    if(any(class(object) %in% c("schmid","omega"))){bifactor <- TRUE}    ly <- object$loadings[]    ps <- Phi(object)    te <- diag(object$uniquenesses)    S <- object$r    SigmaHat <- ly %*% ps %*% t(ly) + te  } else{ # Just a loadings matrix, or a list with loadings as x$loadings, correlation matrix x$r    if(is.list(object)){      ly <- object$loadings      if(!is.null(object$r)) S <- object$r      if(!is.null(object$S)) S <- object$S      if(!is.null(object$Phi)) ps <- object$Phi      if(!is.null(object$ps)) ps <- object$ps    } else{ # Just a loadings matrix      ly <- object      ps <- S <- NULL    }    if(is.null(ps)){      te <- diag(1-diag(ly %*% t(ly)))      ps <- diag(ncol(ly))    } else{      te <- diag(1-diag(ly %*% ps %*% t(ly)))    }    SigmaHat <- ly %*% ps %*% t(ly) + te    if(!is.null(data)){      S <- qgraph::cor_auto(data,verbose = F)    }  }    # Extract Thresholds from dataframe if necessary  if (categorical & is.null(threshold)){    if(is.null(data)){      stop("ERROR: Need original data to calculate categorical omega.")    } else {      threshold <- getThreshold(data)[[1]]    }  }    common <- apply(ly, 2, sum)^2 * diag(ps) # Sums of loadings for each factor squared * factor variances  truevar <- ly %*% ps %*% t(ly) # Model implied covariance matrix  # Set up vectors of values for each factor  error <- total <- alpha <- omega.1 <- omega.2 <- omega.3 <- impliedTotal <- rep(NA, length(common))  if(bifactor==T){    omegaS.1 <- omegaS.2 <- omegaS.3 <- ECV.GS <- ECV.SS <- ECV.SG <- rep(NA, length(common))  } else{    omegaS.1 <- omegaS.2 <- omegaS.3 <- NULL  }  # Don't calculate Alpha/Omega.3 if sample covariance matrix not supplied  if(is.null(S)){    alpha <- omega.3 <- omegaS.3 <- NULL  }  ##### Now for the meat of the function  #### Loop through factors 1:j, calculating out variance fractions and model-based coefficients  #### Not completely sure the rationale behind some calculations, as code was cannibalized from semTools  for (j in 1:length(common)) {     if(!is.null(cut)){ # Use cut to indicate if only loadings greater than X should be counted as contributing to the factor      index <- which(ly[,j] > cut)    } else{      index <- which(ly[,j] != 0) # DEFAULT: Factor made up of all items with nonzero loadings on factor (not good for EFA/ESEM)    }    error[j] <- sum(te[index, index, drop = FALSE]) # Uniquenesses for items loading on factor    if(!is.null(S)){      sigma <- S[index, index, drop = FALSE] # Sample covariance matrix for 'index' items      alpha[j] <- computeAlpha(sigma, length(index)) # Coefficient alpha      total[j] <- sum(sigma) # Sum of sample covariance matrix    }    impliedTotal[j] <- sum(SigmaHat[index, index, drop = FALSE]) # Sum of model-implied covariance matrix for 'index' items    faccontrib <- ly[,j, drop = FALSE] %*% ps[j,j, drop = FALSE] %*% t(ly[,j, drop = FALSE]) # Outer product of factor loadings on factor    if(bifactor==T & j > 1){      # For omega subscale: Calculate variance due to general + group factors      Gcontrib <- ly[,1, drop = FALSE] %*% ps[1,1, drop = FALSE] %*% t(ly[,1, drop = FALSE])      allcontrib <- faccontrib + Gcontrib      allfac <- sum(allcontrib[index, index, drop = FALSE]) # sum of outer product for those items    }    truefac <- diag(faccontrib[index, index, drop = FALSE]) # squared factor loadings    commonfac <- sum(faccontrib[index, index, drop = FALSE]) # sum of outer product for those items        if (categorical) {      # Either omegaS for correlated factors or omegaH/HS for bifactor      omega.2[j] <- omegaCat(truevar = faccontrib[index, index, drop = FALSE],                             implied = SigmaHat[index, index, drop = FALSE],                             threshold = threshold[index],                             denom = SigmaHat[index, index, drop = FALSE])      if(!is.null(S)){        omega.3[j] <- omegaCat(truevar = faccontrib[index, index, drop = FALSE],                               implied = SigmaHat[index, index, drop = FALSE],                               threshold = threshold[index],                               denom = sigma)      }            if(bifactor==T){        # Omega.1 is equal to Omega.2 in the bifactor case        omega.1[j] <- omega.2[j]        if(j == 1) { # Omega Total          omegaS.1[j] <- omegaS.2 <- omegaCat(truevar = truevar,                                               implied = SigmaHat,                                              threshold = threshold,                                              denom = SigmaHat) # Model-implied vcov matrix          if(!is.null(S)){            omegaS.3 <- omegaCat(truevar = truevar,                                  implied = SigmaHat,                                 threshold = threshold,                                 denom = S) # Observed vcov matrix          }          # Scale level ECV          ECV <- ECV.GS[1] <- sum(ly[,1]^2)/sum(ly^2)           ECV.SS[1] <- ECV.SG[1] <- NA        } else{ # Omega Subscale          omegaS.1[j] <- omegaS.2[j] <- omegaCat(truevar = allcontrib[index, index, drop = FALSE],                                                 implied = SigmaHat[index, index, drop = FALSE],                                                 threshold = threshold[index],                                                 denom = SigmaHat[index, index, drop = FALSE])          if(!is.null(S)){            omegaS.3[j] <- omegaCat(truevar = allcontrib[index, index, drop = FALSE],                                    implied = SigmaHat[index, index, drop = FALSE],                                    threshold = threshold[index],                                    denom = sigma)          }          # Subscale-level ECVs (ECV.SS, ECV.GS, and ECV.SG)          ECV.GS[j] <- sum(ly[index,1]^2)/sum(ly[index,]^2)           ECV.SS[j] <- sum(ly[index,j]^2)/sum(ly[index,]^2)           ECV.SG[j] <- sum(ly[,j]^2)/sum(ly^2)         }      } else{        # Only calculate Omega.1 separately if not bifactor        omega.1[j] <- omegaCat(truevar = faccontrib[index, index, drop = FALSE],                               implied = SigmaHat[index, index, drop = FALSE],                               threshold = threshold[index],                               denom = faccontrib[index, index, drop = FALSE] +                                  te[index, index, drop = FALSE])      }    } else { # For the continuous case            # Either omegaS for correlated factors or omegaH/HS for bifactor      omega.2[j] <- commonfac / impliedTotal[j] # Model-implied      if(!is.null(S)) omega.3[j] <- commonfac / total[j] # Observed            if(bifactor==T){        if(j==1){ # Omega Total          omegaS.1[j] <- omegaS.2[j] <- sum(truevar) / sum(SigmaHat)          if(!is.null(S)) omegaS.3[j] <- sum(truevar) / sum(S)          # Scale level ECV          ECV <- ECV.GS[1] <- sum(ly[,1]^2)/sum(ly^2)           ECV.SS[1] <- ECV.SG[1] <- NA        } else{ # Omega Subscale          omegaS.1[j] <- omegaS.2[j] <- allfac / impliedTotal[j]          if(!is.null(S)) omegaS.3[j] <- allfac / total[j]          # Subscale-level ECVs (ECV.SS, ECV.GS, and ECV.SG)          ECV.GS[j] <- sum(ly[index,1]^2)/sum(ly[index,]^2)           ECV.SS[j] <- sum(ly[index,j]^2)/sum(ly[index,]^2)           ECV.SG[j] <- sum(ly[,j]^2)/sum(ly^2)         }      } else{        omega.1[j] <- commonfac / (commonfac + error[j])      }    }  }  if(bifactor==FALSE){ # Calculate Omega Total for correlated-factor case    # Name the output vectors    names(omega.1) <- names(omega.2) <- colnames(ly)     if(!is.null(S)) names(omega.3) <- names(alpha) <- colnames(ly)    # Compute Overall Alpha (if S is provided)    if(!is.null(S)) alpha <- c("Total" = computeAlpha(S, nrow(S)), alpha)    if (categorical) { # Categorical      omega.1 <- c("Total" = omegaCat(truevar = truevar,                                      implied = SigmaHat,                                      threshold = threshold,                                      denom = truevar + te), omega.1)      omega.2 <- c("Total" = omegaCat(truevar = truevar,                                      implied = SigmaHat,                                      threshold = threshold,                                      denom = SigmaHat), omega.2)      if(!is.null(S)){        omega.3 <- c("Total" = omegaCat(truevar = truevar,                                        implied = SigmaHat,                                        threshold = threshold,                                        denom = S), omega.3)      }    } else { # Continuous case      omega.1 <- c("Total" = sum(truevar) / (sum(truevar) + sum(te)), omega.1)      omega.2 <- c("Total" = sum(truevar) / (sum(SigmaHat)), omega.2)      if(!is.null(S)) omega.3 <- c("Total" = sum(truevar) / (sum(S)), omega.3)    }  } else{ # If Bifactor true, add PUC (ECV is added above now with subscale ECV)    # Percentage of Uncontaminated Correlations (PUC)    if(!is.null(cut)){      grps <- (ly[,-1] > cut) # Assign item to a group if it loads > cut    } else{      grps <- (ly[,-1] != 0) # Assign item to a group if it has nonzero loading    }    cors <- grps %*% t(grps)    total_cors <- nrow(ly)*(nrow(ly)-1)/2    uncontam_cors <- sum(cors[lower.tri(cors)]==0)    PUC <- uncontam_cors/total_cors * 100    IECV <- apply(ly,1,function(x){x[1]^2/sum(x^2)})    # Name the output vectors    names(omega.1) <- names(omega.2) <- colnames(ly)     if(!is.null(S)) names(omega.3) <- names(alpha) <- colnames(ly)  }    if(bifactor==T){    omegaT <- c("Omega.T.1"=unname(omegaS.1[1]),"Omega.T.2"=unname(omegaS.2[1]))    if(!is.null(S)) omegaT <- c(omegaT,"Omega.T.3"=unname(omegaS.3[1]))    omegaH <- c("Omega.H.1"=unname(omega.1[1]),"Omega.H.2"=unname(omega.2[1]))    if(!is.null(S)) omegaH <- c(omegaH,"Omega.H.3"=unname(omega.3[1]))    result <- list("Omega.S.1" = omegaS.1, "Omega.HS.1" = omega.1,                    "Omega.S.2" = omegaS.2, "Omega.HS.2" = omega.2,                   "Omega.S.3" = omegaS.3, "Omega.HS.3" = omega.3,                   "Omega.T"=omegaT,"Omega.H"=omegaH,"ECV"=ECV,"ECV_GS"=ECV.GS,"ECV_SS"=ECV.SS,"ECV_SG"=ECV.SG,                   "I-ECV"=IECV,"PUC"=PUC,"total_cors"=total_cors,"uncontam_cors"=uncontam_cors,                   "bifactor" = bifactor,"categorical" = categorical,"digits"=digits)  } else{    omegaT <- c("Omega.T.1"=unname(omega.1[1]),"Omega.T.2"=unname(omega.2[1]))    if(!is.null(S)) omegaT <- c(omegaT,"Omega.T.3"=unname(omega.3[1]))    result <- list("Omega.S.1" = omega.1, "Omega.S.2" = omega.2,"Omega.S.3" = omega.3,                   "Omega.T"=omegaT,"bifactor" = bifactor,"categorical" = categorical,"digits"=digits)  }    result$Call <- match.call()  result$objectData <- data  result$alpha <- alpha  result$H <- 1/(1+1/colSums(ly^2/(1-ly^2)))  result$FD <- sqrt(diag(ps %*% t(ly) %*% MASS::ginv(SigmaHat) %*% ly %*% ps))  result$GH <- result$FD^2  result$AVE <- sum(diag(truevar))/ sum((diag(truevar) + diag(te)))  if(any(class(object)=="matrix")){    loads <- object  } else{    loads <- loadings(object,bifactor=bifactor,add.h2=F)  }  result$Loadings <- loads    class(result) <- "Omega"  return(result)}# Little function to predict Value Added Ration (>1.1 means added value for subscale) using bifactor coefs# Based on equation 3.14 from Dueber, 2020 (p.76) https://uknowledge.uky.edu/edsc_etds/59/predict_VAR <- function(omegaS,ECV.SS){  return(0.120 + 1.055*omegaS - 0.460*ECV.SS + 1.961*ECV.SS^2)}print.Omega <- function(x,type=2,round=NULL,extra=F,loadings=F){  if(is.null(round)) round <- x$digits  if(!type %in% 1:3) type <- 2  if(is.null(x$Omega.S.3) & type==3){    message("Type 3 Omega not available for this object. Displaying type 2 instead.\n")    type <- 2  }  cat("Call: ")  print(x$Call)  cat("\n")  cat("Reliability Coefficients: (")  if(x$categorical) cat("Categorical ")  cat("Omega Type",type)  if(type == 1) cat("; Bollen, 1980; Raykov, 2001)")  if(type == 2) cat("; Bentler, 1972, 2009)")  if(type == 3) cat("; McDonald, 1999)")  cat("\n Omega Total:",round(x$Omega.T[type],round))  if(!is.null(x$alpha)){    cat("\n ")     if(x$categorical) cat("Ordinal ")    cat("Alpha:",round(x$alpha[1],round))  }  cat("\n AVE:",round(x$AVE,round))  cat("\n")  if(x$bifactor==TRUE){    cat(paste0(" Omega Hierarchical: ",round(x$Omega.H[type],round),        "\n OmegaH/OmegaT: ",round(x$Omega.H[type]/x$Omega.T[type],round),        "\n Explained Common Variance: ",round(x$ECV,round)),        paste0("\n PUC: ",round(x$PUC,1),"% (",x$uncontam_cors,"/",x$total_cors,")\n"))    cat("\nExplained Common Variance for each item (I-ECVs):\n")    print(round(x$`I-ECV`,3))  }  cat("\nOmega Coefficients")  if(!is.null(x$BootCoefs)){    cat(paste0(" and ",round(100*x$ci.width),"% CIs (BCa Bootstrap, ",x$nBoot," iterations)"))  }  cat(":\n")  if(is.null(x$BootCoefs)){    if(type==1){      omega_mat <- rbind("Omega.S"=x$Omega.S.1,"Omega.HS"=x$Omega.HS.1)      if(x$bifactor){omega_mat <- rbind(omega_mat,"ECV_GS"=x$ECV_GS,"ECV_SS"=x$ECV_SS,"VAR.pred"=predict_VAR(x$Omega.S.1,x$ECV_SS))}    } else if(type==3){      omega_mat <- rbind("Omega.S"=x$Omega.S.3,"Omega.HS"=x$Omega.HS.3)      if(x$bifactor){omega_mat <- rbind(omega_mat,"ECV_GS"=x$ECV_GS,"ECV_SS"=x$ECV_SS,"VAR.pred"=predict_VAR(x$Omega.S.2,x$ECV_SS))}    } else{ # Type 2      omega_mat <- rbind("Omega.S"=x$Omega.S.2,"Omega.HS"=x$Omega.HS.2)      if(x$bifactor){omega_mat <- rbind(omega_mat,"ECV_GS"=x$ECV_GS,"ECV_SS"=x$ECV_SS,"VAR.pred"=predict_VAR(x$Omega.S.3,x$ECV_SS))}    }    if(extra){      if(x$bifactor){        omega_mat <- rbind(omega_mat,"Alpha"=x$alpha,"H"=x$H,"FD"=x$FD)      } else{        omega_mat <- rbind(omega_mat,"Alpha"=x$alpha,"H"=c(NA,x$H),"FD"=c(NA,x$FD))      }    }    # Remove redundant values in 1-factor case:    if(ncol(omega_mat)==2 & !x$bifactor){      omega_mat <- omega_mat[,2,drop=FALSE]      rownames(omega_mat) <- gsub(".S$","",rownames(omega_mat))    }    print(round(omega_mat,round))  } else{ # Bootstrapping has occurred - print coefs + CIs instead    print(round(x$BootCoefs,round))  }  if(loadings){    cat("\nLoadings:\n")    print(x$Loadings,nd=round)  }}####################### Function not yet finished. # calcOmega.boot.cfa# Runs calcOmega B times on bootstrapped models# Calculate BCa bootstrapped CIs for omega coefficients and fit indices from correlated-factor or bifactor CFA# Zack Williams# last updated 05/03/2020calcOmega.boot.cfa <- function(fit,data=NULL,omega.type=2,B=999,alpha=0.05,round=3,cut=0,bifactor=F,                               extraCoefs=T,cores=detectCores()-1,seed=12345){  # fit = fitted lavaan object  # data = dataframe (can be left blank, and data from lavaan model should be pulled)  # Type = type of omega (1, 2, or 3): see calcOmega function for differences  # B = # bootstrap samples  # alpha = 1 - CI coverage  # round = # digits to round output to  # cut = in case of EFA, what factor loading is needed to assign a variable as "loading" on a factor (helps for calculation of subscale omegas)  # bifactor = is it a bifactor model?  # extraCoefs = also bootstrap ordinal alpha, coefficient H, factor determinacy coefficient, and ECV in bifactor cases  # cores = number of cores used in multi-core processing  # seed = sets a seed (default = 12345) to produce same results each time  if(is.null(data)){    data <- fit@Data@X[[1]]    ncol(data)    colnames(data)[fit@Data@ov$idx] <- fit@Data@ov.names[[1]]  }  set.seed(seed)  # Function: refit CFA with same parameters in original model  refitCFA <- function(dat){    cfa.boot <- cfa(fit@ParTable,data = dat,ordered = inspect(fit,"ordered"),std.lv=TRUE)      return(calcOmega(cfa.boot,data = dat,bifactor = bifactor,cut=cut))  }  # Function: Get matrix of parameters from omega fit object  coefMat <- function(x,type=omega.type,extra=extraCoefs){    if(!type %in% 1:3) type <- 2    if(is.null(x$Omega.S.3) & type==3){      type <- 2    }    if(type==1){      omega_mat <- rbind("Omega.S"=x$Omega.S.1,"Omega.HS"=x$Omega.HS.1)    } else if(type==3){      omega_mat <- rbind("Omega.S"=x$Omega.S.3,"Omega.HS"=x$Omega.HS.3)    } else{ # Type 2      omega_mat <- rbind("Omega.S"=x$Omega.S.2,"Omega.HS"=x$Omega.HS.2)    }    # Extra coefficients - add ordinal alpha, H and Factor Determinacy coefficients    if(extraCoefs){      if(x$bifactor){        omega_mat <- rbind(omega_mat,"Alpha"=x$alpha,"H"=x$H,"FD"=x$FD,                           "ECV"=c(x$ECV,rep(NA,ncol(omega_mat)-1)))      } else{        omega_mat <- rbind(omega_mat,"Alpha"=x$alpha,"H"=c(NA,x$H),"FD"=c(NA,x$FD))      }    }    return(omega_mat)  }  # Get observed data values  obs.om <- calcOmega(fit,data = data,bifactor = bifactor,cut=cut)  obs.mat <- coefMat(obs.om)  obs.vals <- c(obs.mat)  names(obs.vals) <- sapply(colnames(obs.mat),function(X){paste(X,rownames(obs.mat),sep=".")})  obs.vals <- c(na.omit(obs.vals)) # remove any NAs from matrix    # perform bootstrap to generate B values of calcOmega output  cat("Calculating Bootstrap Samples:\n")  BS <- suppressWarnings(pbsapply(1:B,function(x){    # Bootstrapped sample    b <- data[sample(nrow(data),replace=T),]    bmod <- refitCFA(b)    return(c(na.omit(c(coefMat(bmod)))))  },cl=cores))    # Jackknife for BCa CI  cat("Calculating Jackknife Samples:\n")  jk_mat <- t(pbsapply(1:nrow(data),function(i){    tempjk <- refitCFA(data[-i,])    return(c(na.omit(c(coefMat(tempjk)))))  },cl=cores))    CIs <- sapply(1:length(obs.vals),function(val){    BS.Values <- sort(BS[val,])    # if all bootstrap samples yield same value, use it for both ends of CI    if (min(BS.Values) == max(BS.Values)){      CI.Lower <- CI.Upper <- BS.Values[1]    } else {      # bootstrap percentile bias corrected and accelerated (BCa)      # cf. Efron & Tibshirani (1993, Ch. 14)       # calculate bias-correction and acceleration parameters (z0 and a)      Diff <- mean(jk_mat[,val])-jk_mat[,val]      z0 <- qnorm(((sum(BS.Values < obs.vals[val]) + (sum(BS.Values == obs.vals[val])+1)/2)/(B+1)))      # z0 calculation: cf. Efron & Tibshirani (1993, p. 186, formula 14.14)       # However, if ties are present, original formula overestimates bias, thus tie term added      # See https://www.medcalc.org/manual/note-bcabootstrap.php for more details      a <- sum(Diff^3)/(6*(sum(Diff^2))^1.5) # cf. Efron & Tibshirani (1993, p.186/15.15 and p. 328/22.29)       # adjust location of endpoints, cf. Efron & Tibshirani (1993, p.185/14.10)       alpha1 <- pnorm(z0+((z0+qnorm(alpha/2))/(1-(a*(z0+qnorm(alpha/2))))))      alpha2 <- pnorm(z0+((z0-qnorm(alpha/2))/(1-(a*(z0-qnorm(alpha/2))))))      # if either endpoint undefined, replace it with value for percentile CI      if (is.na(alpha1)) {alpha1 <- (alpha/2)}      if (is.na(alpha2)) {alpha2 <- (1-(alpha/2))}      if (round(alpha1*B)<1) {        CI.Lower <- BS.Values[1]      } else {        CI.Lower <- BS.Values[round(alpha1 * B)]      }      CI.Upper <- BS.Values[round(alpha2 * B)]	    }    return(c("CI.Lower"=CI.Lower, "CI.Upper"=CI.Upper))  })    row.names(BS) <- colnames(CIs) <- names(obs.vals)  # Generate result Omega object (from observed Omega object)  result <- obs.om  result$Call <- match.call() # update call to match this function rather than calcOmega call  result$BootCoefs <- cbind("Value"=obs.vals,t(CIs)) # coef values + CIs  # Remove redundant column if 1-factor solution (i.e., don't print out both total and factor 1 omegas, which are the same)  if(length(grep(".*Omega.S$",rownames(result$BootCoefs)))==2){    result$BootCoefs <- t(result$BootCoefs[-grep("^Total",rownames(result$BootCoefs)),,drop=FALSE])    colnames(result$BootCoefs) <- gsub(".S$","",colnames(result$BootCoefs))  }  result$ci.width <- 1-alpha # confidence interval width (for printing)  result$nBoot <- B # number of bootstrap iterations  # Return result (of class "Omega")  return(result)}###### # efa2cfa function# Zack Williams# 7/27/2019 - updated 04/07/2021 to add factor names, slid cutpoint, ability to input f as loading matrix# Takes a rotated psych EFA solution, wipes all loadings smaller than "cut" (default: |load| < 0.1) # and turns it into a CFA solution, using those remaining loadings from the EFA as start values# Allows for EFA fits to be tested with SEM fit indices, SEM.misspec, etc.efa2cfa <- function(f,more=NULL,data,cut=0.1,startvals=T,missing="pairwise",make.bifactor=F,                    fnames=NULL,orthogonal=NULL,selector="bic",...){  if(is.matrix(f)){    loads <- f[]  } else {    loads <- loads(f,add.h2 = F,selector=selector) # Loading matrix  }  nfactors <- ncol(loads)  if(is.null(fnames)){    fnames <- colnames(loads)  }  efa.loads <- zapsmall(matrix(round(loads, 2), nrow = nrow(loads), ncol = ncol(loads)))  # Add in target matrix (based on slid cutpoint)  if(cut=="dif"){    if(!is.null(Phi(f)) & !all(Phi(f) == diag(nfactors))){stop("ERROR: cut = 'dif' only implemented for bifactor loading matrices.")}    slid_target <- get_target_from(efa.loads)$Targ  }  item.names <- rownames(loads)  if(is.null(item.names)){stop("Items must be named in loading matrix. Please check input data.")}  # See whether factors are orthogonal ("orthogonal" arg will override)  if(!is.null(orthogonal)){    orth <- orthogonal  } else{    if(is.null(Phi(f)) | all(Phi(f) == diag(nfactors))){      orth <- TRUE    } else{      orth <- FALSE    }  }      # Syntax to turn factor loadings into lavaan syntax  terms <- sapply(1:nfactors,function(i){    # paste0("F",i,"=~ ", paste0(c(efa.loads[,i])," * ", item.names, collapse = " + "))    if(cut=="dif"){      highloads <- which(is.na(slid_target[,i]))    } else{      highloads <- which(abs(efa.loads[,i])>cut)    }    if(startvals){ # use the EFA values as starting vales      if(is.null(fnames) || is.na(fnames[i])){ # If no factor name        paste0("F",i,"=~ ", paste0("start(",c(efa.loads[highloads,i]),") * ", item.names[highloads], collapse = " + "))      } else{        paste0(fnames[i],"=~ ", paste0("start(",c(efa.loads[highloads,i]),") * ", item.names[highloads], collapse = " + "))      }    } else{ # No starting values (default startvals)      if(is.null(fnames) || is.na(fnames[i])){ # If no factor name        paste0("F",i,"=~ ",paste(item.names[highloads], collapse = " + "))      } else{        paste0(fnames[i],"=~ ",paste(item.names[highloads], collapse = " + "))      }    }  })  # If make.bifactor is true, add a general factor to the syntax and also make orthogonal rotation  if(make.bifactor){    orth <- TRUE    bfac <- paste0("G =~ ", paste0(item.names, collapse = " + "))  } else{    bfac <- NULL  }    terms <- c(bfac,terms,more) # Add whatever other lavaan conditions you want, in "more" item  syntax <- paste0(terms,collapse="\n")  # print(syntax) # for debugging  fit <- lavaan::cfa(syntax, data = data, orthogonal=orth, verbose = F, std.lv=T,missing=missing,...)  fit@Cache$ModelSyntax <- syntax  return(fit)}########################### nfactor.rmsea function## Zack Williams## August 3, 2019## Fit Sequential factor analysis models to the data, and determine ∆RMSEA between fits## If ∆RMSEA > 0.015, retain additional factor. If < 0.015, stop.# Works for both continuous variables (ML extraction) or categorical variables (WLSMV extraction using semTools::efaUnrotate)# Based on Finch, W. H. (2019). Educational and Psychological Measurement# http://doi.org/10.1177/0013164419865769nfactor.rmsea <- function(X,categorical = F,n.obs = NA,verbose=T){    if(dim(X)[1]==dim(X)[2]){ # X is correlation matrix    if(categorical==T){ # Can't use correlation matrix as input for categorical data      stop("\n\n FATAL ERROR: Raw data is needed to calculate WLSMV fit indices.")    } else if(is.na(n.obs)){      stop("\n\n FATAL ERROR: Data is a correlation matrix without number of observations specified.")    }  }     rmsea_vals <- cfi_vals <- tli_vals <- NULL  result <- list()  n.fac <- 1  stop <- FALSE  if(verbose == T & categorical == T){    message("Conducting lavaan EFA... may take a while.\n")  }  while(stop==FALSE){    if(categorical){ # WLSMV estimator      if(verbose){        message(paste0(" Fitting ",n.fac," factor EFA..."))      }      fa <- semTools::efaUnrotate(X,nf = n.fac,ordered=categorical,parameterization="Theta")      result[[paste0("Mod.",n.fac,"F")]] <- fa      rmsea_vals <- c(rmsea_vals,unname(fitMeasures(fa,"rmsea.scaled")))      cfi_vals <- c(cfi_vals,unname(fitMeasures(fa,"cfi.scaled")))      tli_vals <- c(tli_vals,unname(fitMeasures(fa,"tli.scaled")))    } else{ # ML estimator      fa <- suppressWarnings(psych::fa(X,nfactors = n.fac,fm="ml",rotate="none",n.obs=n.obs))      result[[paste0("Mod.",n.fac,"F")]] <- fa      rmsea_vals <- c(rmsea_vals,unname(fa$RMSEA[1]))      tli_vals <- c(tli_vals,unname(fa$TLI))      cfi_vals <- c(cfi_vals,((fa$null.chisq - fa$null.dof) - (fa$STATISTIC - fa$dof))/                      (fa$null.chisq - fa$null.dof))    }        # Check to see if ∆RMSEA meets cutoff    if(n.fac == 1){ # 1 factor: no comparison      n.fac <- n.fac + 1    } else if(-diff(rmsea_vals[c(n.fac-1,n.fac)]) > 0.015){ # ∆RMSEA > 0.015      n.fac <- n.fac + 1 # Increment factor, check again    } else{ # ∆RMSEA < 0.015 (retain n.fac - 1 factors)      stop <- TRUE    }  }  if(verbose == T & categorical == T){    message(" Done!\n")  }  fit_stats <- rbind("CFI"=cfi_vals,"TLI"=tli_vals,"RMSEA"=rmsea_vals)  colnames(fit_stats) <- paste0(1:n.fac,"F")      result$Call <- match.call()  result$categorical <- categorical  result$N.factors <- n.fac - 1  result$RMSEA.Values <- rmsea_vals  result$RMSEA.Differences <- -diff(rmsea_vals)  names(result$RMSEA.Differences) <- paste0(1:(n.fac-1),"F.v.",2:n.fac,"F")  result$Fit.stats <- fit_stats    class(result) <- "nf.rmsea"  return(result)}# Print function for nfactor.rmsea objectprint.nf.rmsea <- function(x){  cat("Call: ")  print(x$Call)  cat("\n")  cat(paste0("The sequential RMSEA method suggests that ",x$N.factors," factors be retained.\n"))  cat("\nRMSEA differences:\n")  print(round(x$RMSEA.Differences,3))  cat("\nModel Fit Statistics:\n")  print(round(x$Fit.stats,3))}##### Regularized SEM: FA.regsem## Exploratory Factor Analysis using regularized SEM (no rotation needed)## Spits out a 'psych' fa object using the as.fa() function## Zack Williams## 08/06/2019 - updated 10/13/2020 to add progress bar# Based on maximum likelihood estimated SEM (can be used on a polychoric correlation matrix for ordinal items but this has not been studied)# Scharf, F., & Nestler, S. (2019). Should Regularization Replace Simple Structure Rotation in # Exploratory Factor Analysis?. Structural Equation Modeling: A Multidisciplinary Journal, 1-15.# http://doi.org/10.1080/10705511.2018.1558060fa.regsem <- function(data,nfactors=1,n.obs=NULL,categorical=F,                      type="enet",mult.start=T,n.lambda = 20,lambda.start = 0.001,                      jump = 5*10^-5,beta=seq(0.05,0.95,0.05),cores=detectCores()-1,...){    if(dim(data)[1]==dim(data)[2]){ # if correlation matrix supplied    if(is.null(n.obs)){      stop("Must include number of observations to fit correlation matrix.")    }    cormat <- data  } else{ # if raw data supplied    if(categorical){      cormat <- cor_auto(data,verbose = F)    } else{      cormat <- cor(data)    }    n.obs <- nrow(data)  }    if(nfactors > 1){    efa_model <- efaModel(nfactors, colnames(data))  } else{    efa_model <- paste0("f1 =~",paste(colnames(data),collapse = " + "))  }    semFit <- cfa(efa_model,sample.cov = cormat,sample.nobs = n.obs, std.lv = TRUE,                 auto.fix.single = FALSE, se = "none")    # Default settings - based on simulation used in Scharf & Nestler (2019)  # Can change using the options available  if(type=="enet"){    # If enet, vary enet parameter (beta) as well    message(paste0("Testing ",length(beta)*n.lambda," parameter combinations. This may take a while...\n"))    # Default of 380 (20*19) parameter combinations    regList <- pblapply(beta,function(par){      cv_regsem(model = semFit, pars_pen = "loadings",mult.start = mult.start,                n.lambda = n.lambda, type = "enet", jump = jump,alpha = par,                 lambda.start = lambda.start,...)    },cl=cores)        # Calculate best model BIC for each parameter combination    BICs <- unlist(lapply(regList,function(reg){      if(all(class(reg)!="cvregsem")) return(NaN) # Now handles errors in various parameters      fits <- reg$fits      min(fits[fits[,2] == 0,4])    }))        # Choose best model by BIC    reg <- regList[[which.min(BICs)]]  } else{ # type = LASSO    reg <- cv_regsem(model = semFit, pars_pen = "loadings",mult.start = mult.start,                     n.lambda = n.lambda, type = type, jump = jump,                      lambda.start = lambda.start,...)  }    # Turn the loadings from regularized SEM into a loading matrix  loads <- matrix(reg$final_pars[1:(ncol(data)*nfactors)],nrow=ncol(data))    # Extract correlations of factors  fcors <- reg$final_pars[(ncol(data)*nfactors+1):((ncol(data)*nfactors+nfactors*(nfactors-1)/2))]    phi <- diag(nfactors)  phi[lower.tri(phi)] <- fcors  phi <- t(phi)  phi[lower.tri(phi)] <- fcors    result <- as.fa(loads,r = cormat,n.obs=n.obs,Phi = phi,fm="ml",rotate = "none")  result$regsem <- reg # Saves cv_regsem output in the fa object  return(result)}### regsem.bifactor## Zack Williams, 10/13/20## Wrapper function for regsem(data) %>% efa2cfa(make.bifactor=T)## Allows you to run an exploratory bifactor SEM based on the results of regularized SEM# Allows for full customization of regsem, as well as additional specifications of model based on 'more'# Arguments can be given to efa2cfa lavaan function using cfa.args listregsem.bifactor <- regsem.bf <- function(data,nfactors=2,n.obs=NULL,ordered=T,type="enet",                                         mult.start=T,n.lambda = 20,lambda.start = 0.001,                                         jump = 5*10^-5,beta=seq(0.05,0.95,0.05),                                         more=NULL,cut=0.05,startvals=F,missing="pairwise",file=NULL,...){  # Use regsem to generate EFA model (categorical is default)  rsmod <- fa.regsem(data=data,nfactors=nfactors,n.obs=n.obs,categorical=any(ordered != FALSE),type=type,                     mult.start=mult.start,n.lambda = n.lambda,lambda.start = lambda.start,                     jump = jump,beta=seq(0.05,0.95,0.05),cores=detectCores()-1) # Doesn't have all regsem args    cfamod <- efa2cfa(rsmod,more=more,data=data,cut=cut,startvals=startvals,missing=missing,                    ordered=ordered,make.bifactor=T,...)  cfamod@Cache$regsem <- rsmod    # If desired, save result as a file  if(!is.null(file)){    if(!grepl("\\.[Rr][Dd][Ss]$",file)){ # whoops forgot extension!      file <- paste0(file,".RDS")    }    saveRDS(cfamod,file=file) # Saves an RDS file with chosen name  }  return(cfamod)}#### More regularized SEM, this time using lslx package### Zack Williams, 04/04/2021## Ref: Huang, P. H. (2020). Penalized Least Squares for Structural Equation Modeling with Ordinal Responses. ## Multivariate Behavioral Research, 1-19. https://doi.org/10.1080/00273171.2020.1820309 ## See also https://doi.org/10.1007/s11336-017-9566-9 (for ML) and https://doi.org/10.18637/jss.v093.i07 (for lslx intro)####################################################################################################### ARGUMENTS:# X = data set or correlation matrix# spec is vector of specific factor loadings. Can include NA for just general factor loads (if bifactor is true). like mirt::bfactor# If you want something more than one factor per variable supported, provide spec as a list# each item of the spec list should include a numeric vector with all items that load on that specific factor# penalty: can be "lasso," "ridge," "elastic_net," or "mcp" (default)# The factor names will be taken from the spec list (if applicable), or the 'spec.names' argument# bifactor: will additionally estimate a general factor onto which all items have (unpenalized) loadings# orthogonal: whether factors should be able to correlate with each other# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.# ... = additional args for plsem function# return.lslx: if true, returns output of plsem (does not convert to lavaan)CFA.lslx <- CFA.regsem <- function(X,spec=NULL,more=NULL,ordered=TRUE,penalty="mcp",selector="bic",                                   bifactor=FALSE,orthogonal=FALSE,spec.names=NULL,startvals=F,                                   return.lslx=F,verbose=T,...){  if(!is.data.frame(X)){    stop("ERROR: 'X' must be a data frame containing raw item data.")  }  inames <- colnames(X)  # Parse data on specific factors (either list or vector input)  if(is.vector(spec) & !is.list(spec)){ # vector input     if(!is.numeric(spec)){stop("ERROR: 'spec' must be a numeric vector or list of specific factor assignments.")}    if(length(spec)!=ncol(X)){      stop("ERROR: 'spec' values must be specified for all variables. Use 'NA' to have a variable not load on the general factor")    }    if(any(is.na(spec)) & !bifactor){stop("ERROR: NA values are not allowed in 'spec' unless bifactor is set to TRUE")}    spec <- as.numeric(as.factor(spec)) # if numbers not consecutive or started at 2, fix that.    n.spec <- max(spec,na.rm = T) # how many specific factors?    spec_list <- lapply(1:n.spec,function(fac){which(spec==fac)})  } else if("list" %in% class(spec)){ # list input    if(is.null(spec.names) & !is.null(names(spec))){spec.names <- names(spec)} # get factor names from list    n.spec <- length(spec)    if(!is.numeric(spec[[1]])){stop("ERROR: 'spec' must be a numeric vector or list of specific factor assignments.")}    spec_list <- lapply(spec,as.numeric)  } else{    stop("ERROR: 'spec' must be a numeric vector or list of specific factor assignments.")  }  # Fix spec_list and get rid of any empty entries  empty_vals <- which(unlist(lapply(spec_list,function(x){length(x)==0})))  if(length(empty_vals)>0){spec_list <- spec_list[-empty_vals]}  # Throw an error if a factor has only one non-penalized loading  if(any(unlist(lapply(spec_list,length))==1)){    stop("ERROR: More than one item should be assigned to each factor in 'spec'")  }  # Now create matrix of keys  key_mat <- psych::make.keys(ncol(X),spec_list)==1  if(is.null(spec.names)){    spec.names <- paste0("S",1:n.spec)  }  # Now take the specific factor list and turn it into a lavaan model  spec_mod <- lapply(1:n.spec,function(s){    fname <- spec.names[s]    s_items <- which(key_mat[,s])    pen_s_items <- which(!key_mat[,s])        s_syntax <- paste0("\n",paste(fname,"=~",paste(inames[s_items],collapse = " + ")))    if(length(pen_s_items)>0){      pen_s_syntax <- paste0("\npen() * ",paste(fname,"=~",paste(inames[pen_s_items],collapse = " + ")))    } else{      pen_s_syntax <- NULL    }    return(paste0(s_syntax,pen_s_syntax))  })  spec_mod_nopen <- lapply(1:n.spec,function(s){    fname <- spec.names[s]    s_items <- which(key_mat[,s])    return(paste0("\n",paste(fname,"=~",paste(inames[s_items],collapse = " + "))))  })  # If bifactor, add a general factor with non-penalized loadings from all items and make it orthogonal  if(bifactor){    orthogonal <- TRUE    g_mod <- paste("G =~",paste(inames,collapse = " + "))    g_mod_covars <- paste0("\nG ~~ 1 * G + ",paste("0 *",spec.names,collapse = " + "))  } else{    g_mod <- g_mod_covars <- NULL  }  # Now create lavaan model syntax for the remaining parts of the model (variances/covariances, "more")  mod_covars <- lapply(1:n.spec,function(i){    if(i==n.spec | !orthogonal){      return(paste0("\n",spec.names[i]," ~~ 1 * ",spec.names[i]))    } else{      return(paste0("\n",spec.names[i]," ~~ 1 * ",spec.names[i]," + ",paste("0 *",spec.names[(i+1):n.spec],collapse = " + ")))    }  })    # Final lavaan model syntax  MOD_FINAL <- paste0(g_mod,paste(c(spec_mod,g_mod_covars,mod_covars),collapse=""),"\n",more)  message("Fitting regularized factor models... ")  if(any(TRUE %in% ordered)){    cfa_lslx <- plsem(MOD_FINAL,data=X,ordered_variable = inames,loss="dwls",...,verbose = verbose)  } else if(length(ordered)>1){    cfa_lslx <- plsem(MOD_FINAL,data=X,ordered_variable = ordered,loss="dwls",...,verbose = verbose)  } else{    cfa_lslx <- plsem(MOD_FINAL,data=X,loss="ml",...,verbose = verbose)  }  message("Done!")  if(return.lslx){    return(cfa_lslx)  }  # Now convert to lavaan (any nonzero loading is fit)  if(any(TRUE %in% ordered) | length(ordered)>1){    result <- efa2cfa(cfa_lslx,data=X,cut = 0,startvals = startvals,ordered=ordered,missing="pairwise",orthogonal=orthogonal,more=more,selector="bic")  } else{    result <- efa2cfa(cfa_lslx,data=X,cut = 0,startvals = startvals,ordered=F,estimator="MLMV",missing="fiml",orthogonal=orthogonal,more=more,selector="bic")  }  result@Cache$lslx_mod <- cfa_lslx  return(result)}# Corrected Item-total Polyserial (and Pearson) Correlations# Zack Williams# 1/2/2018, updated 09/30/20itemTotalCor <- ITcor <- function(item_mat,round.output=TRUE,round.digits=3,type="both"){  cor_mat <- NULL  for(i in 1:ncol(item_mat)){    cor_mat = rbind(cor_mat,c(      psych::polyserial(rowSums(item_mat[,-i]),as.data.frame(item_mat[,i])),      cor(rowSums(item_mat[,-i]),as.data.frame(item_mat[,i]),use = "pairwise")    ))  }  outmat <- data.frame(cor_mat,row.names=colnames(item_mat))  names(outmat) <- c("Polyserial","Pearson")  if(type %in% c("Poly","poly","Polyserial","polyserial","biserial","Biserial")){    output <- outmat[,1]    names(output) <- colnames(item_mat)    cat("Corrected Item-Total Polyserial Correlations:\n")  } else if(type %in% c("Pearson","pearson")){    output <- outmat[,2]    names(output) <- colnames(item_mat)    cat("Corrected Item-Total Pearson Correlations:\n")  } else {    output <- outmat  }  if(round.output){    return(round(output,round.digits))  } else{    return(output)  } }### ESEM.bifactor - quick function to fit exploratory correlated-factors model in lavaan (ordinal WLSMV by default)## Zack Williams, 09/04/2021# Use lavaan to create an ESEM model (Starts with EFA, finds anchor items, fixes cross loadings of anchors, refits as CFA)# For bifactor EFA, uses iterative Schmid-Leiman target rotation (see SLi function for more details)##### Uses information from following blogs:### https://solomonkurz.netlify.app/post/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/ (for EFA)### https://msilvestrin.me/post/esem/ (for ESEM)############################################################################################################################ Arguments:# X = data set or correlation matrix# n.specific is an integer# each item of the spec list should include a numeric vector with all items that load on that specific factor# use rotation.args to modify technical rotation details (default geomin with epsilon = 0.05)# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.ESEM.bifactor <- ESEM.bf <- function(X,n.specific=2,more=NULL,rotation="geominQ",ordered=TRUE,std.lv=TRUE,missing="pairwise",fnames=c("G",paste0("S",1:n.specific)),                                     fm=ifelse(is.character(ordered) | ordered[1]==TRUE,"mhrm","minres"),cutpoint= "dif", iterations = 20, BiFAD.start = FALSE, Target=NULL, cutpoint.bifad=0.2, ...){  # Catch low numbers of factors  if(n.specific < 2) stop("Bifactor ESEM requires two or more specific factors! To fit a bifactor model with one specific factor, use CFA.bifactor function.")    # If target matrix supplied, make sure it's specified correctly  if(!is.null(Target)){    BiFAD.start <- TRUE    # Catch incorrectly-specified target matrix (e.g., partial specification not supported)    if(nrow(Target) != ncol(X)) stop(paste0("Target matrix has incorrect number of rows (input has ",nrow(Target),", should be ",ncol(X),")."))    if(!all(Target %in% c(1,0,-1)) | any(is.na(Target))) stop("All values in target matrix must be either 0 or ±1.")    message("Target matrix supplied. Ignoring number of specific factors.")    # If only specific factor target present, add a general factor    if(sum(abs(Target[,1])) != nrow(Target)){      Target <- cbind("G"=1,Target)    }    n.specific <- ncol(Target)-1    if(length(fnames)!=ncol(Target)){fnames <- c("G",paste0("S",1:n.specific))}  }    inames <- colnames(X)  nitems <- length(inames)  nfactors <- n.specific + 1    # Fit efa model using direct Schmid-Leiman algorithm  efa_fit <- SLiD(matrix=X,specific_factors = n.specific,fm=fm,                  rot=rotation,cutpoint=cutpoint,iterations = iterations,BiFAD=BiFAD.start,cutpoint.bifad=cutpoint.bifad,B=Target)      efa_loads <- loads(efa_fit,add.h2 = F)  # Pick anchors based on which item for each factor has the highest difference between loading^4 and sum of cross-loads^4  allowable_rows <- (1:nrow(efa_loads))[rowSums(efa_loads^2) < 1]  which_anchors <- vapply(1:nfactors,function(i){    return(allowable_rows[which.max((efa_loads^4)[allowable_rows,i] - rowSums((efa_loads^4)[allowable_rows,-i,drop=FALSE]))])  },c(1))  # If one of the anchors is duplicated, replace with the next best item using same criteria  if(any(duplicated(which_anchors))){    for(i in which(duplicated(which_anchors))){      potential_anchors <- (1:nitems)[-which_anchors]      potential_anchors <- potential_anchors[which(potential_anchors %in% allowable_rows)]      which_anchors[i] <- potential_anchors[which.max((efa_loads^4)[potential_anchors,i] - rowSums((efa_loads^4)[potential_anchors,-i,drop=FALSE]))]    }  }  # Now generate the ESEM model  esem_mod <- paste0(paste(lapply(1:nfactors,function(f){    paste0(fnames[f]," =~ ",paste(vapply(1:nitems,function(i){      if(i %in% which_anchors[-f]){ # Item cross-loading fixed (item selected as anchor for a factor other than 'f')        paste0(round(efa_loads[i,f],3),"*", inames[i])      } else{ # Item loading not fixed        paste0("start(",round(efa_loads[i,f],3),")*", inames[i])      }    },c("1")),collapse = " + "))  }),collapse = "\n "),more)  # Fit ESEM model  message("Fitting ESEM model. May take a while for large datasets/complex models.")  result <- cfa(esem_mod,data=X,ordered=ordered,std.lv=std.lv,missing=missing,orthogonal=T,...)  message("Done!")  # Add model syntax to the lavaan object  result@Cache$ModelSyntax <- esem_mod  return(result)}### ESEM.oblique - function to fit exploratory correlated-factors model in lavaan (ordinal WLSMV by default)## Zack Williams, 09/03/2021# Use lavaan to create an ESEM model (Starts with EFA, finds anchor items, fixes cross loadings of anchors, refits as CFA)##### Uses information from following blogs:### https://solomonkurz.netlify.app/post/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/ (for EFA)### https://msilvestrin.me/post/esem/ (for ESEM)############################################################################################################################ Arguments:# X = data set or correlation matrix# nfactors is an integer# each item of the spec list should include a numeric vector with all items that load on that specific factor# use rotation.args to modify technical rotation details (default geomin with epsilon = 0.05, but any GPFoblq method will work)# more = extra lavaan syntax that is added to the end of model. Can account for correlated errors, etc.ESEM.oblique <- ESEM.ob <- function(X,nfactors=2,more=NULL,rotation="geomin",ordered=TRUE,std.lv=TRUE,missing="pairwise",fnames=paste0("F",1:nfactors),MHRM=TRUE,                                    methodArgs=NULL,Target=NULL,...){  # Catch low numbers of factors  if(nfactors < 2) stop("ESEM requires two or more factors! Use CFA.1f function to fit single-factor model.")  inames <- colnames(X)  nitems <- length(inames)    # If target matrix supplied, make sure it's specified correctly  if(!is.null(Target)){    # Catch incorrectly-specified target matrix (e.g., partial specification not supported)    if(nrow(Target) != ncol(X)) stop(paste0("Target matrix has incorrect number of rows (input has ",nrow(Target),", should be ",ncol(X),")."))    message("Target matrix supplied. Ignoring number of specific factors.")    nfactors <- ncol(Target)    methodArgs <- c(methodArgs,list("Target"=Target))    rotation <- "target"  } else if(is.null(methodArgs$delta) & rotation == "geomin"){    methodArgs <- c(methodArgs,list(delta=0.05))  }    # Actually fit EFA model  if(MHRM & ordered[1]==TRUE){ # use mirt for MHRM factor analysis    efa_fit <- mirt(X,nfactors,method = "MHRM")  } else{ # Use WLSMV to handle mixed-type or continuous data    # EFA model    efa_mod <- paste0(paste(paste0('efa(\"efa\")*',fnames),collapse=" + \n")," =~ ",paste(inames,collapse=" + "),"\n",more)    message("Fitting EFA model...")    efa_fit <- sem(efa_mod,data=X,ordered=ordered,rotation="none",std.lv=std.lv,missing=missing,...)    message("Done!")  }  L_unrotate <- loads(efa_fit,add.h2 = F)  efa_loads <- GPFoblq(L_unrotate,method=rotation,methodArgs=methodArgs,maxit=10000)$loadings  # Pick anchors based on which item for each factor has the highest difference between loading^4 and sum of cross-loads^4  allowable_rows <- (1:nrow(efa_loads))[rowSums(L_unrotate^2) < 1]  which_anchors <- vapply(1:nfactors,function(i){    return(allowable_rows[which.max((efa_loads^4)[allowable_rows,i] - rowSums((efa_loads^4)[allowable_rows,-i,drop=FALSE]))])  },c(1))  # If one of the anchors is duplicated, replace with the next best item using same criteria  if(any(duplicated(which_anchors))){    for(i in which(duplicated(which_anchors))){      potential_anchors <- (1:nitems)[-which_anchors]      potential_anchors <- potential_anchors[which(potential_anchors %in% allowable_rows)]      which_anchors[i] <- potential_anchors[which.max((efa_loads^4)[potential_anchors,i] - rowSums((efa_loads^4)[potential_anchors,-i,drop=FALSE]))]    }  }  # Now generate the ESEM model  esem_mod <- paste0(paste(lapply(1:nfactors,function(f){    paste0(fnames[f]," =~ ",paste(vapply(1:nitems,function(i){      if(i %in% which_anchors[-f]){ # Item cross-loading fixed (item selected as anchor for a factor other than 'f')        paste0(round(efa_loads[i,f],2),"*", inames[i])      } else{ # Item loading not fixed        paste0("start(",round(efa_loads[i,f],2),")*", inames[i])      }    },c("1")),collapse = " + "))  }),collapse = "\n "),more)  # Fit ESEM model  message("Fitting ESEM model. May take a while for large datasets/complex models.")  result <- cfa(esem_mod,data=X,ordered=ordered,std.lv=std.lv,missing=missing,...)  message("Done!")  # Add model syntax to the lavaan object  result@Cache$ModelSyntax <- esem_mod  return(result)}#### Finch and French (2016) eta-squared effect sizes#### Zack Williams#### 9/15/2019# Based on: Finch, W. H., & French, B. F. (2016). Quantifying the influence of partial scalar invariance # on mean comparisons: two proposed effect sizes. # International Journal of Quantitative Research in Education, 3(4), 292-313.## Works only for 2-group, 1-factor case at the momentinvar.es <- function(syntax.config,syntax.invar,data,ordered=NULL,groupvar,composite,digits=3){  # syntax.config - syntax for full model  # syntax.invar - syntax for model with non-invariant items removed  # data - data frame used to fit models  # ordered - which items are categorical? (default = none)  # groupvar - string which provides the grouping variable  # composite - score composite of manifest indicators (mean)  # If NAs in composite, then those values not used to compute either eta-squared value  # digits - number of digits to roung to (default 3)    groups <- as.factor(data[,groupvar])  g1 <- which(groups==levels(groups)[1])  g2 <- which(groups==levels(groups)[2])    composite_g1 <- composite[g1]  composite_g2 <- composite[g2]  # Observed eta-squared when comparing groups on "composite"  # Note that this is based only on complete data points for simplicity  es_obs <- etaSquared(aov(composite~groups))[1]    # Now fit a model  fit.part.invar <- measEq.syntax(syntax.invar,data=data,ordered=ordered,missing="pairwise",                                  group=groupvar,parameterization = "theta",ID.fac = "std.lv", ID.cat = "Wu.Estabrook.2016",                                  group.equal = c("thresholds","loadings","intercepts","residuals","lv.variances","lv.covariances")                                  ,return.fit = T)    factor.scores <- predict(fit.part.invar)   group2.factor.scores <- factor.scores[[2]]    group1.model <- cfa(syntax.config,data=data[g1,],missing="pairwise",meanstructure=TRUE,std.lv=T)  group1.model.parms <- parameterEstimates(group1.model) #Obtain model parameters for group 1    # Extract loadings/intercepts from parameter table  item_names <- group1.model.parms[which(group1.model.parms$op=="=~"),3]    # Predicted values for each item score  # Note that categorical items get rounded off at nearest whole number to approximate real data  group2.predict.score <- rowMeans(sapply(item_names,function(i){    if(i %in% ordered){ # categorical - round approximate score      return(round(group1.model.parms[which(group1.model.parms$op=="~1" & group1.model.parms$lhs==i),4] +                      group1.model.parms[which(group1.model.parms$op=="=~" & group1.model.parms$rhs==i),4] *                     group2.factor.scores))    } else{ # not categorical - do not round      return(group1.model.parms[which(group1.model.parms$op=="~1" & group1.model.parms$lhs==i),4] +                group1.model.parms[which(group1.model.parms$op=="=~" & group1.model.parms$rhs==i),4] *               group2.factor.scores)    }      }))  group2.predict.score[which(is.na(composite_g2))] <- NA # insert NAs where original score was NA    grps <- c(rep("g2",length(group2.predict.score)),rep("g1",length(composite_g1)))  scores <- c(group2.predict.score,composite_g1)    es_pred <- etaSquared(aov(scores~grps))[1]    return(round(c("ES.diff"=abs(es_pred - es_obs),"ES.ratio"=abs(es_pred - es_obs)/es_obs,                 "ES.obs"=es_obs,"ES.pred"=es_pred),digits))}##################################################################################### sim.ordinal - Simulate ordinal variables with same properties as data input## Zack Williams## 05/11/2020# df: data frame with ordinal variables to be simulated (can also have continuous variables)# Note: All items treated as ordinal by default. # If you want an item treated as continuous, use "continuous = c(item#)"# Set the seed using 'seed' argument for reproducible results################################################################################### Returns a list object with items $data (output data frame), and $RMSE (root-mean-squared error of correlations and thresholds)# May be worth playing around with 'seed' input to find the result with the lowest RMSE before saving that as the simulated datasim.ordinal <- function(df,continuous=NULL,seed=12345){  set.seed(seed)  inames <- names(df) # names of items  if(is.null(continuous)){ # All ordinal items    polycor <- suppressWarnings(lavCor(df,output=c("cor"),ordered = inames))    thresh <- suppressWarnings(lavCor(df,output=c("th"),ordered = inames))  } else { # at least one continuous item    if(!is.numeric(continuous)) stop("Input to 'continuous' should be an item number.")    polycor <- suppressWarnings(lavCor(df,output=c("cor"),ordered = inames[-continuous]))    thresh <- suppressWarnings(lavCor(df,output=c("th"),ordered = inames[-continuous]))  }  # Now since there is supposedly an underlying MVN distribution creating the item responses, simulate that  mvndata <- MASS::mvrnorm(nrow(df),mu=rep(0,ncol(df)),Sigma = polycor) # note: requires MASS package    # Apply item thresholds to normal data to generate ordinal responses  result <- data.frame(sapply(inames,function(i){    lvls <- unique(sort(as.numeric(df[,i]))) # Values the item can take    inum <- grep(paste0("^",i,"$"),names(df)) # get item index in df    if(inum %in% continuous){return(mvndata[,inum])} # if continuous item, do nothing    taus <- thresh[grep(paste0("^",i,"\\|t\\d+$"),names(thresh))] # find thresholds for that item    out <- lvls[cut(mvndata[,inum],breaks=c(min(mvndata[,inum]),taus,max(mvndata[,inum])),                    labels=F,right=F,include.lowest = T)] # Cut normal data at thresholds, convert to original item scale    return(out)  }))    # Now test how similar the sample characteristics are to original data  if(is.null(continuous)){ # All ordinal items    polycor_res <- suppressWarnings(lavCor(result,output=c("cor"),ordered = inames))    thresh_res <- suppressWarnings(lavCor(result,output=c("th"),ordered = inames))  } else { # at least one continuous item    polycor_res <- suppressWarnings(lavCor(result,output=c("cor"),ordered = inames[-continuous]))    thresh_res <- suppressWarnings(lavCor(result,output=c("th"),ordered = inames[-continuous]))  }    RMSE.cormat <- sqrt(mean((polycor_res - polycor)[lower.tri(polycor)]^2))  RMSE.thresh <- sqrt(mean((thresh_res - thresh)^2))    return(list("data"=result,"RMSE"=c("RMSE.cor"=RMSE.cormat,"RMSE.thresh"=RMSE.thresh)))}############################################################################################################################################## revised.pa: Revised parallel analysis (Green et al., 2012; https://doi.org/10.1177/0013164411422252)## Original code by Sam Green (from his website), modified by Zack Williams## Modifications equal explained variance effect sizes (https://doi.org/10.1177/0013164418754611) and ## support for ordinal variables (not tested with polytomous, but dichotomous data here: https://doi.org/10.1177/0013164415581898)## Progress bars now implemented as well (04/04/2021)## Note that the revised.pa function with polychoric correlations takes a very long time. Recommended to run overnight## Also be aware that this method tends to find small "lumps" in the data and declare they're factors. They probably are, but maybe not significant ones.## Effect sizes may help with the problem of minor factor identification#### General Information:## This program is written in R code and conducts revised parallel analysis (R-PA). # The references for articles about R-PA are listed in the Publication section of this website. # In order to conduct this program, you need to have dowloaded the R software to your computer # (https://www.r-project.org/). In addition, you need to have installed the psych package  # (http://personality-project.org/r/psych/) that is accessed to conduct the factor analyses.# To install the psych package, type in "install.packages("psych")" in the console.## In conducting an R-PA, you must input your correlation matrix. We refer to this matrix as the# researcher's correlation matrix. The program inserts squared multiple correlations (SMCs) along # the diagonal of this correlation matrix in place of the 1s. We refer to the resulting matrix # as the researcher's reduced correlation matrix. Factors are extracted from the researcher's # reduced correlation matrix using principal axis factor analysis (with non-iterated communalities). # The eigenvalues and eigenvectors from the factoring of the reseacher's reduced correlation # matrix are saved.# A defined number of parallel datasets (typically 100) are generated. Each dataset has the same number of # variables and the same sample size as involved in the computation of the researcher's # correlation matrix. Assuming normality, each parallel dataset is generated based on the factor # loadings of the principal axis factoring of the researcher's reduced correlation matrix. For details, see# the relevant publications. A parallel correlation matrix is computed for each dataset. SMCs are inserted # along the diagonal of each parallel correlation matrix, yielding a parallel reduced correlation matrix.# Factors are then extracted for each parallel reduced correlation matrix using principal axis # factor analysis (with non-iterated communalities). The eigenvalues from the factoring are saved# for the parallel datasets. The program computes the 95%ile of the eigenvalues for each sequentially # extracted factor.## For each sequential factor, the eigenvalue based on the researcher's reduced correlation matrix is compared # to the 95%ile of the eigenvalues based on the parallel reduced correlation matrices. Based on these# results, the number of factors is estimated as well as other output.  To see a description of the output, see# comments at the end of the program.####################################################################################################################################################################################################################################################################################### General Orietation of the R-PA program:# # This program contains two functions: "gen.pa.data" and "revised.pa". Within the R-PA program, # the "gen.pa.data" function is run first followed by the "revised.pa" function. ## The "gen.pa.data" function generates the parallel datasets for the "revised.pa" function.# Each parallel dataset contains N observations and P variables, which are set to be the same as # the researcher's dataset. ## "revised.pa" is the main function for conducting revised parallel analysis.# You (the researcher) need to supply your own correlation to matrix to run the "revised.pa" function.# The supplied correlation matrix should have P rows and P columns. Alternatively, a data matrix (of N# rows and P columns) may be used. To utilize ordinal data, raw data rather than a correlation matrix# must be supplied.####################################################################################################################################################################################################################################################################################### The "gen.pa.data" function generates parallel datasets for the "revised.pa" function. The parallel# datasets contain N observations and P variables, which are set to be the same as the researcher's dataset. ###########################################################################################################################################gen.pa.data <- function(P, lambda, N, N.Factors){    generated.data <- matrix(NA, nrow=N, ncol=P)    factor.score <- replicate(N.Factors, rnorm(N, 0, 1))  unique.score <- replicate(P, rnorm(N, 0, 1))    unique.loading <- matrix(NA, nrow=P, ncol=1)  unique.loading.sq <- matrix(NA, nrow=P, ncol=1)    lambda.sq <- lambda*lambda    for(p in 1:P){    unique.loading.sq[p,1] <- 1-sum(lambda.sq[p, ])    unique.loading[p,1] <- sqrt(unique.loading.sq[p,1])    generated.data[ ,p] <- (factor.score %*% t(lambda))[ ,p] + unique.score[ ,p]*unique.loading[p,1]  }  return(generated.data)}############################################################################################################################################ The "revised.pa" function conducts R-PA to estimate the number of factors underlying your variables.# By running the "fa" command in this function each time, the screen will print# "maximum iteration exceeded". This is because we set the maximum number of iterations # for convergence (max.iter) at 1 to make sure the squared multiple correlations (SMC=TRUE) # are used as the communality estimates. ###########################################################################################################################################revised.pa <- function(data, N=NULL, N.PD=100, percentile=.95,minres=FALSE,                       poly=FALSE,n.cores=detectCores()-1){  require(psych)  require(parallel)  require(qgraph)  options("mc.cores"=n.cores)  # data is a correlation or covariance matrix or a raw data matrix supplied by users.  # If a covariance matrix is supplied, it will be converted to a correlation matrix.  # If raw data is supplied, the correlation matrix will be calcualted using pairwise deletion.   # N is the number of observations. Must be supplied by users.  # N.PD is the number of parallel datasets for conducting the revised parallel analysis.  # Default N.PD is 100.  # percentile is the percentile to compare eigenvalues for observed dataset and generated  # parallel datasets. Default percentile is .95.     P <- dim(data)[2] # Number of variables    #############################################################################################  # Conduct principal axis factor analysis and save the eigenvalues for the observed dataset  #############################################################################################  if(is.null(N)){    if(isSymmetric(as.matrix(data))){      stop("ERROR: No value of N specified with correlation matrix. Try again.")    } else{      N <- dim(data)[1]    }  }  if(poly==TRUE){ # Modification by Zack W.- if polychoric indicated, calculate correlation matrix     if(isSymmetric(as.matrix(data))){      stop("Raw data input needed for Revised PA with ordinal variables. Try again.")    } else{      data <- cor_auto(data,verbose = F)      taus <- t(bind_rows(getThreshold(data)))    }  }  # MINRES option added by Zack Williams (akin to psych package fa.parallel)  if(minres==TRUE){    fm = "minres"    max.iter = 50  } else{    fm = "pa"    max.iter = 1  }  eigs.data <- suppressWarnings(psych::fa(data, nfactors=1, rotate="none", SMC=TRUE, max.iter=max.iter, fm=fm, warnings=FALSE))$values    # Set up a function for calcualting percentile rank  perc.rank <- function(vec, x){length(vec[vec<=x])/length(vec)}    # Define the max number of factors tested  F.Max <- sum(eigs.data>0)     # Set up an array for saving eigenvalues for the parallel datasets  eigs.para.array <- array(NA, c(N.PD, P, F.Max))     #############################################################################################  # Generate parallel datasets assuming k-1 factors underlying P variables  # Compare eigenvalues for observed dataset and parallel datasets  #############################################################################################    # Start with testing the first factor  T.NFactor=1     while(T.NFactor<=F.Max){ # Loop from the first factor to the max number of factors        # Testing the first factor    if(T.NFactor==1){            # Generate parallel datasets assuming zero factor underlying data            #parallel processing starts here — currently taken out      templist <- pblapply(1:N.PD,function(XX) {        # Simulate multivariate normal data        para.data <- matrix(rnorm(N*P), nrow=N, ncol=P)        # Addition to convert to ordinal data        if(poly==TRUE){          para.data.cat <- matrix(NA, nrow=N, ncol=P) # Matrix for thresholded data          for(i in 1:P){ # For each parameter, convert continuous sim data to categorical            para.data.cat[,i] <- cut(para.data[,i],c(min(para.data[,i]),taus[i,],max(para.data[,i])),                                     labels=1:(length(taus[i,])+1),right=F,include.lowest=T)          }          # Polychoric correlations of categorized data          para.data <- cor_auto(para.data.cat,verbose = FALSE)        }        # Eigenvals of reduced correlation matrix        return(fa(para.data,nfactors=1, rotate="none", SMC=TRUE, max.iter=max.iter, fm=fm, warnings=FALSE)$values)      },cl=n.cores)      eigs.para.array[1:N.PD, ,T.NFactor] <- t(matrix(unlist(templist),ncol=N.PD))            eigs.para <- as.numeric(quantile(eigs.para.array[ ,T.NFactor,T.NFactor], percentile))      eigs.data.rank <- perc.rank(eigs.para.array[ ,T.NFactor,T.NFactor], eigs.data[T.NFactor])      if(eigs.data[T.NFactor]>eigs.para[T.NFactor]){T.NFactor <- T.NFactor+1} else break          }            # Testing the T.NFactor-th factor    if(T.NFactor>1 & T.NFactor<=F.Max){            # Compute the factor loadings assuming (T.NFactor-1) factors underlying the observed data      # These factor loadings will be used to generate parallel datasets      lambda.data <- matrix(as.numeric(fa(data, nfactors=T.NFactor-1, rotate="none", SMC=TRUE, max.iter=max.iter, fm=fm, warnings=FALSE)$loadings),                             nrow=P, ncol=T.NFactor-1, byrow=FALSE)            # Generate parallel datasets assuming (T.NFactor-1) factor underlying data      # parallel processing starts here  - the more cores the better!      templist <- mclapply(1:N.PD,function(XX) {        para.data <- gen.pa.data(P, lambda=lambda.data, N, N.Factors=T.NFactor-1)         # Addition to convert to ordinal data        if(poly==TRUE){          para.data.cat <- matrix(NA, nrow=N, ncol=P) # Matrix for thresholded data          for(i in 1:P){            para.data.cat[,i] <- cut(para.data[,i],c(min(para.data[,i]),taus[i,],max(para.data[,i])),                                     labels=1:(length(taus[i,])+1),right=F,include.lowest=T)          }          para.data <- cor_auto(para.data.cat,verbose = FALSE)        }        fa(para.data,nfactors=1, rotate="none", SMC=TRUE, max.iter=max.iter, fm=fm, warnings=FALSE)$values      })      eigs.para.array[1:N.PD, ,T.NFactor] <- t(matrix(unlist(templist),ncol=N.PD))            eigs.para <- c(eigs.para, as.numeric(quantile(eigs.para.array[ ,T.NFactor,T.NFactor], percentile)))      eigs.data.rank <- c(eigs.data.rank, perc.rank(eigs.para.array[ ,T.NFactor,T.NFactor], eigs.data[T.NFactor]))      if(eigs.data[T.NFactor]>eigs.para[T.NFactor]){T.NFactor <- T.NFactor+1} else break          }      } # End the while loop    n.factors = T.NFactor-1    # Additional code (by Zack Williams) for calculation of Proportion of Indicator Common Variance Due to a Factor  # Formula: Calculate e-values from reduced correlation matrix with SMCs on diagonal  # PCV is the ratio of the factor eigenvalue to the sum of all eigenvalues greater than 0  PCV <- eigs.data[1:F.Max]/sum(eigs.data[1:F.Max])    # Print out results  cat("\n")  cat("The estimated number of factors is ", n.factors, ".\n", sep="")  cat("\n")  cat(c("The eigenvalues extracted for each sequentially extracted factor based on the observed dataset are: \n",         format(round(eigs.data,2), nsmall=2), "\n"), collapse=" ")  cat("\n")  cat(c(paste("The ", percentile*100, "%ile eigenvalues for each sequentially extracted factor based on the parallel datasets are: \n", sep=""),         format(round(eigs.para,2), nsmall=2), "\n"), collapse=" ")  cat("\n")  cat(c("The percentile ranks of the eigenvalues based on the observed dataset compared to the eigenvalues based on the parallel datasets are: \n",         paste(format(round(eigs.data.rank*100,0), nsmall=0), "%ile", sep=""), "\n"), collapse=" ")  cat("\n")  cat(c(paste("The proportions of indicator common variance for each sequentially extracted factor based on the observed dataset are: \n", sep=""),         format(round(PCV,3), nsmall=3), "\n"), collapse=" ")} # End the R-PA function# Conduct revised parallel analysis# revised.pa(data=data_matrix, N=400, poly=F) - Example function call############################################################################################################################################ The program assumes no missing data.  There have been no manuscripts on how to conduct R-PA in the presence of missing data. ## Currently the number of parallel datasets is set at 100 (N.PD=100) and the %ile at .95 (percentile=.95) as the default option.# Thus these two arguments were not specified in the statement.  These are the values recommended in the articles on R-PA.  # You can input a different number of parallel datasets and a different %ile as a cutoff for the eigenvalues for the parallel # datasets to override the default values.## Output values for the R-PA:# # R-PA returns the following output: # 1) the estimated number of factors; # 2) the eigenvalues for each sequentially extracted factor based on the researcher's reduced correlation matrix;# 3) the 95%ile (or other pencentile values specified by the researcher) eigenvalues for each sequentially extracted factor based on#    the parallel datasets. # 4) the percentile ranks of the eigenvalues based on the researcher's reduced correlation matrix compared to the eigenvalues based # on the parallel datasets# 5) effect sizes (Proportion of Indicator Common Variance Due to a Factor)###########################################################################################################################################